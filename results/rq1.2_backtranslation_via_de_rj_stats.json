{
  "('rj', 'LogReg', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.2007428247690391,
        "std": 0.1419136575373543,
        "ci_lower": 0.006926852305398498,
        "ci_upper": 0.48476620088036415
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.745568007212524,
        "std": 0.06792055905855651,
        "ci_lower": 1.6129234244661412,
        "ci_upper": 1.8767263226128974
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.25267547782162675,
        "std": 0.005311698756876643,
        "ci_lower": 0.24232447128697154,
        "ci_upper": 0.2629241308590286
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9112362152873048,
        "std": 0.006640927588194412,
        "ci_lower": 0.8981812752630254,
        "ci_upper": 0.9244784432941466
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5485556524949515,
        "std": 0.005745223926336024,
        "ci_lower": 0.5371286778460722,
        "ci_upper": 0.5594885970117478
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5056472763540157,
        "std": 0.009011179852351668,
        "ci_lower": 0.48881321171995373,
        "ci_upper": 0.5242668927095395
      }
    }
  },
  "('rj', 'LogReg', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.21529024460925508,
        "std": 0.1464430451153813,
        "ci_lower": 0.009876260520898399,
        "ci_upper": 0.523888684912587
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.802183296856619,
        "std": 0.06781319437065665,
        "ci_lower": 1.6712674208531242,
        "ci_upper": 1.936697680858945
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.25349916069258205,
        "std": 0.005123173423289156,
        "ci_lower": 0.2433276118161419,
        "ci_upper": 0.26337715790446087
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9120694413493018,
        "std": 0.006455668612152672,
        "ci_lower": 0.8988054523291101,
        "ci_upper": 0.9241267052970766
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5484375524414495,
        "std": 0.005772071800483455,
        "ci_lower": 0.5369751278360793,
        "ci_upper": 0.5596574760372793
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5098630062352916,
        "std": 0.009044493507072553,
        "ci_lower": 0.4926983523237029,
        "ci_upper": 0.5286950918093128
      }
    }
  },
  "('rj', 'LogReg', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.5159537162272743,
        "std": 0.15986205888447794,
        "ci_lower": 0.1955641140928672,
        "ci_upper": 0.8410852710675281
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.2383847464359572,
        "std": 0.14539954771406352,
        "ci_lower": 0.0189229446208659,
        "ci_upper": 0.5359992166283256
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.7130139544631349,
        "std": 0.06747397789678038,
        "ci_lower": 1.5823222114355966,
        "ci_upper": 1.847601969338742
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.7442689109913795,
        "std": 0.009241664746303604,
        "ci_lower": 0.7251501358671119,
        "ci_upper": 0.761449383300982
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.36770960310313033,
        "std": 0.00788472815964254,
        "ci_lower": 0.35239430988003023,
        "ci_upper": 0.38314872778866166
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.9190576699072293,
        "std": 0.0035325602232305557,
        "ci_lower": 0.9122234332110534,
        "ci_upper": 0.9262625683515355
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.9557520505296478,
        "std": 0.0029745271645779474,
        "ci_lower": 0.9499769328268811,
        "ci_upper": 0.9615860194553514
      }
    }
  },
  "('rj', 'LogReg', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.20938851084172513,
        "std": 0.14344325293783278,
        "ci_lower": 0.010572939026460006,
        "ci_upper": 0.5006631085556161
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.6648379464377414,
        "std": 0.07241699437036545,
        "ci_lower": 1.5162595374508347,
        "ci_upper": 1.8014781450036526
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.20311051767605653,
        "std": 0.009289694439694214,
        "ci_lower": 0.1852461083457522,
        "ci_upper": 0.2215252524801213
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9294859347747847,
        "std": 0.006201965677222974,
        "ci_lower": 0.9168899837136605,
        "ci_upper": 0.9410749960568077
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5147304510275112,
        "std": 0.007580913214924842,
        "ci_lower": 0.500028275278001,
        "ci_upper": 0.5299911016465685
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.44073402591849825,
        "std": 0.01611089580028914,
        "ci_lower": 0.41066720490964587,
        "ci_upper": 0.4734203226633102
      }
    }
  },
  "('rj', 'LogReg', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.17077425134380586,
        "std": 0.14328769220801765,
        "ci_lower": 0.0020748841464047992,
        "ci_upper": 0.4807574402791161
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.8585489920467448,
        "std": 0.0724378832369846,
        "ci_lower": 1.7170755234842412,
        "ci_upper": 1.9983269628932117
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.37666029247034816,
        "std": 0.009233233266493055,
        "ci_lower": 0.3586974381421082,
        "ci_upper": 0.3948202453623878
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.7069545591118944,
        "std": 0.009321708165598512,
        "ci_lower": 0.6885424578938504,
        "ci_upper": 0.7249843390804626
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.7809510087615986,
        "std": 0.006186137823780038,
        "ci_lower": 0.7684921648525961,
        "ci_upper": 0.7927626114444631
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.88762020114767,
        "std": 0.005654527490813776,
        "ci_lower": 0.8757242701119321,
        "ci_upper": 0.8979463715076924
      }
    }
  },
  "('rj', 'SVM', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.18033874779585024,
        "std": 0.1474237915306935,
        "ci_lower": 0.002956998221329701,
        "ci_upper": 0.4955377615211659
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.745568007212524,
        "std": 0.06792055905855651,
        "ci_lower": 1.6129234244661412,
        "ci_upper": 1.8767263226128974
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.25267547782162675,
        "std": 0.005311698756876643,
        "ci_lower": 0.24232447128697154,
        "ci_upper": 0.2629241308590286
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9112362152873048,
        "std": 0.006640927588194412,
        "ci_lower": 0.8981812752630254,
        "ci_upper": 0.9244784432941466
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5485556524949515,
        "std": 0.005745223926336024,
        "ci_lower": 0.5371286778460722,
        "ci_upper": 0.5594885970117478
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5056472763540157,
        "std": 0.009011179852351668,
        "ci_lower": 0.48881321171995373,
        "ci_upper": 0.5242668927095395
      }
    }
  },
  "('rj', 'SVM', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.18270394377600077,
        "std": 0.1456849307030449,
        "ci_lower": 0.0003558235853750253,
        "ci_upper": 0.48469256716926057
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.802183296856619,
        "std": 0.06781319437065665,
        "ci_lower": 1.6712674208531242,
        "ci_upper": 1.936697680858945
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.25349916069258205,
        "std": 0.005123173423289156,
        "ci_lower": 0.2433276118161419,
        "ci_upper": 0.26337715790446087
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9120694413493018,
        "std": 0.006455668612152672,
        "ci_lower": 0.8988054523291101,
        "ci_upper": 0.9241267052970766
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5484375524414495,
        "std": 0.005772071800483455,
        "ci_lower": 0.5369751278360793,
        "ci_upper": 0.5596574760372793
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5098630062352916,
        "std": 0.009044493507072553,
        "ci_lower": 0.4926983523237029,
        "ci_upper": 0.5286950918093128
      }
    }
  },
  "('rj', 'SVM', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.5159537162272743,
        "std": 0.15986205888447794,
        "ci_lower": 0.1955641140928672,
        "ci_upper": 0.8410852710675281
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.20963728507391055,
        "std": 0.14695304451878538,
        "ci_lower": 0.011154006979051504,
        "ci_upper": 0.5147774185174615
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.7130139544631349,
        "std": 0.06747397789678038,
        "ci_lower": 1.5823222114355966,
        "ci_upper": 1.847601969338742
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.7442689109913795,
        "std": 0.009241664746303604,
        "ci_lower": 0.7251501358671119,
        "ci_upper": 0.761449383300982
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.36770960310313033,
        "std": 0.00788472815964254,
        "ci_lower": 0.35239430988003023,
        "ci_upper": 0.38314872778866166
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.9190576699072293,
        "std": 0.0035325602232305557,
        "ci_lower": 0.9122234332110534,
        "ci_upper": 0.9262625683515355
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.9557520505296478,
        "std": 0.0029745271645779474,
        "ci_lower": 0.9499769328268811,
        "ci_upper": 0.9615860194553514
      }
    }
  },
  "('rj', 'SVM', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.19443779339975376,
        "std": 0.14845459219320264,
        "ci_lower": 0.003335652003864719,
        "ci_upper": 0.5041805507126885
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.6648379464377414,
        "std": 0.07241699437036545,
        "ci_lower": 1.5162595374508347,
        "ci_upper": 1.8014781450036526
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.20311051767605653,
        "std": 0.009289694439694214,
        "ci_lower": 0.1852461083457522,
        "ci_upper": 0.2215252524801213
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9294859347747847,
        "std": 0.006201965677222974,
        "ci_lower": 0.9168899837136605,
        "ci_upper": 0.9410749960568077
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5147304510275112,
        "std": 0.007580913214924842,
        "ci_lower": 0.500028275278001,
        "ci_upper": 0.5299911016465685
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.44073402591849825,
        "std": 0.01611089580028914,
        "ci_lower": 0.41066720490964587,
        "ci_upper": 0.4734203226633102
      }
    }
  },
  "('rj', 'SVM', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.3856916325461293,
        "std": 0.15846714534046194,
        "ci_lower": 0.09041917374041253,
        "ci_upper": 0.7120475068146469
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.1907638321134562,
        "std": 0.14810146746575306,
        "ci_lower": 0.0055769941187575655,
        "ci_upper": 0.49538372470732933
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.8585489920467448,
        "std": 0.0724378832369846,
        "ci_lower": 1.7170755234842412,
        "ci_upper": 1.9983269628932117
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.37666029247034816,
        "std": 0.009233233266493055,
        "ci_lower": 0.3586974381421082,
        "ci_upper": 0.3948202453623878
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.7069545591118944,
        "std": 0.009321708165598512,
        "ci_lower": 0.6885424578938504,
        "ci_upper": 0.7249843390804626
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.7809510087615986,
        "std": 0.006186137823780038,
        "ci_lower": 0.7684921648525961,
        "ci_upper": 0.7927626114444631
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.88762020114767,
        "std": 0.005654527490813776,
        "ci_lower": 0.8757242701119321,
        "ci_upper": 0.8979463715076924
      }
    }
  },
  "('rj', 'RoBERTa', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.16080462916307317,
        "std": 0.14722067619366264,
        "ci_lower": 0.0010458788728719818,
        "ci_upper": 0.4764818358128186
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.745568007212524,
        "std": 0.06792055905855651,
        "ci_lower": 1.6129234244661412,
        "ci_upper": 1.8767263226128974
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.25267547782162675,
        "std": 0.005311698756876643,
        "ci_lower": 0.24232447128697154,
        "ci_upper": 0.2629241308590286
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9112362152873048,
        "std": 0.006640927588194412,
        "ci_lower": 0.8981812752630254,
        "ci_upper": 0.9244784432941466
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5485556524949515,
        "std": 0.005745223926336024,
        "ci_lower": 0.5371286778460722,
        "ci_upper": 0.5594885970117478
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5056472763540157,
        "std": 0.009011179852351668,
        "ci_lower": 0.48881321171995373,
        "ci_upper": 0.5242668927095395
      }
    }
  },
  "('rj', 'RoBERTa', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.17176945327451942,
        "std": 0.14626057438561743,
        "ci_lower": 0.00441143792656531,
        "ci_upper": 0.4787851321341586
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.802183296856619,
        "std": 0.06781319437065665,
        "ci_lower": 1.6712674208531242,
        "ci_upper": 1.936697680858945
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.25349916069258205,
        "std": 0.005123173423289156,
        "ci_lower": 0.2433276118161419,
        "ci_upper": 0.26337715790446087
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9120694413493018,
        "std": 0.006455668612152672,
        "ci_lower": 0.8988054523291101,
        "ci_upper": 0.9241267052970766
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5484375524414495,
        "std": 0.005772071800483455,
        "ci_lower": 0.5369751278360793,
        "ci_upper": 0.5596574760372793
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5098630062352916,
        "std": 0.009044493507072553,
        "ci_lower": 0.4926983523237029,
        "ci_upper": 0.5286950918093128
      }
    }
  },
  "('rj', 'RoBERTa', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.24972864234683648,
        "std": 0.14764893441382537,
        "ci_lower": 0.016454105822673887,
        "ci_upper": 0.5440364364782752
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.44949292108390054,
        "std": 0.15983506177789672,
        "ci_lower": 0.13335833594386848,
        "ci_upper": 0.7727533040958386
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.26634786147677175,
        "std": 0.1516989269930123,
        "ci_lower": 0.015044483850792987,
        "ci_upper": 0.5660633305904369
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.7130139544631349,
        "std": 0.06747397789678038,
        "ci_lower": 1.5823222114355966,
        "ci_upper": 1.847601969338742
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.7442689109913795,
        "std": 0.009241664746303604,
        "ci_lower": 0.7251501358671119,
        "ci_upper": 0.761449383300982
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.36770960310313033,
        "std": 0.00788472815964254,
        "ci_lower": 0.35239430988003023,
        "ci_upper": 0.38314872778866166
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.9190576699072293,
        "std": 0.0035325602232305557,
        "ci_lower": 0.9122234332110534,
        "ci_upper": 0.9262625683515355
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.9557520505296478,
        "std": 0.0029745271645779474,
        "ci_lower": 0.9499769328268811,
        "ci_upper": 0.9615860194553514
      }
    }
  },
  "('rj', 'RoBERTa', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.15432504085441523,
        "std": 0.15767011865715305,
        "ci_lower": 0.001320911387688288,
        "ci_upper": 0.5062147683575746
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.6648379464377414,
        "std": 0.07241699437036545,
        "ci_lower": 1.5162595374508347,
        "ci_upper": 1.8014781450036526
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.20311051767605653,
        "std": 0.009289694439694214,
        "ci_lower": 0.1852461083457522,
        "ci_upper": 0.2215252524801213
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9294859347747847,
        "std": 0.006201965677222974,
        "ci_lower": 0.9168899837136605,
        "ci_upper": 0.9410749960568077
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5147304510275112,
        "std": 0.007580913214924842,
        "ci_lower": 0.500028275278001,
        "ci_upper": 0.5299911016465685
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.44073402591849825,
        "std": 0.01611089580028914,
        "ci_lower": 0.41066720490964587,
        "ci_upper": 0.4734203226633102
      }
    }
  },
  "('rj', 'RoBERTa', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.24972864234683648,
        "std": 0.14764893441382537,
        "ci_lower": 0.016454105822673887,
        "ci_upper": 0.5440364364782752
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.24968891018533934,
        "std": 0.14575216960985907,
        "ci_lower": 0.01641187704488334,
        "ci_upper": 0.5422367668283088
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.8585489920467448,
        "std": 0.0724378832369846,
        "ci_lower": 1.7170755234842412,
        "ci_upper": 1.9983269628932117
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.37666029247034816,
        "std": 0.009233233266493055,
        "ci_lower": 0.3586974381421082,
        "ci_upper": 0.3948202453623878
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.7069545591118944,
        "std": 0.009321708165598512,
        "ci_lower": 0.6885424578938504,
        "ci_upper": 0.7249843390804626
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.7809510087615986,
        "std": 0.006186137823780038,
        "ci_lower": 0.7684921648525961,
        "ci_upper": 0.7927626114444631
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.88762020114767,
        "std": 0.005654527490813776,
        "ci_lower": 0.8757242701119321,
        "ci_upper": 0.8979463715076924
      }
    }
  }
}