{
  "('rj', 'LogReg', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.2007428247690391,
        "std": 0.1419136575373543,
        "ci_lower": 0.006926852305398498,
        "ci_upper": 0.48476620088036415
      }
    },
    "quality_estimates": {}
  },
  "('rj', 'LogReg', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.21529024460925508,
        "std": 0.1464430451153813,
        "ci_lower": 0.009876260520898399,
        "ci_upper": 0.523888684912587
      }
    },
    "quality_estimates": {}
  },
  "('rj', 'LogReg', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.5159537162272743,
        "std": 0.15986205888447794,
        "ci_lower": 0.1955641140928672,
        "ci_upper": 0.8410852710675281
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.2383847464359572,
        "std": 0.14539954771406352,
        "ci_lower": 0.0189229446208659,
        "ci_upper": 0.5359992166283256
      }
    },
    "quality_estimates": {}
  },
  "('rj', 'LogReg', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.20938851084172513,
        "std": 0.14344325293783278,
        "ci_lower": 0.010572939026460006,
        "ci_upper": 0.5006631085556161
      }
    },
    "quality_estimates": {}
  },
  "('rj', 'LogReg', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.17077425134380586,
        "std": 0.14328769220801765,
        "ci_lower": 0.0020748841464047992,
        "ci_upper": 0.4807574402791161
      }
    },
    "quality_estimates": {}
  },
  "('rj', 'SVM', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.18033874779585024,
        "std": 0.1474237915306935,
        "ci_lower": 0.002956998221329701,
        "ci_upper": 0.4955377615211659
      }
    },
    "quality_estimates": {}
  },
  "('rj', 'SVM', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.18270394377600077,
        "std": 0.1456849307030449,
        "ci_lower": 0.0003558235853750253,
        "ci_upper": 0.48469256716926057
      }
    },
    "quality_estimates": {}
  },
  "('rj', 'SVM', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.5159537162272743,
        "std": 0.15986205888447794,
        "ci_lower": 0.1955641140928672,
        "ci_upper": 0.8410852710675281
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.20963728507391055,
        "std": 0.14695304451878538,
        "ci_lower": 0.011154006979051504,
        "ci_upper": 0.5147774185174615
      }
    },
    "quality_estimates": {}
  },
  "('rj', 'SVM', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.19443779339975376,
        "std": 0.14845459219320264,
        "ci_lower": 0.003335652003864719,
        "ci_upper": 0.5041805507126885
      }
    },
    "quality_estimates": {}
  },
  "('rj', 'SVM', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.3856916325461293,
        "std": 0.15846714534046194,
        "ci_lower": 0.09041917374041253,
        "ci_upper": 0.7120475068146469
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.1907638321134562,
        "std": 0.14810146746575306,
        "ci_lower": 0.0055769941187575655,
        "ci_upper": 0.49538372470732933
      }
    },
    "quality_estimates": {}
  },
  "('rj', 'RoBERTa', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.16080462916307317,
        "std": 0.14722067619366264,
        "ci_lower": 0.0010458788728719818,
        "ci_upper": 0.4764818358128186
      }
    },
    "quality_estimates": {}
  },
  "('rj', 'RoBERTa', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.17176945327451942,
        "std": 0.14626057438561743,
        "ci_lower": 0.00441143792656531,
        "ci_upper": 0.4787851321341586
      }
    },
    "quality_estimates": {}
  },
  "('rj', 'RoBERTa', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.24972864234683648,
        "std": 0.14764893441382537,
        "ci_lower": 0.016454105822673887,
        "ci_upper": 0.5440364364782752
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.44949292108390054,
        "std": 0.15983506177789672,
        "ci_lower": 0.13335833594386848,
        "ci_upper": 0.7727533040958386
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.26634786147677175,
        "std": 0.1516989269930123,
        "ci_lower": 0.015044483850792987,
        "ci_upper": 0.5660633305904369
      }
    },
    "quality_estimates": {}
  },
  "('rj', 'RoBERTa', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.15432504085441523,
        "std": 0.15767011865715305,
        "ci_lower": 0.001320911387688288,
        "ci_upper": 0.5062147683575746
      }
    },
    "quality_estimates": {}
  },
  "('rj', 'RoBERTa', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.24972864234683648,
        "std": 0.14764893441382537,
        "ci_lower": 0.016454105822673887,
        "ci_upper": 0.5440364364782752
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.24968891018533934,
        "std": 0.14575216960985907,
        "ci_lower": 0.01641187704488334,
        "ci_upper": 0.5422367668283088
      }
    },
    "quality_estimates": {}
  }
}