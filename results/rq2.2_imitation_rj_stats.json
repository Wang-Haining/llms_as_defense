{
  "('rj', 'LogReg', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.44949292108390054,
        "std": 0.15983506177789672,
        "ci_lower": 0.13335833594386848,
        "ci_upper": 0.7727533040958386
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.27876194837209767,
        "std": 0.14589152242779346,
        "ci_lower": 0.023455495907836837,
        "ci_upper": 0.5705902241859974
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 2.4692918230971976,
        "std": 0.059178547415854946,
        "ci_lower": 2.3538064206087195,
        "ci_upper": 2.584383066606549
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.313898912963573,
        "std": 0.006575090312079213,
        "ci_lower": 0.3009117110846271,
        "ci_upper": 0.3266944347532701
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8374397695750563,
        "std": 0.007766861914937587,
        "ci_lower": 0.8212740306059915,
        "ci_upper": 0.8516466516303305
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6822767476610718,
        "std": 0.006543864659061057,
        "ci_lower": 0.6698912254607087,
        "ci_upper": 0.6955425265063182
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.7181199456274917,
        "std": 0.010961838568218217,
        "ci_lower": 0.6977095931355276,
        "ci_upper": 0.7406497114928594
      }
    }
  },
  "('rj', 'LogReg', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.217595337815895,
        "std": 0.1483885683234476,
        "ci_lower": 0.00783913779920457,
        "ci_upper": 0.520128821194825
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.959866043340016,
        "std": 0.06734296091762808,
        "ci_lower": 1.8300414147910553,
        "ci_upper": 2.094974595380942
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.19655400868884376,
        "std": 0.004767525016212022,
        "ci_lower": 0.18679367731696148,
        "ci_upper": 0.20547806922701864
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.929605975477892,
        "std": 0.006051767740532607,
        "ci_lower": 0.9168944569830733,
        "ci_upper": 0.9402526910704313
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5203191069894653,
        "std": 0.005779952989758362,
        "ci_lower": 0.5088946007190613,
        "ci_upper": 0.5314224114059358
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.4819897278126944,
        "std": 0.012886341042289341,
        "ci_lower": 0.45682378065257356,
        "ci_upper": 0.5078248425572786
      }
    }
  },
  "('rj', 'LogReg', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.1780348371315114,
        "std": 0.14514706586741782,
        "ci_lower": 0.0016733798401078707,
        "ci_upper": 0.4875319805361465
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 2.472617204575433,
        "std": 0.05678521089905815,
        "ci_lower": 2.3657338634606515,
        "ci_upper": 2.5862371624956615
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.22161408389210846,
        "std": 0.004948925803160974,
        "ci_lower": 0.21180643146384817,
        "ci_upper": 0.23126282834169642
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9386610691038493,
        "std": 0.005370843930859416,
        "ci_lower": 0.9278666969211877,
        "ci_upper": 0.948735577522321
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5184154385531335,
        "std": 0.005769443523635375,
        "ci_lower": 0.5067670204034096,
        "ci_upper": 0.5294106085451943
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.4868080543934054,
        "std": 0.011818020934648376,
        "ci_lower": 0.463299022683226,
        "ci_upper": 0.5103506265539164
      }
    }
  },
  "('rj', 'LogReg', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.16038995370850012,
        "std": 0.1455570200292394,
        "ci_lower": 0.0024159752419655412,
        "ci_upper": 0.47380672641405136
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 2.3034594920380895,
        "std": 0.06597327032749403,
        "ci_lower": 2.176581635551449,
        "ci_upper": 2.43327327619151
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.19602531545118496,
        "std": 0.004866553103469658,
        "ci_lower": 0.1863823183961383,
        "ci_upper": 0.2057785421098518
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9298352479261706,
        "std": 0.00606138836639382,
        "ci_lower": 0.9184302973652204,
        "ci_upper": 0.9417486149458463
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.526348466217716,
        "std": 0.005657860744288492,
        "ci_lower": 0.5150353264408882,
        "ci_upper": 0.537234914337933
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.49853589455820946,
        "std": 0.012532288943215772,
        "ci_lower": 0.47450528945455234,
        "ci_upper": 0.5229771477971273
      }
    }
  },
  "('rj', 'LogReg', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.4175914408028651,
        "std": 0.15604192512739995,
        "ci_lower": 0.10180037803387029,
        "ci_upper": 0.7192677877613619
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.1872814072077723,
        "std": 0.14598058626085178,
        "ci_lower": 0.00247887590136409,
        "ci_upper": 0.49758657948343954
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.829494967842259,
        "std": 0.06289941195233763,
        "ci_lower": 1.7122412712091581,
        "ci_upper": 1.9597403449658226
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.23869664893145556,
        "std": 0.005474657529933237,
        "ci_lower": 0.2278175005899385,
        "ci_upper": 0.24905417269513913
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8876192784477582,
        "std": 0.007698654812310602,
        "ci_lower": 0.8723626672886144,
        "ci_upper": 0.9022247550957129
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6075343589644693,
        "std": 0.006085916932651871,
        "ci_lower": 0.5951554419865532,
        "ci_upper": 0.6193047274121403
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.6615786772903643,
        "std": 0.010728494236267554,
        "ci_lower": 0.6415586592237212,
        "ci_upper": 0.6837826986956782
      }
    }
  },
  "('rj', 'SVM', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.18406466084292455,
        "std": 0.14409271729224357,
        "ci_lower": 0.006106066793387239,
        "ci_upper": 0.48671811242924584
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 2.4692918230971976,
        "std": 0.059178547415854946,
        "ci_lower": 2.3538064206087195,
        "ci_upper": 2.584383066606549
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.313898912963573,
        "std": 0.006575090312079213,
        "ci_lower": 0.3009117110846271,
        "ci_upper": 0.3266944347532701
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8374397695750563,
        "std": 0.007766861914937587,
        "ci_lower": 0.8212740306059915,
        "ci_upper": 0.8516466516303305
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6822767476610718,
        "std": 0.006543864659061057,
        "ci_lower": 0.6698912254607087,
        "ci_upper": 0.6955425265063182
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.7181199456274917,
        "std": 0.010961838568218217,
        "ci_lower": 0.6977095931355276,
        "ci_upper": 0.7406497114928594
      }
    }
  },
  "('rj', 'SVM', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.19536979657354317,
        "std": 0.14562571800908192,
        "ci_lower": 0.00024836514235504535,
        "ci_upper": 0.49376218433730135
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.959866043340016,
        "std": 0.06734296091762808,
        "ci_lower": 1.8300414147910553,
        "ci_upper": 2.094974595380942
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.19655400868884376,
        "std": 0.004767525016212022,
        "ci_lower": 0.18679367731696148,
        "ci_upper": 0.20547806922701864
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.929605975477892,
        "std": 0.006051767740532607,
        "ci_lower": 0.9168944569830733,
        "ci_upper": 0.9402526910704313
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5203191069894653,
        "std": 0.005779952989758362,
        "ci_lower": 0.5088946007190613,
        "ci_upper": 0.5314224114059358
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.4819897278126944,
        "std": 0.012886341042289341,
        "ci_lower": 0.45682378065257356,
        "ci_upper": 0.5078248425572786
      }
    }
  },
  "('rj', 'SVM', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.18622036131845526,
        "std": 0.15108982637314794,
        "ci_lower": 0.007838737416424319,
        "ci_upper": 0.5097808923248834
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 2.472617204575433,
        "std": 0.05678521089905815,
        "ci_lower": 2.3657338634606515,
        "ci_upper": 2.5862371624956615
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.22161408389210846,
        "std": 0.004948925803160974,
        "ci_lower": 0.21180643146384817,
        "ci_upper": 0.23126282834169642
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9386610691038493,
        "std": 0.005370843930859416,
        "ci_lower": 0.9278666969211877,
        "ci_upper": 0.948735577522321
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5184154385531335,
        "std": 0.005769443523635375,
        "ci_lower": 0.5067670204034096,
        "ci_upper": 0.5294106085451943
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.4868080543934054,
        "std": 0.011818020934648376,
        "ci_lower": 0.463299022683226,
        "ci_upper": 0.5103506265539164
      }
    }
  },
  "('rj', 'SVM', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.24972864234683648,
        "std": 0.14764893441382537,
        "ci_lower": 0.016454105822673887,
        "ci_upper": 0.5440364364782752
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.17388780548473873,
        "std": 0.14920928162150904,
        "ci_lower": 0.0034358122916234627,
        "ci_upper": 0.4989377161725045
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 2.3034594920380895,
        "std": 0.06597327032749403,
        "ci_lower": 2.176581635551449,
        "ci_upper": 2.43327327619151
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.19602531545118496,
        "std": 0.004866553103469658,
        "ci_lower": 0.1863823183961383,
        "ci_upper": 0.2057785421098518
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9298352479261706,
        "std": 0.00606138836639382,
        "ci_lower": 0.9184302973652204,
        "ci_upper": 0.9417486149458463
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.526348466217716,
        "std": 0.005657860744288492,
        "ci_lower": 0.5150353264408882,
        "ci_upper": 0.537234914337933
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.49853589455820946,
        "std": 0.012532288943215772,
        "ci_lower": 0.47450528945455234,
        "ci_upper": 0.5229771477971273
      }
    }
  },
  "('rj', 'SVM', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.17740920461760823,
        "std": 0.15181593497243423,
        "ci_lower": 0.00299368566689773,
        "ci_upper": 0.5113715942738863
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.829494967842259,
        "std": 0.06289941195233763,
        "ci_lower": 1.7122412712091581,
        "ci_upper": 1.9597403449658226
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.23869664893145556,
        "std": 0.005474657529933237,
        "ci_lower": 0.2278175005899385,
        "ci_upper": 0.24905417269513913
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8876192784477582,
        "std": 0.007698654812310602,
        "ci_lower": 0.8723626672886144,
        "ci_upper": 0.9022247550957129
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6075343589644693,
        "std": 0.006085916932651871,
        "ci_lower": 0.5951554419865532,
        "ci_upper": 0.6193047274121403
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.6615786772903643,
        "std": 0.010728494236267554,
        "ci_lower": 0.6415586592237212,
        "ci_upper": 0.6837826986956782
      }
    }
  },
  "('rj', 'RoBERTa', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.1942306271119301,
        "std": 0.14515751926605433,
        "ci_lower": 0.000919566566375537,
        "ci_upper": 0.5003345431536633
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 2.4692918230971976,
        "std": 0.059178547415854946,
        "ci_lower": 2.3538064206087195,
        "ci_upper": 2.584383066606549
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.313898912963573,
        "std": 0.006575090312079213,
        "ci_lower": 0.3009117110846271,
        "ci_upper": 0.3266944347532701
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8374397695750563,
        "std": 0.007766861914937587,
        "ci_lower": 0.8212740306059915,
        "ci_upper": 0.8516466516303305
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6822767476610718,
        "std": 0.006543864659061057,
        "ci_lower": 0.6698912254607087,
        "ci_upper": 0.6955425265063182
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.7181199456274917,
        "std": 0.010961838568218217,
        "ci_lower": 0.6977095931355276,
        "ci_upper": 0.7406497114928594
      }
    }
  },
  "('rj', 'RoBERTa', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.16310291222119452,
        "std": 0.1498559079986694,
        "ci_lower": 0.005502102262939394,
        "ci_upper": 0.4937262753105312
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.959866043340016,
        "std": 0.06734296091762808,
        "ci_lower": 1.8300414147910553,
        "ci_upper": 2.094974595380942
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.19655400868884376,
        "std": 0.004767525016212022,
        "ci_lower": 0.18679367731696148,
        "ci_upper": 0.20547806922701864
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.929605975477892,
        "std": 0.006051767740532607,
        "ci_lower": 0.9168944569830733,
        "ci_upper": 0.9402526910704313
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5203191069894653,
        "std": 0.005779952989758362,
        "ci_lower": 0.5088946007190613,
        "ci_upper": 0.5314224114059358
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.4819897278126944,
        "std": 0.012886341042289341,
        "ci_lower": 0.45682378065257356,
        "ci_upper": 0.5078248425572786
      }
    }
  },
  "('rj', 'RoBERTa', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3856916325461293,
        "std": 0.15846714534046194,
        "ci_lower": 0.09041917374041253,
        "ci_upper": 0.7120475068146469
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.18909872752177143,
        "std": 0.14484064332818278,
        "ci_lower": 0.005243761123877899,
        "ci_upper": 0.49486844797793067
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 2.472617204575433,
        "std": 0.05678521089905815,
        "ci_lower": 2.3657338634606515,
        "ci_upper": 2.5862371624956615
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.22161408389210846,
        "std": 0.004948925803160974,
        "ci_lower": 0.21180643146384817,
        "ci_upper": 0.23126282834169642
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9386610691038493,
        "std": 0.005370843930859416,
        "ci_lower": 0.9278666969211877,
        "ci_upper": 0.948735577522321
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5184154385531335,
        "std": 0.005769443523635375,
        "ci_lower": 0.5067670204034096,
        "ci_upper": 0.5294106085451943
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.4868080543934054,
        "std": 0.011818020934648376,
        "ci_lower": 0.463299022683226,
        "ci_upper": 0.5103506265539164
      }
    }
  },
  "('rj', 'RoBERTa', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.1594456536483608,
        "std": 0.1508192346642969,
        "ci_lower": 0.0007698381818044236,
        "ci_upper": 0.48477956302489705
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 2.3034594920380895,
        "std": 0.06597327032749403,
        "ci_lower": 2.176581635551449,
        "ci_upper": 2.43327327619151
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.19602531545118496,
        "std": 0.004866553103469658,
        "ci_lower": 0.1863823183961383,
        "ci_upper": 0.2057785421098518
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9298352479261706,
        "std": 0.00606138836639382,
        "ci_lower": 0.9184302973652204,
        "ci_upper": 0.9417486149458463
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.526348466217716,
        "std": 0.005657860744288492,
        "ci_lower": 0.5150353264408882,
        "ci_upper": 0.537234914337933
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.49853589455820946,
        "std": 0.012532288943215772,
        "ci_lower": 0.47450528945455234,
        "ci_upper": 0.5229771477971273
      }
    }
  },
  "('rj', 'RoBERTa', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.1838935681077677,
        "std": 0.1437982834535658,
        "ci_lower": 0.0006180127110772529,
        "ci_upper": 0.47424422329160215
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.829494967842259,
        "std": 0.06289941195233763,
        "ci_lower": 1.7122412712091581,
        "ci_upper": 1.9597403449658226
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.23869664893145556,
        "std": 0.005474657529933237,
        "ci_lower": 0.2278175005899385,
        "ci_upper": 0.24905417269513913
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8876192784477582,
        "std": 0.007698654812310602,
        "ci_lower": 0.8723626672886144,
        "ci_upper": 0.9022247550957129
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6075343589644693,
        "std": 0.006085916932651871,
        "ci_lower": 0.5951554419865532,
        "ci_upper": 0.6193047274121403
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.6615786772903643,
        "std": 0.010728494236267554,
        "ci_lower": 0.6415586592237212,
        "ci_upper": 0.6837826986956782
      }
    }
  }
}