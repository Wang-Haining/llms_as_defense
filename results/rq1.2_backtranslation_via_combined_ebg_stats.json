{
  "('ebg', 'LogReg', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.42645454313287656,
        "std": 0.1559938750461524,
        "ci_lower": 0.137918937581085,
        "ci_upper": 0.748278251934814
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.66310346446024,
        "std": 0.15320939802705993,
        "ci_lower": 0.3558544575822297,
        "ci_upper": 0.9492625627773653
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.35802564153176414,
        "std": 0.15453577212705627,
        "ci_lower": 0.07237633344886635,
        "ci_upper": 0.6660654923135063
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.6037420530661968,
        "std": 0.05815294939516305,
        "ci_lower": 2.488762642488344,
        "ci_upper": 2.716982175021956
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.6439009719525655,
        "std": 0.008038232846893263,
        "ci_lower": 0.6278452076435808,
        "ci_upper": 0.65956249162625
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.4602207029427754,
        "std": 0.006146104390736708,
        "ci_lower": 0.44809890646472506,
        "ci_upper": 0.47221685717210465
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.8642777969686875,
        "std": 0.0038790653612387057,
        "ci_lower": 0.8563674498317486,
        "ci_upper": 0.8714546255469894
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.959149233740705,
        "std": 0.0019422843443493447,
        "ci_lower": 0.9553669527690178,
        "ci_upper": 0.9629519203201423
      }
    }
  },
  "('ebg', 'LogReg', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.21696600533925262,
        "std": 0.15097396474024674,
        "ci_lower": 0.006336258248271914,
        "ci_upper": 0.5254646587636552
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.14960098660912624,
        "std": 0.15553132152961344,
        "ci_lower": 0.00024316744168733715,
        "ci_upper": 0.48868927028043724
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.5450005093990744,
        "std": 0.05843365656353738,
        "ci_lower": 2.424130140339718,
        "ci_upper": 2.655227109506569
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.2122393914278428,
        "std": 0.00662847122105458,
        "ci_lower": 0.19899787846783482,
        "ci_upper": 0.22470661308136625
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9228433129114691,
        "std": 0.004910360466493343,
        "ci_lower": 0.9126901239687134,
        "ci_upper": 0.9320936973747901
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5069763118633451,
        "std": 0.008993325858583553,
        "ci_lower": 0.4896154428254233,
        "ci_upper": 0.5251945835541864
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.13163104567040407,
        "std": 0.012957595634192787,
        "ci_lower": 0.10757775005006318,
        "ci_upper": 0.15722261107958715
      }
    }
  },
  "('ebg', 'LogReg', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.3411703633764144,
        "std": 0.15785283117098922,
        "ci_lower": 0.05574837730405922,
        "ci_upper": 0.6480791954190802
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.5261050989600696,
        "std": 0.16056385253809366,
        "ci_lower": 0.2114712343774928,
        "ci_upper": 0.848279573197231
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.2760403483135284,
        "std": 0.14892132862801094,
        "ci_lower": 0.012428418071539587,
        "ci_upper": 0.5735393116138581
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.458697300563387,
        "std": 0.05540567161270269,
        "ci_lower": 2.3512808408691757,
        "ci_upper": 2.565717236189713
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.3750294303991741,
        "std": 0.006522011499087035,
        "ci_lower": 0.36235597947897097,
        "ci_upper": 0.38761696322504113
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.701751648162747,
        "std": 0.006131584544084197,
        "ci_lower": 0.6904248149690942,
        "ci_upper": 0.7141133565904557
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.761940533845676,
        "std": 0.00408607575495042,
        "ci_lower": 0.7540807211838872,
        "ci_upper": 0.770251267453844
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.9323384283525241,
        "std": 0.002487664165292224,
        "ci_lower": 0.927597774661828,
        "ci_upper": 0.937230145094151
      }
    }
  },
  "('ebg', 'LogReg', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.1624620801804,
        "std": 0.14658379624989498,
        "ci_lower": 0.0013635951412811836,
        "ci_upper": 0.47563220964679537
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.1548730598755117,
        "std": 0.14397627795001705,
        "ci_lower": 0.002057353731817245,
        "ci_upper": 0.4622148859313543
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.574920336266988,
        "std": 0.05976524260296353,
        "ci_lower": 2.4532423270682306,
        "ci_upper": 2.6906421788230697
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.24030359236754495,
        "std": 0.00802355392017397,
        "ci_lower": 0.22532389057652066,
        "ci_upper": 0.25658445082132136
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9150051480884224,
        "std": 0.005146683960027426,
        "ci_lower": 0.9047994997199387,
        "ci_upper": 0.9248123227291901
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5135166781291627,
        "std": 0.007404995095778761,
        "ci_lower": 0.4988696106706105,
        "ci_upper": 0.5279256526073253
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.13174759341501846,
        "std": 0.012710441988790787,
        "ci_lower": 0.1083377109276566,
        "ci_upper": 0.15822838904785957
      }
    }
  },
  "('ebg', 'LogReg', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.1510426018340253,
        "std": 0.14733916347181156,
        "ci_lower": 0.0019619339546042077,
        "ci_upper": 0.47508911193627734
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.559946393119946,
        "std": 0.059241397223317324,
        "ci_lower": 2.4403633152979802,
        "ci_upper": 2.672912347018219
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.2690769212887084,
        "std": 0.010973283196711382,
        "ci_lower": 0.24748196941823944,
        "ci_upper": 0.2910212805832286
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8874485292478191,
        "std": 0.006264761010605969,
        "ci_lower": 0.8753488074857576,
        "ci_upper": 0.8996293200251816
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5204679297686228,
        "std": 0.008737288730989888,
        "ci_lower": 0.503860931870148,
        "ci_upper": 0.538064252428384
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.15159324890667325,
        "std": 0.014557594995909142,
        "ci_lower": 0.12444599388062429,
        "ci_upper": 0.1810020663128352
      }
    }
  },
  "('ebg', 'SVM', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.5411251329825744,
        "std": 0.16004393772281902,
        "ci_lower": 0.23885691908773168,
        "ci_upper": 0.8726573184758236
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.6904106369227356,
        "std": 0.14947807036564376,
        "ci_lower": 0.37656871402758596,
        "ci_upper": 0.9506333544051696
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.2733478299446068,
        "std": 0.14933276426736133,
        "ci_lower": 0.02551392827291765,
        "ci_upper": 0.568367957186746
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.6037420530661968,
        "std": 0.05815294939516305,
        "ci_lower": 2.488762642488344,
        "ci_upper": 2.716982175021956
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.6439009719525655,
        "std": 0.008038232846893263,
        "ci_lower": 0.6278452076435808,
        "ci_upper": 0.65956249162625
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.4602207029427754,
        "std": 0.006146104390736708,
        "ci_lower": 0.44809890646472506,
        "ci_upper": 0.47221685717210465
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.8642777969686875,
        "std": 0.0038790653612387057,
        "ci_lower": 0.8563674498317486,
        "ci_upper": 0.8714546255469894
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.959149233740705,
        "std": 0.0019422843443493447,
        "ci_lower": 0.9553669527690178,
        "ci_upper": 0.9629519203201423
      }
    }
  },
  "('ebg', 'SVM', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.18400042185528304,
        "std": 0.15142585952723722,
        "ci_lower": 0.005111049814274888,
        "ci_upper": 0.5046177424211228
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.15184424743603453,
        "std": 0.1412606945428983,
        "ci_lower": 0.0007415084497575047,
        "ci_upper": 0.4647503643526372
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.5450005093990744,
        "std": 0.05843365656353738,
        "ci_lower": 2.424130140339718,
        "ci_upper": 2.655227109506569
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.2122393914278428,
        "std": 0.00662847122105458,
        "ci_lower": 0.19899787846783482,
        "ci_upper": 0.22470661308136625
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9228433129114691,
        "std": 0.004910360466493343,
        "ci_lower": 0.9126901239687134,
        "ci_upper": 0.9320936973747901
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5069763118633451,
        "std": 0.008993325858583553,
        "ci_lower": 0.4896154428254233,
        "ci_upper": 0.5251945835541864
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.13163104567040407,
        "std": 0.012957595634192787,
        "ci_lower": 0.10757775005006318,
        "ci_upper": 0.15722261107958715
      }
    }
  },
  "('ebg', 'SVM', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.3022908454126393,
        "std": 0.15290620134623778,
        "ci_lower": 0.04237316748200466,
        "ci_upper": 0.6152740587683058
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.5411251329825744,
        "std": 0.16004393772281902,
        "ci_lower": 0.23885691908773168,
        "ci_upper": 0.8726573184758236
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.22784670983480404,
        "std": 0.14833596322357406,
        "ci_lower": 0.010813347722735771,
        "ci_upper": 0.528803232283978
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.458697300563387,
        "std": 0.05540567161270269,
        "ci_lower": 2.3512808408691757,
        "ci_upper": 2.565717236189713
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.3750294303991741,
        "std": 0.006522011499087035,
        "ci_lower": 0.36235597947897097,
        "ci_upper": 0.38761696322504113
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.701751648162747,
        "std": 0.006131584544084197,
        "ci_lower": 0.6904248149690942,
        "ci_upper": 0.7141133565904557
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.761940533845676,
        "std": 0.00408607575495042,
        "ci_lower": 0.7540807211838872,
        "ci_upper": 0.770251267453844
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.9323384283525241,
        "std": 0.002487664165292224,
        "ci_lower": 0.927597774661828,
        "ci_upper": 0.937230145094151
      }
    }
  },
  "('ebg', 'SVM', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.18400042185528304,
        "std": 0.15142585952723722,
        "ci_lower": 0.005111049814274888,
        "ci_upper": 0.5046177424211228
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.15918686943268875,
        "std": 0.15142884657400907,
        "ci_lower": 0.0009122990389077642,
        "ci_upper": 0.489765877605012
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.574920336266988,
        "std": 0.05976524260296353,
        "ci_lower": 2.4532423270682306,
        "ci_upper": 2.6906421788230697
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.24030359236754495,
        "std": 0.00802355392017397,
        "ci_lower": 0.22532389057652066,
        "ci_upper": 0.25658445082132136
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9150051480884224,
        "std": 0.005146683960027426,
        "ci_lower": 0.9047994997199387,
        "ci_upper": 0.9248123227291901
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5135166781291627,
        "std": 0.007404995095778761,
        "ci_lower": 0.4988696106706105,
        "ci_upper": 0.5279256526073253
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.13174759341501846,
        "std": 0.012710441988790787,
        "ci_lower": 0.1083377109276566,
        "ci_upper": 0.15822838904785957
      }
    }
  },
  "('ebg', 'SVM', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.18400042185528304,
        "std": 0.15142585952723722,
        "ci_lower": 0.005111049814274888,
        "ci_upper": 0.5046177424211228
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.1537087296964105,
        "std": 0.14675292706272422,
        "ci_lower": 0.0005655160576055524,
        "ci_upper": 0.47429044858796554
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.559946393119946,
        "std": 0.059241397223317324,
        "ci_lower": 2.4403633152979802,
        "ci_upper": 2.672912347018219
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.2690769212887084,
        "std": 0.010973283196711382,
        "ci_lower": 0.24748196941823944,
        "ci_upper": 0.2910212805832286
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8874485292478191,
        "std": 0.006264761010605969,
        "ci_lower": 0.8753488074857576,
        "ci_upper": 0.8996293200251816
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5204679297686228,
        "std": 0.008737288730989888,
        "ci_lower": 0.503860931870148,
        "ci_upper": 0.538064252428384
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.15159324890667325,
        "std": 0.014557594995909142,
        "ci_lower": 0.12444599388062429,
        "ci_upper": 0.1810020663128352
      }
    }
  },
  "('ebg', 'RoBERTa', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.5673920512467088,
        "std": 0.16039584259874629,
        "ci_lower": 0.24297131650077797,
        "ci_upper": 0.8857177907693298
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.66310346446024,
        "std": 0.15320939802705993,
        "ci_lower": 0.3558544575822297,
        "ci_upper": 0.9492625627773653
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.5357411644511548,
        "std": 0.15687062069113888,
        "ci_lower": 0.22013472756198654,
        "ci_upper": 0.8493385818156233
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.6037420530661968,
        "std": 0.05815294939516305,
        "ci_lower": 2.488762642488344,
        "ci_upper": 2.716982175021956
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.6439009719525655,
        "std": 0.008038232846893263,
        "ci_lower": 0.6278452076435808,
        "ci_upper": 0.65956249162625
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.4602207029427754,
        "std": 0.006146104390736708,
        "ci_lower": 0.44809890646472506,
        "ci_upper": 0.47221685717210465
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.8642777969686875,
        "std": 0.0038790653612387057,
        "ci_lower": 0.8563674498317486,
        "ci_upper": 0.8714546255469894
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.959149233740705,
        "std": 0.0019422843443493447,
        "ci_lower": 0.9553669527690178,
        "ci_upper": 0.9629519203201423
      }
    }
  },
  "('ebg', 'RoBERTa', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.14742018697887352,
        "std": 0.15451260964187083,
        "ci_lower": 0.0014616550101322924,
        "ci_upper": 0.48541724986528206
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.5450005093990744,
        "std": 0.05843365656353738,
        "ci_lower": 2.424130140339718,
        "ci_upper": 2.655227109506569
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.2122393914278428,
        "std": 0.00662847122105458,
        "ci_lower": 0.19899787846783482,
        "ci_upper": 0.22470661308136625
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9228433129114691,
        "std": 0.004910360466493343,
        "ci_lower": 0.9126901239687134,
        "ci_upper": 0.9320936973747901
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5069763118633451,
        "std": 0.008993325858583553,
        "ci_lower": 0.4896154428254233,
        "ci_upper": 0.5251945835541864
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.13163104567040407,
        "std": 0.012957595634192787,
        "ci_lower": 0.10757775005006318,
        "ci_upper": 0.15722261107958715
      }
    }
  },
  "('ebg', 'RoBERTa', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.5261050989600696,
        "std": 0.16056385253809366,
        "ci_lower": 0.2114712343774928,
        "ci_upper": 0.848279573197231
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.66310346446024,
        "std": 0.15320939802705993,
        "ci_lower": 0.3558544575822297,
        "ci_upper": 0.9492625627773653
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.5036442543942096,
        "std": 0.1559345353549894,
        "ci_lower": 0.19863160621233275,
        "ci_upper": 0.814869689241982
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.458697300563387,
        "std": 0.05540567161270269,
        "ci_lower": 2.3512808408691757,
        "ci_upper": 2.565717236189713
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.3750294303991741,
        "std": 0.006522011499087035,
        "ci_lower": 0.36235597947897097,
        "ci_upper": 0.38761696322504113
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.701751648162747,
        "std": 0.006131584544084197,
        "ci_lower": 0.6904248149690942,
        "ci_upper": 0.7141133565904557
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.761940533845676,
        "std": 0.00408607575495042,
        "ci_lower": 0.7540807211838872,
        "ci_upper": 0.770251267453844
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.9323384283525241,
        "std": 0.002487664165292224,
        "ci_lower": 0.927597774661828,
        "ci_upper": 0.937230145094151
      }
    }
  },
  "('ebg', 'RoBERTa', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.13433075910336884,
        "std": 0.14193658368460427,
        "ci_lower": 0.00017578252063913738,
        "ci_upper": 0.4351320540848377
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.574920336266988,
        "std": 0.05976524260296353,
        "ci_lower": 2.4532423270682306,
        "ci_upper": 2.6906421788230697
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.24030359236754495,
        "std": 0.00802355392017397,
        "ci_lower": 0.22532389057652066,
        "ci_upper": 0.25658445082132136
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9150051480884224,
        "std": 0.005146683960027426,
        "ci_lower": 0.9047994997199387,
        "ci_upper": 0.9248123227291901
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5135166781291627,
        "std": 0.007404995095778761,
        "ci_lower": 0.4988696106706105,
        "ci_upper": 0.5279256526073253
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.13174759341501846,
        "std": 0.012710441988790787,
        "ci_lower": 0.1083377109276566,
        "ci_upper": 0.15822838904785957
      }
    }
  },
  "('ebg', 'RoBERTa', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.21696600533925262,
        "std": 0.15097396474024674,
        "ci_lower": 0.006336258248271914,
        "ci_upper": 0.5254646587636552
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.14677171581326456,
        "std": 0.15782272097425273,
        "ci_lower": 0.0015818590053975066,
        "ci_upper": 0.5028223588365823
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.559946393119946,
        "std": 0.059241397223317324,
        "ci_lower": 2.4403633152979802,
        "ci_upper": 2.672912347018219
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.2690769212887084,
        "std": 0.010973283196711382,
        "ci_lower": 0.24748196941823944,
        "ci_upper": 0.2910212805832286
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8874485292478191,
        "std": 0.006264761010605969,
        "ci_lower": 0.8753488074857576,
        "ci_upper": 0.8996293200251816
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5204679297686228,
        "std": 0.008737288730989888,
        "ci_lower": 0.503860931870148,
        "ci_upper": 0.538064252428384
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.15159324890667325,
        "std": 0.014557594995909142,
        "ci_lower": 0.12444599388062429,
        "ci_upper": 0.1810020663128352
      }
    }
  }
}