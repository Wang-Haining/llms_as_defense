{
  "('ebg', 'LogReg', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.21696600533925262,
        "std": 0.15097396474024674,
        "ci_lower": 0.006336258248271914,
        "ci_upper": 0.5254646587636552
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.5261050989600696,
        "std": 0.16056385253809366,
        "ci_lower": 0.2114712343774928,
        "ci_upper": 0.848279573197231
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.25006927737919193,
        "std": 0.15016257371865016,
        "ci_lower": 0.015394662687275703,
        "ci_upper": 0.5594968148482071
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.2672167273541968,
        "std": 0.052113584662429435,
        "ci_lower": 2.162574149982492,
        "ci_upper": 2.366534243948899
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.22617652963788687,
        "std": 0.0034030299551289413,
        "ci_lower": 0.21953221165364206,
        "ci_upper": 0.23265629450925393
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8144329284575357,
        "std": 0.005884997076708827,
        "ci_lower": 0.8032417694992225,
        "ci_upper": 0.8263001082964285
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6771834718455249,
        "std": 0.003631472882798456,
        "ci_lower": 0.670408316436632,
        "ci_upper": 0.6845624238223768
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8866069089335411,
        "std": 0.003308377313702225,
        "ci_lower": 0.8800259925560231,
        "ci_upper": 0.8930574827483014
      }
    }
  },
  "('ebg', 'LogReg', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.19652666369483304,
        "std": 0.1479391672935303,
        "ci_lower": 0.003529935670758092,
        "ci_upper": 0.5073225092382845
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.24604318958555685,
        "std": 0.1464063251877246,
        "ci_lower": 0.0166889840275688,
        "ci_upper": 0.5375058893552458
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.18488125951896184,
        "std": 0.1425551074509804,
        "ci_lower": 0.004418071205366381,
        "ci_upper": 0.4827451210108054
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.467205545611591,
        "std": 0.0562336273188206,
        "ci_lower": 2.3591894963242717,
        "ci_upper": 2.5790017759945636
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.1469487377533773,
        "std": 0.0032269655050751666,
        "ci_lower": 0.14041213344523903,
        "ci_upper": 0.15303526240608675
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9382308149441011,
        "std": 0.00413657412701086,
        "ci_lower": 0.9300503040736632,
        "ci_upper": 0.9465740218842859
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.4893650964343262,
        "std": 0.00412696221632013,
        "ci_lower": 0.48111998200238715,
        "ci_upper": 0.49738198697525304
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.11464487519973872,
        "std": 0.011984788880640307,
        "ci_lower": 0.0910350898714138,
        "ci_upper": 0.1383577339854699
      }
    }
  },
  "('ebg', 'LogReg', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.42645454313287656,
        "std": 0.1559938750461524,
        "ci_lower": 0.137918937581085,
        "ci_upper": 0.748278251934814
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.5993963539762319,
        "std": 0.15575521893150973,
        "ci_lower": 0.28598228455395597,
        "ci_upper": 0.9064456205181831
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.3598076879213996,
        "std": 0.15698161889797735,
        "ci_lower": 0.07894460033789237,
        "ci_upper": 0.6802379794766299
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.6269292298149005,
        "std": 0.05832778113034454,
        "ci_lower": 2.5141075762895495,
        "ci_upper": 2.743735604982672
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.5058097033677661,
        "std": 0.006943378706237528,
        "ci_lower": 0.4921730025886197,
        "ci_upper": 0.5196085372259011
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.6159036433852443,
        "std": 0.006086255009061851,
        "ci_lower": 0.6046967903568554,
        "ci_upper": 0.628503805830262
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.8193830601865693,
        "std": 0.0038333396484678253,
        "ci_lower": 0.8119243773451079,
        "ci_upper": 0.8268479486920031
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.9468019597304654,
        "std": 0.0022346112850695496,
        "ci_lower": 0.9425981782827532,
        "ci_upper": 0.9513183114290776
      }
    }
  },
  "('ebg', 'LogReg', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.1624620801804,
        "std": 0.14658379624989498,
        "ci_lower": 0.0013635951412811836,
        "ci_upper": 0.47563220964679537
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.2605938262246618,
        "std": 0.14752966112542545,
        "ci_lower": 0.020784435919778667,
        "ci_upper": 0.5635640253180653
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.17397436217909829,
        "std": 0.15523856504513325,
        "ci_lower": 0.002742615126718065,
        "ci_upper": 0.5146666608234857
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.687059265613096,
        "std": 0.058637762896240266,
        "ci_lower": 2.5762664821793044,
        "ci_upper": 2.807186873242816
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.17497948558218748,
        "std": 0.0032413066837629753,
        "ci_lower": 0.16834021215720343,
        "ci_upper": 0.18087618006466272
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9417099040185573,
        "std": 0.0038283039427552583,
        "ci_lower": 0.934117706970391,
        "ci_upper": 0.9488763510459403
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.49792877323007645,
        "std": 0.004145644285765375,
        "ci_lower": 0.489429475724676,
        "ci_upper": 0.5058885471361496
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.11923314137104606,
        "std": 0.012414650357646164,
        "ci_lower": 0.09492025520120081,
        "ci_upper": 0.14347378827762394
      }
    }
  },
  "('ebg', 'LogReg', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.18400042185528304,
        "std": 0.15142585952723722,
        "ci_lower": 0.005111049814274888,
        "ci_upper": 0.5046177424211228
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.16816878705621366,
        "std": 0.15239737904922418,
        "ci_lower": 0.0028535840161120225,
        "ci_upper": 0.5000531842032632
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.3962518052396584,
        "std": 0.05557495430461736,
        "ci_lower": 2.284724983678308,
        "ci_upper": 2.502308634917579
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.12933142718417306,
        "std": 0.0030373768577315664,
        "ci_lower": 0.12302031664580755,
        "ci_upper": 0.1349955805612274
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9421130200097265,
        "std": 0.003852217405642392,
        "ci_lower": 0.9345274814757278,
        "ci_upper": 0.9498003181887362
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.49360086175039464,
        "std": 0.003974952489315841,
        "ci_lower": 0.4860929134948334,
        "ci_upper": 0.5017297309293639
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.11348201958975548,
        "std": 0.011716347358210935,
        "ci_lower": 0.09156824262526093,
        "ci_upper": 0.1369954411506311
      }
    }
  },
  "('ebg', 'SVM', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.27495671170619695,
        "std": 0.14960014028463836,
        "ci_lower": 0.02937024618980144,
        "ci_upper": 0.580229830035496
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.3856916325461293,
        "std": 0.15846714534046194,
        "ci_lower": 0.09041917374041253,
        "ci_upper": 0.7120475068146469
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.20858528900946013,
        "std": 0.14847448407739744,
        "ci_lower": 0.0062252388896288455,
        "ci_upper": 0.5136573868053832
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.2672167273541968,
        "std": 0.052113584662429435,
        "ci_lower": 2.162574149982492,
        "ci_upper": 2.366534243948899
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.22617652963788687,
        "std": 0.0034030299551289413,
        "ci_lower": 0.21953221165364206,
        "ci_upper": 0.23265629450925393
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8144329284575357,
        "std": 0.005884997076708827,
        "ci_lower": 0.8032417694992225,
        "ci_upper": 0.8263001082964285
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6771834718455249,
        "std": 0.003631472882798456,
        "ci_lower": 0.670408316436632,
        "ci_upper": 0.6845624238223768
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8866069089335411,
        "std": 0.003308377313702225,
        "ci_lower": 0.8800259925560231,
        "ci_upper": 0.8930574827483014
      }
    }
  },
  "('ebg', 'SVM', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.18400042185528304,
        "std": 0.15142585952723722,
        "ci_lower": 0.005111049814274888,
        "ci_upper": 0.5046177424211228
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.2605938262246618,
        "std": 0.14752966112542545,
        "ci_lower": 0.020784435919778667,
        "ci_upper": 0.5635640253180653
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.18949998411179073,
        "std": 0.1545220388814941,
        "ci_lower": 0.004767647602113303,
        "ci_upper": 0.5175513508971037
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.467205545611591,
        "std": 0.0562336273188206,
        "ci_lower": 2.3591894963242717,
        "ci_upper": 2.5790017759945636
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.1469487377533773,
        "std": 0.0032269655050751666,
        "ci_lower": 0.14041213344523903,
        "ci_upper": 0.15303526240608675
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9382308149441011,
        "std": 0.00413657412701086,
        "ci_lower": 0.9300503040736632,
        "ci_upper": 0.9465740218842859
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.4893650964343262,
        "std": 0.00412696221632013,
        "ci_lower": 0.48111998200238715,
        "ci_upper": 0.49738198697525304
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.11464487519973872,
        "std": 0.011984788880640307,
        "ci_lower": 0.0910350898714138,
        "ci_upper": 0.1383577339854699
      }
    }
  },
  "('ebg', 'SVM', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.47851621696186136,
        "std": 0.15989594199992885,
        "ci_lower": 0.17484717694510316,
        "ci_upper": 0.8001920413124228
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.6436401543699543,
        "std": 0.14842105989488832,
        "ci_lower": 0.32936241120972404,
        "ci_upper": 0.9014467140354169
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.2595219997629568,
        "std": 0.14906801847664206,
        "ci_lower": 0.014822990646016413,
        "ci_upper": 0.5616528292862054
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.6269292298149005,
        "std": 0.05832778113034454,
        "ci_lower": 2.5141075762895495,
        "ci_upper": 2.743735604982672
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.5058097033677661,
        "std": 0.006943378706237528,
        "ci_lower": 0.4921730025886197,
        "ci_upper": 0.5196085372259011
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.6159036433852443,
        "std": 0.006086255009061851,
        "ci_lower": 0.6046967903568554,
        "ci_upper": 0.628503805830262
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.8193830601865693,
        "std": 0.0038333396484678253,
        "ci_lower": 0.8119243773451079,
        "ci_upper": 0.8268479486920031
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.9468019597304654,
        "std": 0.0022346112850695496,
        "ci_lower": 0.9425981782827532,
        "ci_upper": 0.9513183114290776
      }
    }
  },
  "('ebg', 'SVM', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.19652666369483304,
        "std": 0.1479391672935303,
        "ci_lower": 0.003529935670758092,
        "ci_upper": 0.5073225092382845
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.17415912375882397,
        "std": 0.16089225964301088,
        "ci_lower": 0.0034922194324378177,
        "ci_upper": 0.526886785020609
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.687059265613096,
        "std": 0.058637762896240266,
        "ci_lower": 2.5762664821793044,
        "ci_upper": 2.807186873242816
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.17497948558218748,
        "std": 0.0032413066837629753,
        "ci_lower": 0.16834021215720343,
        "ci_upper": 0.18087618006466272
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9417099040185573,
        "std": 0.0038283039427552583,
        "ci_lower": 0.934117706970391,
        "ci_upper": 0.9488763510459403
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.49792877323007645,
        "std": 0.004145644285765375,
        "ci_lower": 0.489429475724676,
        "ci_upper": 0.5058885471361496
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.11923314137104606,
        "std": 0.012414650357646164,
        "ci_lower": 0.09492025520120081,
        "ci_upper": 0.14347378827762394
      }
    }
  },
  "('ebg', 'SVM', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.18400042185528304,
        "std": 0.15142585952723722,
        "ci_lower": 0.005111049814274888,
        "ci_upper": 0.5046177424211228
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.24604318958555685,
        "std": 0.1464063251877246,
        "ci_lower": 0.0166889840275688,
        "ci_upper": 0.5375058893552458
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.17496724560532467,
        "std": 0.15848050939005767,
        "ci_lower": 0.004439095931166898,
        "ci_upper": 0.5203958992377126
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.3962518052396584,
        "std": 0.05557495430461736,
        "ci_lower": 2.284724983678308,
        "ci_upper": 2.502308634917579
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.12933142718417306,
        "std": 0.0030373768577315664,
        "ci_lower": 0.12302031664580755,
        "ci_upper": 0.1349955805612274
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9421130200097265,
        "std": 0.003852217405642392,
        "ci_lower": 0.9345274814757278,
        "ci_upper": 0.9498003181887362
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.49360086175039464,
        "std": 0.003974952489315841,
        "ci_lower": 0.4860929134948334,
        "ci_upper": 0.5017297309293639
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.11348201958975548,
        "std": 0.011716347358210935,
        "ci_lower": 0.09156824262526093,
        "ci_upper": 0.1369954411506311
      }
    }
  },
  "('ebg', 'RoBERTa', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.504334502243066,
        "std": 0.1570727044056076,
        "ci_lower": 0.18486014052943936,
        "ci_upper": 0.807144510667877
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.6344167095090265,
        "std": 0.15340999474227324,
        "ci_lower": 0.30933306480478934,
        "ci_upper": 0.912070466627938
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.47245426059625695,
        "std": 0.15982759618258902,
        "ci_lower": 0.15229760393114344,
        "ci_upper": 0.7856182822835436
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.2672167273541968,
        "std": 0.052113584662429435,
        "ci_lower": 2.162574149982492,
        "ci_upper": 2.366534243948899
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.22617652963788687,
        "std": 0.0034030299551289413,
        "ci_lower": 0.21953221165364206,
        "ci_upper": 0.23265629450925393
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8144329284575357,
        "std": 0.005884997076708827,
        "ci_lower": 0.8032417694992225,
        "ci_upper": 0.8263001082964285
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6771834718455249,
        "std": 0.003631472882798456,
        "ci_lower": 0.670408316436632,
        "ci_upper": 0.6845624238223768
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8866069089335411,
        "std": 0.003308377313702225,
        "ci_lower": 0.8800259925560231,
        "ci_upper": 0.8930574827483014
      }
    }
  },
  "('ebg', 'RoBERTa', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.21696600533925262,
        "std": 0.15097396474024674,
        "ci_lower": 0.006336258248271914,
        "ci_upper": 0.5254646587636552
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.15018533399472983,
        "std": 0.1576660703076875,
        "ci_lower": 0.001172964495335092,
        "ci_upper": 0.5005471165355696
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.467205545611591,
        "std": 0.0562336273188206,
        "ci_lower": 2.3591894963242717,
        "ci_upper": 2.5790017759945636
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.1469487377533773,
        "std": 0.0032269655050751666,
        "ci_lower": 0.14041213344523903,
        "ci_upper": 0.15303526240608675
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9382308149441011,
        "std": 0.00413657412701086,
        "ci_lower": 0.9300503040736632,
        "ci_upper": 0.9465740218842859
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.4893650964343262,
        "std": 0.00412696221632013,
        "ci_lower": 0.48111998200238715,
        "ci_upper": 0.49738198697525304
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.11464487519973872,
        "std": 0.011984788880640307,
        "ci_lower": 0.0910350898714138,
        "ci_upper": 0.1383577339854699
      }
    }
  },
  "('ebg', 'RoBERTa', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.5557589926618536,
        "std": 0.15701870148218697,
        "ci_lower": 0.23779710445250826,
        "ci_upper": 0.8586864340374704
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.6436401543699543,
        "std": 0.14842105989488832,
        "ci_lower": 0.32936241120972404,
        "ci_upper": 0.9014467140354169
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.5278581631360828,
        "std": 0.1570151393492214,
        "ci_lower": 0.20404337140334125,
        "ci_upper": 0.8386675196983544
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.6269292298149005,
        "std": 0.05832778113034454,
        "ci_lower": 2.5141075762895495,
        "ci_upper": 2.743735604982672
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.5058097033677661,
        "std": 0.006943378706237528,
        "ci_lower": 0.4921730025886197,
        "ci_upper": 0.5196085372259011
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.6159036433852443,
        "std": 0.006086255009061851,
        "ci_lower": 0.6046967903568554,
        "ci_upper": 0.628503805830262
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.8193830601865693,
        "std": 0.0038333396484678253,
        "ci_lower": 0.8119243773451079,
        "ci_upper": 0.8268479486920031
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.9468019597304654,
        "std": 0.0022346112850695496,
        "ci_lower": 0.9425981782827532,
        "ci_upper": 0.9513183114290776
      }
    }
  },
  "('ebg', 'RoBERTa', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.18400042185528304,
        "std": 0.15142585952723722,
        "ci_lower": 0.005111049814274888,
        "ci_upper": 0.5046177424211228
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.14255636740558264,
        "std": 0.15197805443851595,
        "ci_lower": 0.0005718602485771251,
        "ci_upper": 0.4760348901705362
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.687059265613096,
        "std": 0.058637762896240266,
        "ci_lower": 2.5762664821793044,
        "ci_upper": 2.807186873242816
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.17497948558218748,
        "std": 0.0032413066837629753,
        "ci_lower": 0.16834021215720343,
        "ci_upper": 0.18087618006466272
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9417099040185573,
        "std": 0.0038283039427552583,
        "ci_lower": 0.934117706970391,
        "ci_upper": 0.9488763510459403
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.49792877323007645,
        "std": 0.004145644285765375,
        "ci_lower": 0.489429475724676,
        "ci_upper": 0.5058885471361496
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.11923314137104606,
        "std": 0.012414650357646164,
        "ci_lower": 0.09492025520120081,
        "ci_upper": 0.14347378827762394
      }
    }
  },
  "('ebg', 'RoBERTa', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.14717025512394583,
        "std": 0.1549862958542142,
        "ci_lower": 0.0012563444611723147,
        "ci_upper": 0.49051123312955197
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.3962518052396584,
        "std": 0.05557495430461736,
        "ci_lower": 2.284724983678308,
        "ci_upper": 2.502308634917579
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.12933142718417306,
        "std": 0.0030373768577315664,
        "ci_lower": 0.12302031664580755,
        "ci_upper": 0.1349955805612274
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9421130200097265,
        "std": 0.003852217405642392,
        "ci_lower": 0.9345274814757278,
        "ci_upper": 0.9498003181887362
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.49360086175039464,
        "std": 0.003974952489315841,
        "ci_lower": 0.4860929134948334,
        "ci_upper": 0.5017297309293639
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.11348201958975548,
        "std": 0.011716347358210935,
        "ci_lower": 0.09156824262526093,
        "ci_upper": 0.1369954411506311
      }
    }
  }
}