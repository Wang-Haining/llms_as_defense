{
  "('ebg', 'LogReg', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.14809453888137578,
        "std": 0.15459811402881896,
        "ci_lower": 0.0006703247478273308,
        "ci_upper": 0.48755225285681475
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.271362570063038,
        "std": 0.05289337767165418,
        "ci_lower": 2.1678881100059564,
        "ci_upper": 2.374563755421228
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.10914767948044848,
        "std": 0.0026118413270408334,
        "ci_lower": 0.10395975325133475,
        "ci_upper": 0.11429860952084311
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.920790656305555,
        "std": 0.005015622639330771,
        "ci_lower": 0.9115312267187131,
        "ci_upper": 0.9311116889657306
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.4747106860722205,
        "std": 0.0036506468093433506,
        "ci_lower": 0.467684192676719,
        "ci_upper": 0.48204192665975754
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.09643973161102923,
        "std": 0.010622703409487682,
        "ci_lower": 0.07718632699756095,
        "ci_upper": 0.11747260467154129
      }
    }
  },
  "('ebg', 'LogReg', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.5411251329825744,
        "std": 0.16004393772281902,
        "ci_lower": 0.23885691908773168,
        "ci_upper": 0.8726573184758236
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.251257117201116,
        "std": 0.1484200240840437,
        "ci_lower": 0.019557150941782887,
        "ci_upper": 0.5465316895213831
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.74504152780814,
        "std": 0.051792332739673555,
        "ci_lower": 2.645645250465759,
        "ci_upper": 2.848339124973246
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.23632592897691063,
        "std": 0.0037944446742274184,
        "ci_lower": 0.22903941641153494,
        "ci_upper": 0.24391118279177743
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8121035377744622,
        "std": 0.006283968656076992,
        "ci_lower": 0.7997364097021461,
        "ci_upper": 0.8244780151972089
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6579994506114258,
        "std": 0.003760117118189837,
        "ci_lower": 0.6506327869141271,
        "ci_upper": 0.665430940692647
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8263550063633307,
        "std": 0.004387441090845842,
        "ci_lower": 0.8179907805887493,
        "ci_upper": 0.8351557415521496
      }
    }
  },
  "('ebg', 'LogReg', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.19652666369483304,
        "std": 0.1479391672935303,
        "ci_lower": 0.003529935670758092,
        "ci_upper": 0.5073225092382845
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.24604318958555685,
        "std": 0.1464063251877246,
        "ci_lower": 0.0166889840275688,
        "ci_upper": 0.5375058893552458
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.17759641310368113,
        "std": 0.14814659193942759,
        "ci_lower": 0.002606527031574771,
        "ci_upper": 0.4875294753849926
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.375139207635539,
        "std": 0.05168049720777828,
        "ci_lower": 2.269222919265024,
        "ci_upper": 2.4725578042910317
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.11153300395350564,
        "std": 0.002440631623594991,
        "ci_lower": 0.10700871174588783,
        "ci_upper": 0.11656329523780569
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9166917215885172,
        "std": 0.0050627027732365396,
        "ci_lower": 0.9065550056727157,
        "ci_upper": 0.9262163629552891
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.4747583865892422,
        "std": 0.0034179546023924857,
        "ci_lower": 0.46775055915612157,
        "ci_upper": 0.48121394690053026
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.10703760263093334,
        "std": 0.010503482652276025,
        "ci_lower": 0.08659939887114088,
        "ci_upper": 0.12785749342867558
      }
    }
  },
  "('ebg', 'LogReg', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.1624620801804,
        "std": 0.14658379624989498,
        "ci_lower": 0.0013635951412811836,
        "ci_upper": 0.47563220964679537
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.19652666369483304,
        "std": 0.1479391672935303,
        "ci_lower": 0.003529935670758092,
        "ci_upper": 0.5073225092382845
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.16202511676855944,
        "std": 0.1521453485238747,
        "ci_lower": 0.0037714878369541657,
        "ci_upper": 0.4891658379967641
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.6459513925730915,
        "std": 0.05402337519411609,
        "ci_lower": 2.5394197002646397,
        "ci_upper": 2.749763006118775
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.14578994243493001,
        "std": 0.002669742260417445,
        "ci_lower": 0.14039996479703504,
        "ci_upper": 0.15083981875872085
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9236972412028135,
        "std": 0.004834075379887291,
        "ci_lower": 0.9139346841729873,
        "ci_upper": 0.9328612469870712
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.48735058490634026,
        "std": 0.0036976709920136007,
        "ci_lower": 0.4801180238384504,
        "ci_upper": 0.49476555766544067
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.10640276190253804,
        "std": 0.010081596481306231,
        "ci_lower": 0.0875148795522473,
        "ci_upper": 0.12648522488191266
      }
    }
  },
  "('ebg', 'LogReg', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.3563737438852544,
        "std": 0.15604657785114398,
        "ci_lower": 0.07655638132644489,
        "ci_upper": 0.6811175091148759
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.5871807296145594,
        "std": 0.15710552141990033,
        "ci_lower": 0.2762991532401892,
        "ci_upper": 0.9039691609871225
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.30593170488272875,
        "std": 0.1546577084670691,
        "ci_lower": 0.04797460017265789,
        "ci_upper": 0.6384716383698288
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.634372890552789,
        "std": 0.05711263364664667,
        "ci_lower": 2.5213205669316077,
        "ci_upper": 2.74663489616334
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.3947448017094675,
        "std": 0.0068217513818349,
        "ci_lower": 0.38159783339541337,
        "ci_upper": 0.4081622236673004
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.6311198670188773,
        "std": 0.00700747867335696,
        "ci_lower": 0.6170735102485458,
        "ci_upper": 0.6446663026242305
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.7556594141093399,
        "std": 0.004214626412628237,
        "ci_lower": 0.7471832198320547,
        "ci_upper": 0.7635903892612678
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8967247951443468,
        "std": 0.0033838461374847482,
        "ci_lower": 0.8901120077217508,
        "ci_upper": 0.903285477684328
      }
    }
  },
  "('ebg', 'SVM', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.1624620801804,
        "std": 0.14658379624989498,
        "ci_lower": 0.0013635951412811836,
        "ci_upper": 0.47563220964679537
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.19652666369483304,
        "std": 0.1479391672935303,
        "ci_lower": 0.003529935670758092,
        "ci_upper": 0.5073225092382845
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.1717901016812085,
        "std": 0.15207729372392673,
        "ci_lower": 0.003513441120623529,
        "ci_upper": 0.4997188191876174
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.271362570063038,
        "std": 0.05289337767165418,
        "ci_lower": 2.1678881100059564,
        "ci_upper": 2.374563755421228
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.10914767948044848,
        "std": 0.0026118413270408334,
        "ci_lower": 0.10395975325133475,
        "ci_upper": 0.11429860952084311
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.920790656305555,
        "std": 0.005015622639330771,
        "ci_lower": 0.9115312267187131,
        "ci_upper": 0.9311116889657306
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.4747106860722205,
        "std": 0.0036506468093433506,
        "ci_lower": 0.467684192676719,
        "ci_upper": 0.48204192665975754
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.09643973161102923,
        "std": 0.010622703409487682,
        "ci_lower": 0.07718632699756095,
        "ci_upper": 0.11747260467154129
      }
    }
  },
  "('ebg', 'SVM', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.3022908454126393,
        "std": 0.15290620134623778,
        "ci_lower": 0.04237316748200466,
        "ci_upper": 0.6152740587683058
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.4165233409995908,
        "std": 0.15774133991043482,
        "ci_lower": 0.11591048305923744,
        "ci_upper": 0.744050861006474
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.2218596841508,
        "std": 0.1494529140077296,
        "ci_lower": 0.01180853299653424,
        "ci_upper": 0.5323422343535801
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.74504152780814,
        "std": 0.051792332739673555,
        "ci_lower": 2.645645250465759,
        "ci_upper": 2.848339124973246
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.23632592897691063,
        "std": 0.0037944446742274184,
        "ci_lower": 0.22903941641153494,
        "ci_upper": 0.24391118279177743
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8121035377744622,
        "std": 0.006283968656076992,
        "ci_lower": 0.7997364097021461,
        "ci_upper": 0.8244780151972089
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6579994506114258,
        "std": 0.003760117118189837,
        "ci_lower": 0.6506327869141271,
        "ci_upper": 0.665430940692647
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8263550063633307,
        "std": 0.004387441090845842,
        "ci_lower": 0.8179907805887493,
        "ci_upper": 0.8351557415521496
      }
    }
  },
  "('ebg', 'SVM', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.18400042185528304,
        "std": 0.15142585952723722,
        "ci_lower": 0.005111049814274888,
        "ci_upper": 0.5046177424211228
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.24604318958555685,
        "std": 0.1464063251877246,
        "ci_lower": 0.0166889840275688,
        "ci_upper": 0.5375058893552458
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.17753745853575384,
        "std": 0.14895952972409657,
        "ci_lower": 0.0005752967107569473,
        "ci_upper": 0.49656165748570325
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.375139207635539,
        "std": 0.05168049720777828,
        "ci_lower": 2.269222919265024,
        "ci_upper": 2.4725578042910317
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.11153300395350564,
        "std": 0.002440631623594991,
        "ci_lower": 0.10700871174588783,
        "ci_upper": 0.11656329523780569
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9166917215885172,
        "std": 0.0050627027732365396,
        "ci_lower": 0.9065550056727157,
        "ci_upper": 0.9262163629552891
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.4747583865892422,
        "std": 0.0034179546023924857,
        "ci_lower": 0.46775055915612157,
        "ci_upper": 0.48121394690053026
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.10703760263093334,
        "std": 0.010503482652276025,
        "ci_lower": 0.08659939887114088,
        "ci_upper": 0.12785749342867558
      }
    }
  },
  "('ebg', 'SVM', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.1624620801804,
        "std": 0.14658379624989498,
        "ci_lower": 0.0013635951412811836,
        "ci_upper": 0.47563220964679537
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.16106147730388812,
        "std": 0.154750381590555,
        "ci_lower": 0.0026219426114104673,
        "ci_upper": 0.5065799666056845
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.6459513925730915,
        "std": 0.05402337519411609,
        "ci_lower": 2.5394197002646397,
        "ci_upper": 2.749763006118775
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.14578994243493001,
        "std": 0.002669742260417445,
        "ci_lower": 0.14039996479703504,
        "ci_upper": 0.15083981875872085
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9236972412028135,
        "std": 0.004834075379887291,
        "ci_lower": 0.9139346841729873,
        "ci_upper": 0.9328612469870712
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.48735058490634026,
        "std": 0.0036976709920136007,
        "ci_lower": 0.4801180238384504,
        "ci_upper": 0.49476555766544067
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.10640276190253804,
        "std": 0.010081596481306231,
        "ci_lower": 0.0875148795522473,
        "ci_upper": 0.12648522488191266
      }
    }
  },
  "('ebg', 'SVM', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.32300244969814457,
        "std": 0.15548491580870608,
        "ci_lower": 0.06159220968399123,
        "ci_upper": 0.6445781584583743
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.5557589926618536,
        "std": 0.15701870148218697,
        "ci_lower": 0.23779710445250826,
        "ci_upper": 0.8586864340374704
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.24161842001775727,
        "std": 0.14739746221409994,
        "ci_lower": 0.017453589270762102,
        "ci_upper": 0.5417705418349738
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.634372890552789,
        "std": 0.05711263364664667,
        "ci_lower": 2.5213205669316077,
        "ci_upper": 2.74663489616334
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.3947448017094675,
        "std": 0.0068217513818349,
        "ci_lower": 0.38159783339541337,
        "ci_upper": 0.4081622236673004
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.6311198670188773,
        "std": 0.00700747867335696,
        "ci_lower": 0.6170735102485458,
        "ci_upper": 0.6446663026242305
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.7556594141093399,
        "std": 0.004214626412628237,
        "ci_lower": 0.7471832198320547,
        "ci_upper": 0.7635903892612678
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8967247951443468,
        "std": 0.0033838461374847482,
        "ci_lower": 0.8901120077217508,
        "ci_upper": 0.903285477684328
      }
    }
  },
  "('ebg', 'RoBERTa', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.19652666369483304,
        "std": 0.1479391672935303,
        "ci_lower": 0.003529935670758092,
        "ci_upper": 0.5073225092382845
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.14772578654885182,
        "std": 0.15742433714851695,
        "ci_lower": 0.0009429949260917138,
        "ci_upper": 0.50087704496088
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.271362570063038,
        "std": 0.05289337767165418,
        "ci_lower": 2.1678881100059564,
        "ci_upper": 2.374563755421228
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.10914767948044848,
        "std": 0.0026118413270408334,
        "ci_lower": 0.10395975325133475,
        "ci_upper": 0.11429860952084311
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.920790656305555,
        "std": 0.005015622639330771,
        "ci_lower": 0.9115312267187131,
        "ci_upper": 0.9311116889657306
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.4747106860722205,
        "std": 0.0036506468093433506,
        "ci_lower": 0.467684192676719,
        "ci_upper": 0.48204192665975754
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.09643973161102923,
        "std": 0.010622703409487682,
        "ci_lower": 0.07718632699756095,
        "ci_upper": 0.11747260467154129
      }
    }
  },
  "('ebg', 'RoBERTa', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.44881224203247433,
        "std": 0.15561118466988502,
        "ci_lower": 0.14032618475157368,
        "ci_upper": 0.7577125470424216
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.5871807296145594,
        "std": 0.15710552141990033,
        "ci_lower": 0.2762991532401892,
        "ci_upper": 0.9039691609871225
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.388046010532323,
        "std": 0.15255194051840734,
        "ci_lower": 0.10015781142989087,
        "ci_upper": 0.6982997686171707
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.74504152780814,
        "std": 0.051792332739673555,
        "ci_lower": 2.645645250465759,
        "ci_upper": 2.848339124973246
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.23632592897691063,
        "std": 0.0037944446742274184,
        "ci_lower": 0.22903941641153494,
        "ci_upper": 0.24391118279177743
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8121035377744622,
        "std": 0.006283968656076992,
        "ci_lower": 0.7997364097021461,
        "ci_upper": 0.8244780151972089
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6579994506114258,
        "std": 0.003760117118189837,
        "ci_lower": 0.6506327869141271,
        "ci_upper": 0.665430940692647
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8263550063633307,
        "std": 0.004387441090845842,
        "ci_lower": 0.8179907805887493,
        "ci_upper": 0.8351557415521496
      }
    }
  },
  "('ebg', 'RoBERTa', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1624620801804,
        "std": 0.14658379624989498,
        "ci_lower": 0.0013635951412811836,
        "ci_upper": 0.47563220964679537
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.21696600533925262,
        "std": 0.15097396474024674,
        "ci_lower": 0.006336258248271914,
        "ci_upper": 0.5254646587636552
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.1527680273728784,
        "std": 0.15094949134562202,
        "ci_lower": 0.001176466753810312,
        "ci_upper": 0.471721882123803
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.375139207635539,
        "std": 0.05168049720777828,
        "ci_lower": 2.269222919265024,
        "ci_upper": 2.4725578042910317
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.11153300395350564,
        "std": 0.002440631623594991,
        "ci_lower": 0.10700871174588783,
        "ci_upper": 0.11656329523780569
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9166917215885172,
        "std": 0.0050627027732365396,
        "ci_lower": 0.9065550056727157,
        "ci_upper": 0.9262163629552891
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.4747583865892422,
        "std": 0.0034179546023924857,
        "ci_lower": 0.46775055915612157,
        "ci_upper": 0.48121394690053026
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.10703760263093334,
        "std": 0.010503482652276025,
        "ci_lower": 0.08659939887114088,
        "ci_upper": 0.12785749342867558
      }
    }
  },
  "('ebg', 'RoBERTa', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.13979876148453765,
        "std": 0.1533768964152769,
        "ci_lower": 0.0007691452590902194,
        "ci_upper": 0.4796307228947738
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.6459513925730915,
        "std": 0.05402337519411609,
        "ci_lower": 2.5394197002646397,
        "ci_upper": 2.749763006118775
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.14578994243493001,
        "std": 0.002669742260417445,
        "ci_lower": 0.14039996479703504,
        "ci_upper": 0.15083981875872085
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9236972412028135,
        "std": 0.004834075379887291,
        "ci_lower": 0.9139346841729873,
        "ci_upper": 0.9328612469870712
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.48735058490634026,
        "std": 0.0036976709920136007,
        "ci_lower": 0.4801180238384504,
        "ci_upper": 0.49476555766544067
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.10640276190253804,
        "std": 0.010081596481306231,
        "ci_lower": 0.0875148795522473,
        "ci_upper": 0.12648522488191266
      }
    }
  },
  "('ebg', 'RoBERTa', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.5411251329825744,
        "std": 0.16004393772281902,
        "ci_lower": 0.23885691908773168,
        "ci_upper": 0.8726573184758236
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.6152682990688627,
        "std": 0.15541627098136146,
        "ci_lower": 0.3125660035753271,
        "ci_upper": 0.9218959002211766
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.5101278685773806,
        "std": 0.15665639539371434,
        "ci_lower": 0.2034017337122347,
        "ci_upper": 0.8220622524962983
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.634372890552789,
        "std": 0.05711263364664667,
        "ci_lower": 2.5213205669316077,
        "ci_upper": 2.74663489616334
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.3947448017094675,
        "std": 0.0068217513818349,
        "ci_lower": 0.38159783339541337,
        "ci_upper": 0.4081622236673004
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.6311198670188773,
        "std": 0.00700747867335696,
        "ci_lower": 0.6170735102485458,
        "ci_upper": 0.6446663026242305
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.7556594141093399,
        "std": 0.004214626412628237,
        "ci_lower": 0.7471832198320547,
        "ci_upper": 0.7635903892612678
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8967247951443468,
        "std": 0.0033838461374847482,
        "ci_lower": 0.8901120077217508,
        "ci_upper": 0.903285477684328
      }
    }
  }
}