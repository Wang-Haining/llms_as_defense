{
  "('ebg', 'LogReg', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.15003168599907957,
        "std": 0.14807043678441859,
        "ci_lower": 0.0005757987377591409,
        "ci_upper": 0.47325048722051316
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.5891092628777708,
        "std": 0.05733992324406784,
        "ci_lower": 2.477079476051716,
        "ci_upper": 2.6996725079682107
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.17126451317000707,
        "std": 0.002789271329345615,
        "ci_lower": 0.1657267403785408,
        "ci_upper": 0.1766844812330996
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9454052056947853,
        "std": 0.003586512441825667,
        "ci_lower": 0.9384568392629411,
        "ci_upper": 0.95221825997318
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.48501152880376536,
        "std": 0.0034583013641931127,
        "ci_lower": 0.4779357034810458,
        "ci_upper": 0.4914180693759588
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.15518593124310703,
        "std": 0.012354948990249727,
        "ci_lower": 0.13265322191570722,
        "ci_upper": 0.18114617652635456
      }
    }
  },
  "('ebg', 'LogReg', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.21696600533925262,
        "std": 0.15097396474024674,
        "ci_lower": 0.006336258248271914,
        "ci_upper": 0.5254646587636552
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.4165233409995908,
        "std": 0.15774133991043482,
        "ci_lower": 0.11591048305923744,
        "ci_upper": 0.744050861006474
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.21563887541356888,
        "std": 0.14814198387076927,
        "ci_lower": 0.003285773202685783,
        "ci_upper": 0.5300032154790589
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 3.0191219460744327,
        "std": 0.05185565976091394,
        "ci_lower": 2.9133499110222263,
        "ci_upper": 3.118126006737043
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.3232413632463189,
        "std": 0.004619347540056891,
        "ci_lower": 0.3143939832568105,
        "ci_upper": 0.33225208835730274
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8294256203269603,
        "std": 0.005357154892883683,
        "ci_lower": 0.8189969398410999,
        "ci_upper": 0.8396751662157332
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6715733045110378,
        "std": 0.004466286284381596,
        "ci_lower": 0.6627317179141962,
        "ci_upper": 0.6801462669884095
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.827961734881199,
        "std": 0.0057923205111785935,
        "ci_lower": 0.8171142792853673,
        "ci_upper": 0.8400084261741686
      }
    }
  },
  "('ebg', 'LogReg', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Ministral",
    "effectiveness_estimates": {},
    "quality_estimates": {}
  },
  "('ebg', 'LogReg', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.3022908454126393,
        "std": 0.15290620134623778,
        "ci_lower": 0.04237316748200466,
        "ci_upper": 0.6152740587683058
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.221070149148055,
        "std": 0.14892958271855186,
        "ci_lower": 0.012889694260877294,
        "ci_upper": 0.5309644435681324
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.361579305731894,
        "std": 0.056363640840796835,
        "ci_lower": 2.253348440756035,
        "ci_upper": 2.474684931639532
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.21667675906278158,
        "std": 0.003265973467800868,
        "ci_lower": 0.20991907895863687,
        "ci_upper": 0.2227769338056803
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8938668320565626,
        "std": 0.005028276027845834,
        "ci_lower": 0.8837738648865215,
        "ci_upper": 0.9033978179919803
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5763192036479109,
        "std": 0.003792481588771883,
        "ci_lower": 0.5686743631556301,
        "ci_upper": 0.583502332000251
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.6965270594926444,
        "std": 0.007676124415336092,
        "ci_lower": 0.6809884691800789,
        "ci_upper": 0.710925422780778
      }
    }
  },
  "('ebg', 'LogReg', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.21696600533925262,
        "std": 0.15097396474024674,
        "ci_lower": 0.006336258248271914,
        "ci_upper": 0.5254646587636552
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.15167904987996808,
        "std": 0.14805027457126385,
        "ci_lower": 0.0017753798929138315,
        "ci_upper": 0.47062550954746524
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 3.0235913300729096,
        "std": 0.05580151867252051,
        "ci_lower": 2.9155034111552234,
        "ci_upper": 3.132573674352016
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.19671668259778846,
        "std": 0.0031382687562703094,
        "ci_lower": 0.1906090204935251,
        "ci_upper": 0.20278426051165446
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9496771572671083,
        "std": 0.0033525694213444976,
        "ci_lower": 0.9431754716031602,
        "ci_upper": 0.9560917072376535
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.4953567577990835,
        "std": 0.0038360348598735143,
        "ci_lower": 0.48761780142510064,
        "ci_upper": 0.5029440543316348
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.14248538650624062,
        "std": 0.012340006819645392,
        "ci_lower": 0.11901850734212636,
        "ci_upper": 0.1668122158390158
      }
    }
  },
  "('ebg', 'SVM', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.18400042185528304,
        "std": 0.15142585952723722,
        "ci_lower": 0.005111049814274888,
        "ci_upper": 0.5046177424211228
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.27495671170619695,
        "std": 0.14960014028463836,
        "ci_lower": 0.02937024618980144,
        "ci_upper": 0.580229830035496
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.1706186325491493,
        "std": 0.149994874986471,
        "ci_lower": 0.0031063902928505456,
        "ci_upper": 0.49329975216787403
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.5891092628777708,
        "std": 0.05733992324406784,
        "ci_lower": 2.477079476051716,
        "ci_upper": 2.6996725079682107
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.17126451317000707,
        "std": 0.002789271329345615,
        "ci_lower": 0.1657267403785408,
        "ci_upper": 0.1766844812330996
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9454052056947853,
        "std": 0.003586512441825667,
        "ci_lower": 0.9384568392629411,
        "ci_upper": 0.95221825997318
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.48501152880376536,
        "std": 0.0034583013641931127,
        "ci_lower": 0.4779357034810458,
        "ci_upper": 0.4914180693759588
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.15518593124310703,
        "std": 0.012354948990249727,
        "ci_lower": 0.13265322191570722,
        "ci_upper": 0.18114617652635456
      }
    }
  },
  "('ebg', 'SVM', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.19652666369483304,
        "std": 0.1479391672935303,
        "ci_lower": 0.003529935670758092,
        "ci_upper": 0.5073225092382845
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.459938021129627,
        "std": 0.15712915342385333,
        "ci_lower": 0.16316247909253312,
        "ci_upper": 0.7874310610614852
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.19336713917560158,
        "std": 0.14357371474865774,
        "ci_lower": 0.006619299068171451,
        "ci_upper": 0.49345432562435887
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 3.0191219460744327,
        "std": 0.05185565976091394,
        "ci_lower": 2.9133499110222263,
        "ci_upper": 3.118126006737043
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.3232413632463189,
        "std": 0.004619347540056891,
        "ci_lower": 0.3143939832568105,
        "ci_upper": 0.33225208835730274
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8294256203269603,
        "std": 0.005357154892883683,
        "ci_lower": 0.8189969398410999,
        "ci_upper": 0.8396751662157332
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6715733045110378,
        "std": 0.004466286284381596,
        "ci_lower": 0.6627317179141962,
        "ci_upper": 0.6801462669884095
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.827961734881199,
        "std": 0.0057923205111785935,
        "ci_lower": 0.8171142792853673,
        "ci_upper": 0.8400084261741686
      }
    }
  },
  "('ebg', 'SVM', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Ministral",
    "effectiveness_estimates": {},
    "quality_estimates": {}
  },
  "('ebg', 'SVM', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.18400042185528304,
        "std": 0.15142585952723722,
        "ci_lower": 0.005111049814274888,
        "ci_upper": 0.5046177424211228
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.32300244969814457,
        "std": 0.15548491580870608,
        "ci_lower": 0.06159220968399123,
        "ci_upper": 0.6445781584583743
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.18070212454577478,
        "std": 0.1436371554637254,
        "ci_lower": 0.0018036930771989413,
        "ci_upper": 0.4867879502290891
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.361579305731894,
        "std": 0.056363640840796835,
        "ci_lower": 2.253348440756035,
        "ci_upper": 2.474684931639532
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.21667675906278158,
        "std": 0.003265973467800868,
        "ci_lower": 0.20991907895863687,
        "ci_upper": 0.2227769338056803
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8938668320565626,
        "std": 0.005028276027845834,
        "ci_lower": 0.8837738648865215,
        "ci_upper": 0.9033978179919803
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5763192036479109,
        "std": 0.003792481588771883,
        "ci_lower": 0.5686743631556301,
        "ci_upper": 0.583502332000251
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.6965270594926444,
        "std": 0.007676124415336092,
        "ci_lower": 0.6809884691800789,
        "ci_upper": 0.710925422780778
      }
    }
  },
  "('ebg', 'SVM', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.24604318958555685,
        "std": 0.1464063251877246,
        "ci_lower": 0.0166889840275688,
        "ci_upper": 0.5375058893552458
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.1583433595524761,
        "std": 0.14325523214124228,
        "ci_lower": 0.0029702451279998657,
        "ci_upper": 0.46912030144833117
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 3.0235913300729096,
        "std": 0.05580151867252051,
        "ci_lower": 2.9155034111552234,
        "ci_upper": 3.132573674352016
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.19671668259778846,
        "std": 0.0031382687562703094,
        "ci_lower": 0.1906090204935251,
        "ci_upper": 0.20278426051165446
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9496771572671083,
        "std": 0.0033525694213444976,
        "ci_lower": 0.9431754716031602,
        "ci_upper": 0.9560917072376535
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.4953567577990835,
        "std": 0.0038360348598735143,
        "ci_lower": 0.48761780142510064,
        "ci_upper": 0.5029440543316348
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.14248538650624062,
        "std": 0.012340006819645392,
        "ci_lower": 0.11901850734212636,
        "ci_upper": 0.1668122158390158
      }
    }
  },
  "('ebg', 'RoBERTa', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.152822317051202,
        "std": 0.1581233545352437,
        "ci_lower": 0.0010950085305981181,
        "ci_upper": 0.4920748728337069
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.5891092628777708,
        "std": 0.05733992324406784,
        "ci_lower": 2.477079476051716,
        "ci_upper": 2.6996725079682107
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.17126451317000707,
        "std": 0.002789271329345615,
        "ci_lower": 0.1657267403785408,
        "ci_upper": 0.1766844812330996
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9454052056947853,
        "std": 0.003586512441825667,
        "ci_lower": 0.9384568392629411,
        "ci_upper": 0.95221825997318
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.48501152880376536,
        "std": 0.0034583013641931127,
        "ci_lower": 0.4779357034810458,
        "ci_upper": 0.4914180693759588
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.15518593124310703,
        "std": 0.012354948990249727,
        "ci_lower": 0.13265322191570722,
        "ci_upper": 0.18114617652635456
      }
    }
  },
  "('ebg', 'RoBERTa', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.42645454313287656,
        "std": 0.1559938750461524,
        "ci_lower": 0.137918937581085,
        "ci_upper": 0.748278251934814
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.6344167095090265,
        "std": 0.15340999474227324,
        "ci_lower": 0.30933306480478934,
        "ci_upper": 0.912070466627938
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.40515878834709695,
        "std": 0.15581013830129323,
        "ci_lower": 0.12264799796606471,
        "ci_upper": 0.731357130882089
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 3.0191219460744327,
        "std": 0.05185565976091394,
        "ci_lower": 2.9133499110222263,
        "ci_upper": 3.118126006737043
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.3232413632463189,
        "std": 0.004619347540056891,
        "ci_lower": 0.3143939832568105,
        "ci_upper": 0.33225208835730274
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8294256203269603,
        "std": 0.005357154892883683,
        "ci_lower": 0.8189969398410999,
        "ci_upper": 0.8396751662157332
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6715733045110378,
        "std": 0.004466286284381596,
        "ci_lower": 0.6627317179141962,
        "ci_upper": 0.6801462669884095
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.827961734881199,
        "std": 0.0057923205111785935,
        "ci_lower": 0.8171142792853673,
        "ci_upper": 0.8400084261741686
      }
    }
  },
  "('ebg', 'RoBERTa', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Ministral",
    "effectiveness_estimates": {},
    "quality_estimates": {}
  },
  "('ebg', 'RoBERTa', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.27495671170619695,
        "std": 0.14960014028463836,
        "ci_lower": 0.02937024618980144,
        "ci_upper": 0.580229830035496
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.42645454313287656,
        "std": 0.1559938750461524,
        "ci_lower": 0.137918937581085,
        "ci_upper": 0.748278251934814
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.24928823176313014,
        "std": 0.145853573011348,
        "ci_lower": 0.011108564915110806,
        "ci_upper": 0.5348578070664678
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.361579305731894,
        "std": 0.056363640840796835,
        "ci_lower": 2.253348440756035,
        "ci_upper": 2.474684931639532
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.21667675906278158,
        "std": 0.003265973467800868,
        "ci_lower": 0.20991907895863687,
        "ci_upper": 0.2227769338056803
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8938668320565626,
        "std": 0.005028276027845834,
        "ci_lower": 0.8837738648865215,
        "ci_upper": 0.9033978179919803
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5763192036479109,
        "std": 0.003792481588771883,
        "ci_lower": 0.5686743631556301,
        "ci_upper": 0.583502332000251
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.6965270594926444,
        "std": 0.007676124415336092,
        "ci_lower": 0.6809884691800789,
        "ci_upper": 0.710925422780778
      }
    }
  },
  "('ebg', 'RoBERTa', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.1624620801804,
        "std": 0.14658379624989498,
        "ci_lower": 0.0013635951412811836,
        "ci_upper": 0.47563220964679537
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.14070945937481436,
        "std": 0.14909692949143177,
        "ci_lower": 0.00016247495021201266,
        "ci_upper": 0.46839347769659223
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 3.0235913300729096,
        "std": 0.05580151867252051,
        "ci_lower": 2.9155034111552234,
        "ci_upper": 3.132573674352016
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.19671668259778846,
        "std": 0.0031382687562703094,
        "ci_lower": 0.1906090204935251,
        "ci_upper": 0.20278426051165446
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9496771572671083,
        "std": 0.0033525694213444976,
        "ci_lower": 0.9431754716031602,
        "ci_upper": 0.9560917072376535
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.4953567577990835,
        "std": 0.0038360348598735143,
        "ci_lower": 0.48761780142510064,
        "ci_upper": 0.5029440543316348
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.14248538650624062,
        "std": 0.012340006819645392,
        "ci_lower": 0.11901850734212636,
        "ci_upper": 0.1668122158390158
      }
    }
  }
}