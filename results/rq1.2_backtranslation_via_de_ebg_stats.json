{
  "('ebg', 'LogReg', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.5557589926618536,
        "std": 0.15701870148218697,
        "ci_lower": 0.23779710445250826,
        "ci_upper": 0.8586864340374704
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.7089638453592745,
        "std": 0.15101412560771843,
        "ci_lower": 0.4002635597677743,
        "ci_upper": 0.9574665688594319
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.42518738966228636,
        "std": 0.15834830078908282,
        "ci_lower": 0.11681938999589582,
        "ci_upper": 0.7391102124644243
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.6184701578795098,
        "std": 0.05757686697461955,
        "ci_lower": 2.5063424461677055,
        "ci_upper": 2.731423984441115
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.737878444181667,
        "std": 0.007583409642938012,
        "ci_lower": 0.7231689997421277,
        "ci_upper": 0.7528858713960088
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.3801324533023007,
        "std": 0.005656273332153806,
        "ci_lower": 0.369242753345407,
        "ci_upper": 0.39134683499830153
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.8934685693423933,
        "std": 0.0037548282142785866,
        "ci_lower": 0.8858247766483277,
        "ci_upper": 0.9005891914222716
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.9710407348354965,
        "std": 0.0014638135057954348,
        "ci_lower": 0.9680161945217899,
        "ci_upper": 0.9737741621542118
      }
    }
  },
  "('ebg', 'LogReg', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.24604318958555685,
        "std": 0.1464063251877246,
        "ci_lower": 0.0166889840275688,
        "ci_upper": 0.5375058893552458
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.1518660154630877,
        "std": 0.1509755896696756,
        "ci_lower": 0.0004179429321804026,
        "ci_upper": 0.47510725697899076
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.235105671032439,
        "std": 0.06193421043674967,
        "ci_lower": 2.1128442241066425,
        "ci_upper": 2.3574148068858976
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.1644186716174388,
        "std": 0.007318897902716467,
        "ci_lower": 0.1507319514673021,
        "ci_upper": 0.1790756371023398
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9426784140371405,
        "std": 0.004015477569672091,
        "ci_lower": 0.93508601375926,
        "ci_upper": 0.9509075786020995
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.4800612844629548,
        "std": 0.006512491139760547,
        "ci_lower": 0.46710240365771316,
        "ci_upper": 0.49263599865508434
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.11657307336951336,
        "std": 0.011235977408072787,
        "ci_lower": 0.09453416706258093,
        "ci_upper": 0.1390106144998748
      }
    }
  },
  "('ebg', 'LogReg', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.3856916325461293,
        "std": 0.15846714534046194,
        "ci_lower": 0.09041917374041253,
        "ci_upper": 0.7120475068146469
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.5871807296145594,
        "std": 0.15710552141990033,
        "ci_lower": 0.2762991532401892,
        "ci_upper": 0.9039691609871225
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.3103342538816277,
        "std": 0.15455217501058324,
        "ci_lower": 0.03112129258558013,
        "ci_upper": 0.6123480475515789
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.450865813772771,
        "std": 0.056070413788088025,
        "ci_lower": 2.3439448074556197,
        "ci_upper": 2.5657332747904817
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.40847750159732776,
        "std": 0.008319039144175018,
        "ci_lower": 0.39287985738882053,
        "ci_upper": 0.42567599005332274
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.63200934790205,
        "std": 0.006334144960901779,
        "ci_lower": 0.6195802515282751,
        "ci_upper": 0.6446168390320037
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.7792521765385623,
        "std": 0.0050301729039468265,
        "ci_lower": 0.7693105215005985,
        "ci_upper": 0.7889778318417409
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.9403571396876971,
        "std": 0.0025233599701198458,
        "ci_lower": 0.9356475121077117,
        "ci_upper": 0.9454710618525913
      }
    }
  },
  "('ebg', 'LogReg', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.15299619600834022,
        "std": 0.14738534934251338,
        "ci_lower": 0.0013418463174018505,
        "ci_upper": 0.47021615909362297
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.5510483394543533,
        "std": 0.05875720847454033,
        "ci_lower": 2.4301005922452052,
        "ci_upper": 2.6625373077706964
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.23514292091911862,
        "std": 0.007709568422124837,
        "ci_lower": 0.22002422275094216,
        "ci_upper": 0.2503438794394397
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9175397912600975,
        "std": 0.005157425661540368,
        "ci_lower": 0.9074003306880916,
        "ci_upper": 0.9274930177335181
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5122641013102327,
        "std": 0.007163773410619986,
        "ci_lower": 0.4983563555033465,
        "ci_upper": 0.5260892055463112
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.13174307125402818,
        "std": 0.012935882529444083,
        "ci_lower": 0.10660237104094152,
        "ci_upper": 0.15688440599358142
      }
    }
  },
  "('ebg', 'LogReg', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.1624620801804,
        "std": 0.14658379624989498,
        "ci_lower": 0.0013635951412811836,
        "ci_upper": 0.47563220964679537
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.15568188820254183,
        "std": 0.14760621546685265,
        "ci_lower": 0.0017371105046679197,
        "ci_upper": 0.47845250585278926
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.5661240340173985,
        "std": 0.05924543272374982,
        "ci_lower": 2.444830503259077,
        "ci_upper": 2.6749234636984283
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.26063246093645115,
        "std": 0.010218146070379668,
        "ci_lower": 0.24007489323475145,
        "ci_upper": 0.28045115620462724
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8955451294421632,
        "std": 0.0059834628125573315,
        "ci_lower": 0.883639010756068,
        "ci_upper": 0.9066868289085004
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5236125308354881,
        "std": 0.009258877677669794,
        "ci_lower": 0.5053067973733262,
        "ci_upper": 0.5416052823050213
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.14532233288687105,
        "std": 0.01407534123122959,
        "ci_lower": 0.11916303499863437,
        "ci_upper": 0.1735316164568115
      }
    }
  },
  "('ebg', 'SVM', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.5673920512467088,
        "std": 0.16039584259874629,
        "ci_lower": 0.24297131650077797,
        "ci_upper": 0.8857177907693298
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.7089638453592745,
        "std": 0.15101412560771843,
        "ci_lower": 0.4002635597677743,
        "ci_upper": 0.9574665688594319
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.2799944939417612,
        "std": 0.14863887407923318,
        "ci_lower": 0.02213893587879583,
        "ci_upper": 0.5686210947712528
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.6184701578795098,
        "std": 0.05757686697461955,
        "ci_lower": 2.5063424461677055,
        "ci_upper": 2.731423984441115
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.737878444181667,
        "std": 0.007583409642938012,
        "ci_lower": 0.7231689997421277,
        "ci_upper": 0.7528858713960088
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.3801324533023007,
        "std": 0.005656273332153806,
        "ci_lower": 0.369242753345407,
        "ci_upper": 0.39134683499830153
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.8934685693423933,
        "std": 0.0037548282142785866,
        "ci_lower": 0.8858247766483277,
        "ci_upper": 0.9005891914222716
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.9710407348354965,
        "std": 0.0014638135057954348,
        "ci_lower": 0.9680161945217899,
        "ci_upper": 0.9737741621542118
      }
    }
  },
  "('ebg', 'SVM', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.14804312034704448,
        "std": 0.14827037661761494,
        "ci_lower": 0.00031403625273251614,
        "ci_upper": 0.47305603631479515
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.235105671032439,
        "std": 0.06193421043674967,
        "ci_lower": 2.1128442241066425,
        "ci_upper": 2.3574148068858976
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.1644186716174388,
        "std": 0.007318897902716467,
        "ci_lower": 0.1507319514673021,
        "ci_upper": 0.1790756371023398
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9426784140371405,
        "std": 0.004015477569672091,
        "ci_lower": 0.93508601375926,
        "ci_upper": 0.9509075786020995
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.4800612844629548,
        "std": 0.006512491139760547,
        "ci_lower": 0.46710240365771316,
        "ci_upper": 0.49263599865508434
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.11657307336951336,
        "std": 0.011235977408072787,
        "ci_lower": 0.09453416706258093,
        "ci_upper": 0.1390106144998748
      }
    }
  },
  "('ebg', 'SVM', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.3856916325461293,
        "std": 0.15846714534046194,
        "ci_lower": 0.09041917374041253,
        "ci_upper": 0.7120475068146469
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.5557589926618536,
        "std": 0.15701870148218697,
        "ci_lower": 0.23779710445250826,
        "ci_upper": 0.8586864340374704
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.23988067954246797,
        "std": 0.14724514417503426,
        "ci_lower": 0.01932173247856558,
        "ci_upper": 0.5419648985089408
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.450865813772771,
        "std": 0.056070413788088025,
        "ci_lower": 2.3439448074556197,
        "ci_upper": 2.5657332747904817
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.40847750159732776,
        "std": 0.008319039144175018,
        "ci_lower": 0.39287985738882053,
        "ci_upper": 0.42567599005332274
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.63200934790205,
        "std": 0.006334144960901779,
        "ci_lower": 0.6195802515282751,
        "ci_upper": 0.6446168390320037
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.7792521765385623,
        "std": 0.0050301729039468265,
        "ci_lower": 0.7693105215005985,
        "ci_upper": 0.7889778318417409
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.9403571396876971,
        "std": 0.0025233599701198458,
        "ci_lower": 0.9356475121077117,
        "ci_upper": 0.9454710618525913
      }
    }
  },
  "('ebg', 'SVM', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.1624620801804,
        "std": 0.14658379624989498,
        "ci_lower": 0.0013635951412811836,
        "ci_upper": 0.47563220964679537
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.1520094395450043,
        "std": 0.14679598275130137,
        "ci_lower": 0.0015976690166630283,
        "ci_upper": 0.47379882838623044
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.5510483394543533,
        "std": 0.05875720847454033,
        "ci_lower": 2.4301005922452052,
        "ci_upper": 2.6625373077706964
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.23514292091911862,
        "std": 0.007709568422124837,
        "ci_lower": 0.22002422275094216,
        "ci_upper": 0.2503438794394397
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9175397912600975,
        "std": 0.005157425661540368,
        "ci_lower": 0.9074003306880916,
        "ci_upper": 0.9274930177335181
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5122641013102327,
        "std": 0.007163773410619986,
        "ci_lower": 0.4983563555033465,
        "ci_upper": 0.5260892055463112
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.13174307125402818,
        "std": 0.012935882529444083,
        "ci_lower": 0.10660237104094152,
        "ci_upper": 0.15688440599358142
      }
    }
  },
  "('ebg', 'SVM', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.19652666369483304,
        "std": 0.1479391672935303,
        "ci_lower": 0.003529935670758092,
        "ci_upper": 0.5073225092382845
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.1491786625306282,
        "std": 0.14147582922283494,
        "ci_lower": 0.0021231735297609134,
        "ci_upper": 0.45455346576064465
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.5661240340173985,
        "std": 0.05924543272374982,
        "ci_lower": 2.444830503259077,
        "ci_upper": 2.6749234636984283
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.26063246093645115,
        "std": 0.010218146070379668,
        "ci_lower": 0.24007489323475145,
        "ci_upper": 0.28045115620462724
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8955451294421632,
        "std": 0.0059834628125573315,
        "ci_lower": 0.883639010756068,
        "ci_upper": 0.9066868289085004
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5236125308354881,
        "std": 0.009258877677669794,
        "ci_lower": 0.5053067973733262,
        "ci_upper": 0.5416052823050213
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.14532233288687105,
        "std": 0.01407534123122959,
        "ci_lower": 0.11916303499863437,
        "ci_upper": 0.1735316164568115
      }
    }
  },
  "('ebg', 'RoBERTa', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.5871807296145594,
        "std": 0.15710552141990033,
        "ci_lower": 0.2762991532401892,
        "ci_upper": 0.9039691609871225
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.6780866458797011,
        "std": 0.15279232688093042,
        "ci_lower": 0.3611318773698095,
        "ci_upper": 0.9502484596498406
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.5572487516166925,
        "std": 0.15817707475190032,
        "ci_lower": 0.22817418495205288,
        "ci_upper": 0.8605246117265004
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.6184701578795098,
        "std": 0.05757686697461955,
        "ci_lower": 2.5063424461677055,
        "ci_upper": 2.731423984441115
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.737878444181667,
        "std": 0.007583409642938012,
        "ci_lower": 0.7231689997421277,
        "ci_upper": 0.7528858713960088
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.3801324533023007,
        "std": 0.005656273332153806,
        "ci_lower": 0.369242753345407,
        "ci_upper": 0.39134683499830153
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.8934685693423933,
        "std": 0.0037548282142785866,
        "ci_lower": 0.8858247766483277,
        "ci_upper": 0.9005891914222716
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.9710407348354965,
        "std": 0.0014638135057954348,
        "ci_lower": 0.9680161945217899,
        "ci_upper": 0.9737741621542118
      }
    }
  },
  "('ebg', 'RoBERTa', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.24604318958555685,
        "std": 0.1464063251877246,
        "ci_lower": 0.0166889840275688,
        "ci_upper": 0.5375058893552458
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.15057394704315208,
        "std": 0.1531363870054313,
        "ci_lower": 0.0013002724115966776,
        "ci_upper": 0.47912109992258356
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.235105671032439,
        "std": 0.06193421043674967,
        "ci_lower": 2.1128442241066425,
        "ci_upper": 2.3574148068858976
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.1644186716174388,
        "std": 0.007318897902716467,
        "ci_lower": 0.1507319514673021,
        "ci_upper": 0.1790756371023398
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9426784140371405,
        "std": 0.004015477569672091,
        "ci_lower": 0.93508601375926,
        "ci_upper": 0.9509075786020995
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.4800612844629548,
        "std": 0.006512491139760547,
        "ci_lower": 0.46710240365771316,
        "ci_upper": 0.49263599865508434
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.11657307336951336,
        "std": 0.011235977408072787,
        "ci_lower": 0.09453416706258093,
        "ci_upper": 0.1390106144998748
      }
    }
  },
  "('ebg', 'RoBERTa', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.5411251329825744,
        "std": 0.16004393772281902,
        "ci_lower": 0.23885691908773168,
        "ci_upper": 0.8726573184758236
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.66310346446024,
        "std": 0.15320939802705993,
        "ci_lower": 0.3558544575822297,
        "ci_upper": 0.9492625627773653
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.4987033459904013,
        "std": 0.15665609386153867,
        "ci_lower": 0.18089393735744214,
        "ci_upper": 0.8085872503932051
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.450865813772771,
        "std": 0.056070413788088025,
        "ci_lower": 2.3439448074556197,
        "ci_upper": 2.5657332747904817
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.40847750159732776,
        "std": 0.008319039144175018,
        "ci_lower": 0.39287985738882053,
        "ci_upper": 0.42567599005332274
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.63200934790205,
        "std": 0.006334144960901779,
        "ci_lower": 0.6195802515282751,
        "ci_upper": 0.6446168390320037
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.7792521765385623,
        "std": 0.0050301729039468265,
        "ci_lower": 0.7693105215005985,
        "ci_upper": 0.7889778318417409
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.9403571396876971,
        "std": 0.0025233599701198458,
        "ci_lower": 0.9356475121077117,
        "ci_upper": 0.9454710618525913
      }
    }
  },
  "('ebg', 'RoBERTa', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.24604318958555685,
        "std": 0.1464063251877246,
        "ci_lower": 0.0166889840275688,
        "ci_upper": 0.5375058893552458
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.14680699809170994,
        "std": 0.15661291454979712,
        "ci_lower": 0.0010424505912193642,
        "ci_upper": 0.49458671449828756
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.5510483394543533,
        "std": 0.05875720847454033,
        "ci_lower": 2.4301005922452052,
        "ci_upper": 2.6625373077706964
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.23514292091911862,
        "std": 0.007709568422124837,
        "ci_lower": 0.22002422275094216,
        "ci_upper": 0.2503438794394397
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9175397912600975,
        "std": 0.005157425661540368,
        "ci_lower": 0.9074003306880916,
        "ci_upper": 0.9274930177335181
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5122641013102327,
        "std": 0.007163773410619986,
        "ci_lower": 0.4983563555033465,
        "ci_upper": 0.5260892055463112
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.13174307125402818,
        "std": 0.012935882529444083,
        "ci_lower": 0.10660237104094152,
        "ci_upper": 0.15688440599358142
      }
    }
  },
  "('ebg', 'RoBERTa', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.21696600533925262,
        "std": 0.15097396474024674,
        "ci_lower": 0.006336258248271914,
        "ci_upper": 0.5254646587636552
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.14166644518313795,
        "std": 0.1479202979079214,
        "ci_lower": 0.0026584375537213856,
        "ci_upper": 0.4661673530064737
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.5661240340173985,
        "std": 0.05924543272374982,
        "ci_lower": 2.444830503259077,
        "ci_upper": 2.6749234636984283
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.26063246093645115,
        "std": 0.010218146070379668,
        "ci_lower": 0.24007489323475145,
        "ci_upper": 0.28045115620462724
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8955451294421632,
        "std": 0.0059834628125573315,
        "ci_lower": 0.883639010756068,
        "ci_upper": 0.9066868289085004
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5236125308354881,
        "std": 0.009258877677669794,
        "ci_lower": 0.5053067973733262,
        "ci_upper": 0.5416052823050213
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.14532233288687105,
        "std": 0.01407534123122959,
        "ci_lower": 0.11916303499863437,
        "ci_upper": 0.1735316164568115
      }
    }
  }
}