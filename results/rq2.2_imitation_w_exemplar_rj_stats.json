{
  "('rj', 'LogReg', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.18846231410814182,
        "std": 0.1500545448369251,
        "ci_lower": 0.0033320238465429286,
        "ci_upper": 0.5077396285681862
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 2.356296428400208,
        "std": 0.06386801545123133,
        "ci_lower": 2.2334664004568543,
        "ci_upper": 2.485472278590404
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.30621485830261813,
        "std": 0.006091599161000617,
        "ci_lower": 0.29416522806940626,
        "ci_upper": 0.3179434016762194
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8408443741155716,
        "std": 0.007808755795077058,
        "ci_lower": 0.8249267157437716,
        "ci_upper": 0.8556520994704753
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6704487432172742,
        "std": 0.006112225896956975,
        "ci_lower": 0.6588008076707774,
        "ci_upper": 0.6827267827717154
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.7037293739816944,
        "std": 0.011182478086728226,
        "ci_lower": 0.6816937703178034,
        "ci_upper": 0.7253608764945019
      }
    }
  },
  "('rj', 'LogReg', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3856916325461293,
        "std": 0.15846714534046194,
        "ci_lower": 0.09041917374041253,
        "ci_upper": 0.7120475068146469
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.20351164246920825,
        "std": 0.14073832760359833,
        "ci_lower": 0.012484964540617271,
        "ci_upper": 0.49553671661024973
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 2.152710243954618,
        "std": 0.06635035726166891,
        "ci_lower": 2.03034820412849,
        "ci_upper": 2.290354554863128
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.19914753472361518,
        "std": 0.004770534096095885,
        "ci_lower": 0.18969761080296976,
        "ci_upper": 0.2082932268486715
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9365067420371124,
        "std": 0.005843591460692673,
        "ci_lower": 0.9247578850648317,
        "ci_upper": 0.9473576978265512
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5161106084806363,
        "std": 0.005766829459647013,
        "ci_lower": 0.5040648578957057,
        "ci_upper": 0.526888277590042
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.4610377651258993,
        "std": 0.013068743443610411,
        "ci_lower": 0.4346961364329386,
        "ci_upper": 0.4860053487157337
      }
    }
  },
  "('rj', 'LogReg', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.24972864234683648,
        "std": 0.14764893441382537,
        "ci_lower": 0.016454105822673887,
        "ci_upper": 0.5440364364782752
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.23424518684854803,
        "std": 0.14596585203887888,
        "ci_lower": 0.012600189142309592,
        "ci_upper": 0.5285398417048143
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.8486979028505421,
        "std": 0.0628843610614239,
        "ci_lower": 1.723929224851782,
        "ci_upper": 1.9704031871228966
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.24588336580445225,
        "std": 0.005526348225343119,
        "ci_lower": 0.23519440055531687,
        "ci_upper": 0.25683303549019465
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8791661957858755,
        "std": 0.007829032050834314,
        "ci_lower": 0.8639182387938542,
        "ci_upper": 0.8944918536329765
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6135297915863933,
        "std": 0.00616035917749238,
        "ci_lower": 0.6011334396227198,
        "ci_upper": 0.6252657752342216
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.6532772474400388,
        "std": 0.011830078527324987,
        "ci_lower": 0.6288862059694825,
        "ci_upper": 0.6755411018688194
      }
    }
  },
  "('rj', 'LogReg', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.2081472616390027,
        "std": 0.14786987708062355,
        "ci_lower": 0.005108297714425741,
        "ci_upper": 0.512820465260949
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 2.375258580854012,
        "std": 0.06165288179766145,
        "ci_lower": 2.2498968682852363,
        "ci_upper": 2.4923138910503035
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.2225208216304932,
        "std": 0.004930218602451855,
        "ci_lower": 0.21326402074147022,
        "ci_upper": 0.2324428775253508
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9365080369397699,
        "std": 0.005193719361632272,
        "ci_lower": 0.9260208184312183,
        "ci_upper": 0.9462768096645876
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5200610516906333,
        "std": 0.005815595011332266,
        "ci_lower": 0.508228229584472,
        "ci_upper": 0.5310070527330099
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.4946705431449062,
        "std": 0.012439141367916589,
        "ci_lower": 0.46936027490567506,
        "ci_upper": 0.5178888801278189
      }
    }
  },
  "('rj', 'LogReg', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Ministral",
    "effectiveness_estimates": {},
    "quality_estimates": {}
  },
  "('rj', 'SVM', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.19262668537391886,
        "std": 0.14384810046313398,
        "ci_lower": 0.0038440505069259824,
        "ci_upper": 0.48963933951477423
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 2.356296428400208,
        "std": 0.06386801545123133,
        "ci_lower": 2.2334664004568543,
        "ci_upper": 2.485472278590404
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.30621485830261813,
        "std": 0.006091599161000617,
        "ci_lower": 0.29416522806940626,
        "ci_upper": 0.3179434016762194
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8408443741155716,
        "std": 0.007808755795077058,
        "ci_lower": 0.8249267157437716,
        "ci_upper": 0.8556520994704753
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6704487432172742,
        "std": 0.006112225896956975,
        "ci_lower": 0.6588008076707774,
        "ci_upper": 0.6827267827717154
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.7037293739816944,
        "std": 0.011182478086728226,
        "ci_lower": 0.6816937703178034,
        "ci_upper": 0.7253608764945019
      }
    }
  },
  "('rj', 'SVM', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.3856916325461293,
        "std": 0.15846714534046194,
        "ci_lower": 0.09041917374041253,
        "ci_upper": 0.7120475068146469
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.19541251763420509,
        "std": 0.1407783081375284,
        "ci_lower": 0.0039708438753285275,
        "ci_upper": 0.4872662819937059
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 2.152710243954618,
        "std": 0.06635035726166891,
        "ci_lower": 2.03034820412849,
        "ci_upper": 2.290354554863128
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.19914753472361518,
        "std": 0.004770534096095885,
        "ci_lower": 0.18969761080296976,
        "ci_upper": 0.2082932268486715
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9365067420371124,
        "std": 0.005843591460692673,
        "ci_lower": 0.9247578850648317,
        "ci_upper": 0.9473576978265512
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5161106084806363,
        "std": 0.005766829459647013,
        "ci_lower": 0.5040648578957057,
        "ci_upper": 0.526888277590042
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.4610377651258993,
        "std": 0.013068743443610411,
        "ci_lower": 0.4346961364329386,
        "ci_upper": 0.4860053487157337
      }
    }
  },
  "('rj', 'SVM', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.19570886690810532,
        "std": 0.14656887443301964,
        "ci_lower": 0.007083559429206173,
        "ci_upper": 0.5036484833788016
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.8486979028505421,
        "std": 0.0628843610614239,
        "ci_lower": 1.723929224851782,
        "ci_upper": 1.9704031871228966
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.24588336580445225,
        "std": 0.005526348225343119,
        "ci_lower": 0.23519440055531687,
        "ci_upper": 0.25683303549019465
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8791661957858755,
        "std": 0.007829032050834314,
        "ci_lower": 0.8639182387938542,
        "ci_upper": 0.8944918536329765
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6135297915863933,
        "std": 0.00616035917749238,
        "ci_lower": 0.6011334396227198,
        "ci_upper": 0.6252657752342216
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.6532772474400388,
        "std": 0.011830078527324987,
        "ci_lower": 0.6288862059694825,
        "ci_upper": 0.6755411018688194
      }
    }
  },
  "('rj', 'SVM', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.4175914408028651,
        "std": 0.15604192512739995,
        "ci_lower": 0.10180037803387029,
        "ci_upper": 0.7192677877613619
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.19852405885855273,
        "std": 0.15010636240234695,
        "ci_lower": 0.005135426433113894,
        "ci_upper": 0.5055058192872002
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 2.375258580854012,
        "std": 0.06165288179766145,
        "ci_lower": 2.2498968682852363,
        "ci_upper": 2.4923138910503035
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.2225208216304932,
        "std": 0.004930218602451855,
        "ci_lower": 0.21326402074147022,
        "ci_upper": 0.2324428775253508
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9365080369397699,
        "std": 0.005193719361632272,
        "ci_lower": 0.9260208184312183,
        "ci_upper": 0.9462768096645876
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5200610516906333,
        "std": 0.005815595011332266,
        "ci_lower": 0.508228229584472,
        "ci_upper": 0.5310070527330099
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.4946705431449062,
        "std": 0.012439141367916589,
        "ci_lower": 0.46936027490567506,
        "ci_upper": 0.5178888801278189
      }
    }
  },
  "('rj', 'SVM', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Ministral",
    "effectiveness_estimates": {},
    "quality_estimates": {}
  },
  "('rj', 'RoBERTa', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.4175914408028651,
        "std": 0.15604192512739995,
        "ci_lower": 0.10180037803387029,
        "ci_upper": 0.7192677877613619
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.2193532679368339,
        "std": 0.1439112054100696,
        "ci_lower": 0.01474822989783891,
        "ci_upper": 0.5122556951198924
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 2.356296428400208,
        "std": 0.06386801545123133,
        "ci_lower": 2.2334664004568543,
        "ci_upper": 2.485472278590404
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.30621485830261813,
        "std": 0.006091599161000617,
        "ci_lower": 0.29416522806940626,
        "ci_upper": 0.3179434016762194
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8408443741155716,
        "std": 0.007808755795077058,
        "ci_lower": 0.8249267157437716,
        "ci_upper": 0.8556520994704753
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6704487432172742,
        "std": 0.006112225896956975,
        "ci_lower": 0.6588008076707774,
        "ci_upper": 0.6827267827717154
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.7037293739816944,
        "std": 0.011182478086728226,
        "ci_lower": 0.6816937703178034,
        "ci_upper": 0.7253608764945019
      }
    }
  },
  "('rj', 'RoBERTa', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.1860993925559181,
        "std": 0.14329934817774867,
        "ci_lower": 0.006568387166085445,
        "ci_upper": 0.4755047520241148
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 2.152710243954618,
        "std": 0.06635035726166891,
        "ci_lower": 2.03034820412849,
        "ci_upper": 2.290354554863128
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.19914753472361518,
        "std": 0.004770534096095885,
        "ci_lower": 0.18969761080296976,
        "ci_upper": 0.2082932268486715
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9365067420371124,
        "std": 0.005843591460692673,
        "ci_lower": 0.9247578850648317,
        "ci_upper": 0.9473576978265512
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5161106084806363,
        "std": 0.005766829459647013,
        "ci_lower": 0.5040648578957057,
        "ci_upper": 0.526888277590042
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.4610377651258993,
        "std": 0.013068743443610411,
        "ci_lower": 0.4346961364329386,
        "ci_upper": 0.4860053487157337
      }
    }
  },
  "('rj', 'RoBERTa', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.16994658828478465,
        "std": 0.1438385037789792,
        "ci_lower": 0.0033027571699339397,
        "ci_upper": 0.4797792352367905
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.8486979028505421,
        "std": 0.0628843610614239,
        "ci_lower": 1.723929224851782,
        "ci_upper": 1.9704031871228966
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.24588336580445225,
        "std": 0.005526348225343119,
        "ci_lower": 0.23519440055531687,
        "ci_upper": 0.25683303549019465
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8791661957858755,
        "std": 0.007829032050834314,
        "ci_lower": 0.8639182387938542,
        "ci_upper": 0.8944918536329765
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6135297915863933,
        "std": 0.00616035917749238,
        "ci_lower": 0.6011334396227198,
        "ci_upper": 0.6252657752342216
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.6532772474400388,
        "std": 0.011830078527324987,
        "ci_lower": 0.6288862059694825,
        "ci_upper": 0.6755411018688194
      }
    }
  },
  "('rj', 'RoBERTa', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.15578021453070767,
        "std": 0.14652225267917338,
        "ci_lower": 0.0016161218267262984,
        "ci_upper": 0.4639909629869023
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 2.375258580854012,
        "std": 0.06165288179766145,
        "ci_lower": 2.2498968682852363,
        "ci_upper": 2.4923138910503035
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.2225208216304932,
        "std": 0.004930218602451855,
        "ci_lower": 0.21326402074147022,
        "ci_upper": 0.2324428775253508
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9365080369397699,
        "std": 0.005193719361632272,
        "ci_lower": 0.9260208184312183,
        "ci_upper": 0.9462768096645876
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5200610516906333,
        "std": 0.005815595011332266,
        "ci_lower": 0.508228229584472,
        "ci_upper": 0.5310070527330099
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.4946705431449062,
        "std": 0.012439141367916589,
        "ci_lower": 0.46936027490567506,
        "ci_upper": 0.5178888801278189
      }
    }
  },
  "('rj', 'RoBERTa', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Ministral",
    "effectiveness_estimates": {},
    "quality_estimates": {}
  }
}