{
  "('ebg', 'LogReg', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.19652666369483304,
        "std": 0.1479391672935303,
        "ci_lower": 0.003529935670758092,
        "ci_upper": 0.5073225092382845
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.14968384867230447,
        "std": 0.1473166810203464,
        "ci_lower": 0.002630694216915941,
        "ci_upper": 0.4681123591020166
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.5751558410953717,
        "std": 0.05599143726301565,
        "ci_lower": 2.4657934934997896,
        "ci_upper": 2.6877291349268706
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.17100444241850282,
        "std": 0.0028023921779610043,
        "ci_lower": 0.16528791666160175,
        "ci_upper": 0.1764136690166213
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9445741123702214,
        "std": 0.003841140353005279,
        "ci_lower": 0.9374045905698408,
        "ci_upper": 0.9521518435972414
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.48456429907334725,
        "std": 0.0034498293980205503,
        "ci_lower": 0.4779163553604186,
        "ci_upper": 0.4914454479730269
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.1414352005005387,
        "std": 0.011921824411445566,
        "ci_lower": 0.11888863901521524,
        "ci_upper": 0.16513738904258465
      }
    }
  },
  "('ebg', 'LogReg', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.1546841516944741,
        "std": 0.14944905428527305,
        "ci_lower": 0.002252536812356271,
        "ci_upper": 0.4751814549496288
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 3.0645634577594714,
        "std": 0.0548381750396964,
        "ci_lower": 2.960720703250834,
        "ci_upper": 3.174722019296194
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.19675427119642594,
        "std": 0.0030464953557507047,
        "ci_lower": 0.19074379939724556,
        "ci_upper": 0.20257786993725255
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9524498184895095,
        "std": 0.00318317015273105,
        "ci_lower": 0.9459972791333487,
        "ci_upper": 0.9583265463364707
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.49400816658633073,
        "std": 0.003673103264093104,
        "ci_lower": 0.4869159502452545,
        "ci_upper": 0.5011457215998477
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.1463379932172824,
        "std": 0.011537727453260947,
        "ci_lower": 0.12411793004681071,
        "ci_upper": 0.16836876598058456
      }
    }
  },
  "('ebg', 'LogReg', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.27495671170619695,
        "std": 0.14960014028463836,
        "ci_lower": 0.02937024618980144,
        "ci_upper": 0.580229830035496
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.36867571265835164,
        "std": 0.15586411849818388,
        "ci_lower": 0.06415296453954625,
        "ci_upper": 0.6762136767532781
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.23725621592676413,
        "std": 0.14410321770164222,
        "ci_lower": 0.017382494578799798,
        "ci_upper": 0.531195002937852
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 3.0289626914023335,
        "std": 0.0548013928620194,
        "ci_lower": 2.918802621237878,
        "ci_upper": 3.133050996098403
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.33133450964653444,
        "std": 0.004692088604469252,
        "ci_lower": 0.32287326495640023,
        "ci_upper": 0.3411589649912555
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8253609727250567,
        "std": 0.005243839050146941,
        "ci_lower": 0.8145459603742256,
        "ci_upper": 0.8354015204257785
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6793490801988249,
        "std": 0.004135582548228773,
        "ci_lower": 0.6709782293545579,
        "ci_upper": 0.6870879347168448
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8383694701884281,
        "std": 0.005194002990199779,
        "ci_lower": 0.828342121888831,
        "ci_upper": 0.8486851478127428
      }
    }
  },
  "('ebg', 'LogReg', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.27495671170619695,
        "std": 0.14960014028463836,
        "ci_lower": 0.02937024618980144,
        "ci_upper": 0.580229830035496
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.3856916325461293,
        "std": 0.15846714534046194,
        "ci_lower": 0.09041917374041253,
        "ci_upper": 0.7120475068146469
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.23995106748887204,
        "std": 0.14769674657991366,
        "ci_lower": 0.014470527785148637,
        "ci_upper": 0.5410488247296379
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.8752109227940053,
        "std": 0.053097541766137235,
        "ci_lower": 2.7728463554357754,
        "ci_upper": 2.9816077082777688
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.22393703530028775,
        "std": 0.003349688810040303,
        "ci_lower": 0.2174817093276241,
        "ci_upper": 0.23063995287567826
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8896126600931475,
        "std": 0.005042310212971971,
        "ci_lower": 0.8801475384758899,
        "ci_upper": 0.8998749230090267
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.585770555914271,
        "std": 0.0035403105161633057,
        "ci_lower": 0.5787176380224813,
        "ci_upper": 0.5926348098658579
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.7346946600659462,
        "std": 0.006635212662137384,
        "ci_lower": 0.721241133735762,
        "ci_upper": 0.7475768344287974
      }
    }
  },
  "('ebg', 'LogReg', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.3022908454126393,
        "std": 0.15290620134623778,
        "ci_lower": 0.04237316748200466,
        "ci_upper": 0.6152740587683058
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.16266831304251048,
        "std": 0.15477234443449506,
        "ci_lower": 0.0010621824376779525,
        "ci_upper": 0.5037108067434934
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.7531310921279006,
        "std": 0.056016216594312926,
        "ci_lower": 2.6436495224200462,
        "ci_upper": 2.862429078811172
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.1739663077927347,
        "std": 0.003092480840501757,
        "ci_lower": 0.16797059402102385,
        "ci_upper": 0.18017172876600085
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9442855464930371,
        "std": 0.003687422097181717,
        "ci_lower": 0.93671895069706,
        "ci_upper": 0.9511334092792036
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.4882820533588463,
        "std": 0.003690379518934367,
        "ci_lower": 0.48138350872316704,
        "ci_upper": 0.4959270338761625
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.13182409010063623,
        "std": 0.012142507783828208,
        "ci_lower": 0.1082794804004019,
        "ci_upper": 0.15530203278809815
      }
    }
  },
  "('ebg', 'SVM', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.18400042185528304,
        "std": 0.15142585952723722,
        "ci_lower": 0.005111049814274888,
        "ci_upper": 0.5046177424211228
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.27495671170619695,
        "std": 0.14960014028463836,
        "ci_lower": 0.02937024618980144,
        "ci_upper": 0.580229830035496
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.17260470461253233,
        "std": 0.14656627040366343,
        "ci_lower": 0.0031114451415373647,
        "ci_upper": 0.4886591257677677
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.5751558410953717,
        "std": 0.05599143726301565,
        "ci_lower": 2.4657934934997896,
        "ci_upper": 2.6877291349268706
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.17100444241850282,
        "std": 0.0028023921779610043,
        "ci_lower": 0.16528791666160175,
        "ci_upper": 0.1764136690166213
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9445741123702214,
        "std": 0.003841140353005279,
        "ci_lower": 0.9374045905698408,
        "ci_upper": 0.9521518435972414
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.48456429907334725,
        "std": 0.0034498293980205503,
        "ci_lower": 0.4779163553604186,
        "ci_upper": 0.4914454479730269
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.1414352005005387,
        "std": 0.011921824411445566,
        "ci_lower": 0.11888863901521524,
        "ci_upper": 0.16513738904258465
      }
    }
  },
  "('ebg', 'SVM', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.19652666369483304,
        "std": 0.1479391672935303,
        "ci_lower": 0.003529935670758092,
        "ci_upper": 0.5073225092382845
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.1630692092962724,
        "std": 0.15059529492279797,
        "ci_lower": 0.0038037728698552464,
        "ci_upper": 0.48489095200790205
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 3.0645634577594714,
        "std": 0.0548381750396964,
        "ci_lower": 2.960720703250834,
        "ci_upper": 3.174722019296194
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.19675427119642594,
        "std": 0.0030464953557507047,
        "ci_lower": 0.19074379939724556,
        "ci_upper": 0.20257786993725255
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9524498184895095,
        "std": 0.00318317015273105,
        "ci_lower": 0.9459972791333487,
        "ci_upper": 0.9583265463364707
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.49400816658633073,
        "std": 0.003673103264093104,
        "ci_lower": 0.4869159502452545,
        "ci_upper": 0.5011457215998477
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.1463379932172824,
        "std": 0.011537727453260947,
        "ci_lower": 0.12411793004681071,
        "ci_upper": 0.16836876598058456
      }
    }
  },
  "('ebg', 'SVM', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.24604318958555685,
        "std": 0.1464063251877246,
        "ci_lower": 0.0166889840275688,
        "ci_upper": 0.5375058893552458
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.44881224203247433,
        "std": 0.15561118466988502,
        "ci_lower": 0.14032618475157368,
        "ci_upper": 0.7577125470424216
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.19534570712759636,
        "std": 0.14627477230169617,
        "ci_lower": 0.0079936769699563,
        "ci_upper": 0.5015054953361568
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 3.0289626914023335,
        "std": 0.0548013928620194,
        "ci_lower": 2.918802621237878,
        "ci_upper": 3.133050996098403
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.33133450964653444,
        "std": 0.004692088604469252,
        "ci_lower": 0.32287326495640023,
        "ci_upper": 0.3411589649912555
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8253609727250567,
        "std": 0.005243839050146941,
        "ci_lower": 0.8145459603742256,
        "ci_upper": 0.8354015204257785
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6793490801988249,
        "std": 0.004135582548228773,
        "ci_lower": 0.6709782293545579,
        "ci_upper": 0.6870879347168448
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8383694701884281,
        "std": 0.005194002990199779,
        "ci_lower": 0.828342121888831,
        "ci_upper": 0.8486851478127428
      }
    }
  },
  "('ebg', 'SVM', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.21696600533925262,
        "std": 0.15097396474024674,
        "ci_lower": 0.006336258248271914,
        "ci_upper": 0.5254646587636552
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.3856916325461293,
        "std": 0.15846714534046194,
        "ci_lower": 0.09041917374041253,
        "ci_upper": 0.7120475068146469
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.1839092597835405,
        "std": 0.14783374471415114,
        "ci_lower": 0.006157368063857374,
        "ci_upper": 0.4911825786042554
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.8752109227940053,
        "std": 0.053097541766137235,
        "ci_lower": 2.7728463554357754,
        "ci_upper": 2.9816077082777688
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.22393703530028775,
        "std": 0.003349688810040303,
        "ci_lower": 0.2174817093276241,
        "ci_upper": 0.23063995287567826
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8896126600931475,
        "std": 0.005042310212971971,
        "ci_lower": 0.8801475384758899,
        "ci_upper": 0.8998749230090267
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.585770555914271,
        "std": 0.0035403105161633057,
        "ci_lower": 0.5787176380224813,
        "ci_upper": 0.5926348098658579
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.7346946600659462,
        "std": 0.006635212662137384,
        "ci_lower": 0.721241133735762,
        "ci_upper": 0.7475768344287974
      }
    }
  },
  "('ebg', 'SVM', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.1624620801804,
        "std": 0.14658379624989498,
        "ci_lower": 0.0013635951412811836,
        "ci_upper": 0.47563220964679537
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.2605938262246618,
        "std": 0.14752966112542545,
        "ci_lower": 0.020784435919778667,
        "ci_upper": 0.5635640253180653
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.16526127786434813,
        "std": 0.14987449817242296,
        "ci_lower": 0.00372110653452012,
        "ci_upper": 0.4835066034424779
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.7531310921279006,
        "std": 0.056016216594312926,
        "ci_lower": 2.6436495224200462,
        "ci_upper": 2.862429078811172
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.1739663077927347,
        "std": 0.003092480840501757,
        "ci_lower": 0.16797059402102385,
        "ci_upper": 0.18017172876600085
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9442855464930371,
        "std": 0.003687422097181717,
        "ci_lower": 0.93671895069706,
        "ci_upper": 0.9511334092792036
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.4882820533588463,
        "std": 0.003690379518934367,
        "ci_lower": 0.48138350872316704,
        "ci_upper": 0.4959270338761625
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.13182409010063623,
        "std": 0.012142507783828208,
        "ci_lower": 0.1082794804004019,
        "ci_upper": 0.15530203278809815
      }
    }
  },
  "('ebg', 'RoBERTa', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.19652666369483304,
        "std": 0.1479391672935303,
        "ci_lower": 0.003529935670758092,
        "ci_upper": 0.5073225092382845
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.15138510547849107,
        "std": 0.15703987500307007,
        "ci_lower": 0.001965405382782099,
        "ci_upper": 0.5036574185719175
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.5751558410953717,
        "std": 0.05599143726301565,
        "ci_lower": 2.4657934934997896,
        "ci_upper": 2.6877291349268706
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.17100444241850282,
        "std": 0.0028023921779610043,
        "ci_lower": 0.16528791666160175,
        "ci_upper": 0.1764136690166213
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9445741123702214,
        "std": 0.003841140353005279,
        "ci_lower": 0.9374045905698408,
        "ci_upper": 0.9521518435972414
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.48456429907334725,
        "std": 0.0034498293980205503,
        "ci_lower": 0.4779163553604186,
        "ci_upper": 0.4914454479730269
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.1414352005005387,
        "std": 0.011921824411445566,
        "ci_lower": 0.11888863901521524,
        "ci_upper": 0.16513738904258465
      }
    }
  },
  "('ebg', 'RoBERTa', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.14986120325257787,
        "std": 0.14929407601953268,
        "ci_lower": 0.0013361770071915347,
        "ci_upper": 0.47705855040849143
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 3.0645634577594714,
        "std": 0.0548381750396964,
        "ci_lower": 2.960720703250834,
        "ci_upper": 3.174722019296194
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.19675427119642594,
        "std": 0.0030464953557507047,
        "ci_lower": 0.19074379939724556,
        "ci_upper": 0.20257786993725255
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9524498184895095,
        "std": 0.00318317015273105,
        "ci_lower": 0.9459972791333487,
        "ci_upper": 0.9583265463364707
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.49400816658633073,
        "std": 0.003673103264093104,
        "ci_lower": 0.4869159502452545,
        "ci_upper": 0.5011457215998477
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.1463379932172824,
        "std": 0.011537727453260947,
        "ci_lower": 0.12411793004681071,
        "ci_upper": 0.16836876598058456
      }
    }
  },
  "('ebg', 'RoBERTa', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.47851621696186136,
        "std": 0.15989594199992885,
        "ci_lower": 0.17484717694510316,
        "ci_upper": 0.8001920413124228
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.6344167095090265,
        "std": 0.15340999474227324,
        "ci_lower": 0.30933306480478934,
        "ci_upper": 0.912070466627938
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.4298458747470915,
        "std": 0.16000487947294598,
        "ci_lower": 0.09745355646630673,
        "ci_upper": 0.7292703983102445
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 3.0289626914023335,
        "std": 0.0548013928620194,
        "ci_lower": 2.918802621237878,
        "ci_upper": 3.133050996098403
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.33133450964653444,
        "std": 0.004692088604469252,
        "ci_lower": 0.32287326495640023,
        "ci_upper": 0.3411589649912555
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8253609727250567,
        "std": 0.005243839050146941,
        "ci_lower": 0.8145459603742256,
        "ci_upper": 0.8354015204257785
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6793490801988249,
        "std": 0.004135582548228773,
        "ci_lower": 0.6709782293545579,
        "ci_upper": 0.6870879347168448
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8383694701884281,
        "std": 0.005194002990199779,
        "ci_lower": 0.828342121888831,
        "ci_upper": 0.8486851478127428
      }
    }
  },
  "('ebg', 'RoBERTa', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.3856916325461293,
        "std": 0.15846714534046194,
        "ci_lower": 0.09041917374041253,
        "ci_upper": 0.7120475068146469
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.5261050989600696,
        "std": 0.16056385253809366,
        "ci_lower": 0.2114712343774928,
        "ci_upper": 0.848279573197231
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.3024318462543952,
        "std": 0.1508657101846206,
        "ci_lower": 0.03722959925353006,
        "ci_upper": 0.5978156589443572
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.8752109227940053,
        "std": 0.053097541766137235,
        "ci_lower": 2.7728463554357754,
        "ci_upper": 2.9816077082777688
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.22393703530028775,
        "std": 0.003349688810040303,
        "ci_lower": 0.2174817093276241,
        "ci_upper": 0.23063995287567826
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8896126600931475,
        "std": 0.005042310212971971,
        "ci_lower": 0.8801475384758899,
        "ci_upper": 0.8998749230090267
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.585770555914271,
        "std": 0.0035403105161633057,
        "ci_lower": 0.5787176380224813,
        "ci_upper": 0.5926348098658579
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.7346946600659462,
        "std": 0.006635212662137384,
        "ci_lower": 0.721241133735762,
        "ci_upper": 0.7475768344287974
      }
    }
  },
  "('ebg', 'RoBERTa', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.19652666369483304,
        "std": 0.1479391672935303,
        "ci_lower": 0.003529935670758092,
        "ci_upper": 0.5073225092382845
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.1530713997393821,
        "std": 0.15018257006899774,
        "ci_lower": 0.0007612117431694042,
        "ci_upper": 0.47539189736705195
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.7531310921279006,
        "std": 0.056016216594312926,
        "ci_lower": 2.6436495224200462,
        "ci_upper": 2.862429078811172
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.1739663077927347,
        "std": 0.003092480840501757,
        "ci_lower": 0.16797059402102385,
        "ci_upper": 0.18017172876600085
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9442855464930371,
        "std": 0.003687422097181717,
        "ci_lower": 0.93671895069706,
        "ci_upper": 0.9511334092792036
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.4882820533588463,
        "std": 0.003690379518934367,
        "ci_lower": 0.48138350872316704,
        "ci_upper": 0.4959270338761625
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.13182409010063623,
        "std": 0.012142507783828208,
        "ci_lower": 0.1082794804004019,
        "ci_upper": 0.15530203278809815
      }
    }
  }
}