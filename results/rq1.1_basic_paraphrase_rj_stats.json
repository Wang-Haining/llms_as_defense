{
  "('rj', 'LogReg', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.14386457153935725,
        "std": 0.15137680799545752,
        "ci_lower": 0.001660567377209305,
        "ci_upper": 0.47754665080810915
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.8460939775152017,
        "std": 0.07540427181769728,
        "ci_lower": 1.6941736499567595,
        "ci_upper": 1.989137869989896
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.20298815534573247,
        "std": 0.0049472509638097,
        "ci_lower": 0.19303006418500165,
        "ci_upper": 0.21235595727695294
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9255778098020951,
        "std": 0.006092268344766663,
        "ci_lower": 0.9129639848891234,
        "ci_upper": 0.9367085901989841
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5471812561474767,
        "std": 0.005702186388087124,
        "ci_lower": 0.5357721531096247,
        "ci_upper": 0.558157082473651
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5170134402007295,
        "std": 0.00889027166114832,
        "ci_lower": 0.49941120985581294,
        "ci_upper": 0.5340866030009337
      }
    }
  },
  "('rj', 'LogReg', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.2076245475447945,
        "std": 0.14172790251385575,
        "ci_lower": 0.014022500225378428,
        "ci_upper": 0.5009747337990074
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.745243902957133,
        "std": 0.07228913583552662,
        "ci_lower": 1.603518549420548,
        "ci_upper": 1.8831084157846238
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.45749032789583327,
        "std": 0.008604958842211526,
        "ci_lower": 0.44120658098870474,
        "ci_upper": 0.4749814984630173
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.637717036047718,
        "std": 0.009338527628927376,
        "ci_lower": 0.6191017009704173,
        "ci_upper": 0.6558306212027741
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.8208828249721511,
        "std": 0.005068101910664631,
        "ci_lower": 0.811163474331821,
        "ci_upper": 0.830905099247726
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8997044940246628,
        "std": 0.004955146163749169,
        "ci_lower": 0.890001948768002,
        "ci_upper": 0.9092137107261157
      }
    }
  },
  "('rj', 'LogReg', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.24972864234683648,
        "std": 0.14764893441382537,
        "ci_lower": 0.016454105822673887,
        "ci_upper": 0.5440364364782752
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.19028329978259778,
        "std": 0.1538199346970663,
        "ci_lower": 0.005057363481547367,
        "ci_upper": 0.5241723078089986
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.5607431420639941,
        "std": 0.07399472423384391,
        "ci_lower": 1.4174540239833653,
        "ci_upper": 1.704022623826441
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.14656538397258442,
        "std": 0.004569746173397372,
        "ci_lower": 0.1375963696149602,
        "ci_upper": 0.15553645866070767
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9247343421712219,
        "std": 0.006571870077670784,
        "ci_lower": 0.9121199616218347,
        "ci_upper": 0.9376083299154715
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5372546967883924,
        "std": 0.005647120792369547,
        "ci_lower": 0.5259176689007118,
        "ci_upper": 0.5481373716793313
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5149693279574694,
        "std": 0.010281885338366032,
        "ci_lower": 0.49526626716645905,
        "ci_upper": 0.5358066251146165
      }
    }
  },
  "('rj', 'LogReg', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.19182932496376298,
        "std": 0.1515176349302091,
        "ci_lower": 0.0051763997358236415,
        "ci_upper": 0.5089977428038476
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.7275708566701984,
        "std": 0.07615890951325378,
        "ci_lower": 1.577913445682304,
        "ci_upper": 1.8776577685570663
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.15974428038517055,
        "std": 0.004899055597292067,
        "ci_lower": 0.14985791082141273,
        "ci_upper": 0.1689662218671711
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9171590205321047,
        "std": 0.006806270773078376,
        "ci_lower": 0.9038792118121346,
        "ci_upper": 0.9303915017165552
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.544666686911594,
        "std": 0.005658082860974196,
        "ci_lower": 0.5335451288795454,
        "ci_upper": 0.5555328373265292
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5309105371085832,
        "std": 0.009461982754386542,
        "ci_lower": 0.5123656871310789,
        "ci_upper": 0.5496774543752266
      }
    }
  },
  "('rj', 'LogReg', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.4175914408028651,
        "std": 0.15604192512739995,
        "ci_lower": 0.10180037803387029,
        "ci_upper": 0.7192677877613619
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.1965464229009184,
        "std": 0.14644548951672492,
        "ci_lower": 0.010263834177839996,
        "ci_upper": 0.5017417501829771
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.7724710273511513,
        "std": 0.07603766625320539,
        "ci_lower": 1.6253236693597015,
        "ci_upper": 1.9230869905773056
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.20835935626709562,
        "std": 0.00495242196373937,
        "ci_lower": 0.19860598695900494,
        "ci_upper": 0.21800156137570162
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8447027445727756,
        "std": 0.008175407911578442,
        "ci_lower": 0.8282685240220543,
        "ci_upper": 0.8602267524289574
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6724272157704623,
        "std": 0.005561044172632626,
        "ci_lower": 0.6614832546840895,
        "ci_upper": 0.6832361190230862
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.7931690209182275,
        "std": 0.00783883072581484,
        "ci_lower": 0.778471319449834,
        "ci_upper": 0.8089081452851365
      }
    }
  },
  "('rj', 'SVM', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.18326331142638363,
        "std": 0.14838107556890837,
        "ci_lower": 0.009123335645678885,
        "ci_upper": 0.49982309241999373
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.8460939775152017,
        "std": 0.07540427181769728,
        "ci_lower": 1.6941736499567595,
        "ci_upper": 1.989137869989896
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.20298815534573247,
        "std": 0.0049472509638097,
        "ci_lower": 0.19303006418500165,
        "ci_upper": 0.21235595727695294
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9255778098020951,
        "std": 0.006092268344766663,
        "ci_lower": 0.9129639848891234,
        "ci_upper": 0.9367085901989841
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5471812561474767,
        "std": 0.005702186388087124,
        "ci_lower": 0.5357721531096247,
        "ci_upper": 0.558157082473651
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5170134402007295,
        "std": 0.00889027166114832,
        "ci_lower": 0.49941120985581294,
        "ci_upper": 0.5340866030009337
      }
    }
  },
  "('rj', 'SVM', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.44949292108390054,
        "std": 0.15983506177789672,
        "ci_lower": 0.13335833594386848,
        "ci_upper": 0.7727533040958386
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.1985196907959452,
        "std": 0.14177682824326457,
        "ci_lower": 0.0038740302997876228,
        "ci_upper": 0.48914640408836524
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.745243902957133,
        "std": 0.07228913583552662,
        "ci_lower": 1.603518549420548,
        "ci_upper": 1.8831084157846238
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.45749032789583327,
        "std": 0.008604958842211526,
        "ci_lower": 0.44120658098870474,
        "ci_upper": 0.4749814984630173
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.637717036047718,
        "std": 0.009338527628927376,
        "ci_lower": 0.6191017009704173,
        "ci_upper": 0.6558306212027741
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.8208828249721511,
        "std": 0.005068101910664631,
        "ci_lower": 0.811163474331821,
        "ci_upper": 0.830905099247726
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8997044940246628,
        "std": 0.004955146163749169,
        "ci_lower": 0.890001948768002,
        "ci_upper": 0.9092137107261157
      }
    }
  },
  "('rj', 'SVM', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.18800952711777438,
        "std": 0.14986366859988798,
        "ci_lower": 0.003059317527249088,
        "ci_upper": 0.5028901739961548
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.5607431420639941,
        "std": 0.07399472423384391,
        "ci_lower": 1.4174540239833653,
        "ci_upper": 1.704022623826441
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.14656538397258442,
        "std": 0.004569746173397372,
        "ci_lower": 0.1375963696149602,
        "ci_upper": 0.15553645866070767
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9247343421712219,
        "std": 0.006571870077670784,
        "ci_lower": 0.9121199616218347,
        "ci_upper": 0.9376083299154715
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5372546967883924,
        "std": 0.005647120792369547,
        "ci_lower": 0.5259176689007118,
        "ci_upper": 0.5481373716793313
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5149693279574694,
        "std": 0.010281885338366032,
        "ci_lower": 0.49526626716645905,
        "ci_upper": 0.5358066251146165
      }
    }
  },
  "('rj', 'SVM', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.19300169927885757,
        "std": 0.1523270756630627,
        "ci_lower": 0.0038469062767446036,
        "ci_upper": 0.508501848460421
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.7275708566701984,
        "std": 0.07615890951325378,
        "ci_lower": 1.577913445682304,
        "ci_upper": 1.8776577685570663
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.15974428038517055,
        "std": 0.004899055597292067,
        "ci_lower": 0.14985791082141273,
        "ci_upper": 0.1689662218671711
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9171590205321047,
        "std": 0.006806270773078376,
        "ci_lower": 0.9038792118121346,
        "ci_upper": 0.9303915017165552
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.544666686911594,
        "std": 0.005658082860974196,
        "ci_lower": 0.5335451288795454,
        "ci_upper": 0.5555328373265292
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5309105371085832,
        "std": 0.009461982754386542,
        "ci_lower": 0.5123656871310789,
        "ci_upper": 0.5496774543752266
      }
    }
  },
  "('rj', 'SVM', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.3856916325461293,
        "std": 0.15846714534046194,
        "ci_lower": 0.09041917374041253,
        "ci_upper": 0.7120475068146469
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.19205517791056334,
        "std": 0.1495350319374555,
        "ci_lower": 0.005280760250813242,
        "ci_upper": 0.513028900444529
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.7724710273511513,
        "std": 0.07603766625320539,
        "ci_lower": 1.6253236693597015,
        "ci_upper": 1.9230869905773056
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.20835935626709562,
        "std": 0.00495242196373937,
        "ci_lower": 0.19860598695900494,
        "ci_upper": 0.21800156137570162
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8447027445727756,
        "std": 0.008175407911578442,
        "ci_lower": 0.8282685240220543,
        "ci_upper": 0.8602267524289574
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6724272157704623,
        "std": 0.005561044172632626,
        "ci_lower": 0.6614832546840895,
        "ci_upper": 0.6832361190230862
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.7931690209182275,
        "std": 0.00783883072581484,
        "ci_lower": 0.778471319449834,
        "ci_upper": 0.8089081452851365
      }
    }
  },
  "('rj', 'RoBERTa', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.24972864234683648,
        "std": 0.14764893441382537,
        "ci_lower": 0.016454105822673887,
        "ci_upper": 0.5440364364782752
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.16296949284711604,
        "std": 0.15278058700424962,
        "ci_lower": 0.0010285639491517211,
        "ci_upper": 0.493376693372008
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.8460939775152017,
        "std": 0.07540427181769728,
        "ci_lower": 1.6941736499567595,
        "ci_upper": 1.989137869989896
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.20298815534573247,
        "std": 0.0049472509638097,
        "ci_lower": 0.19303006418500165,
        "ci_upper": 0.21235595727695294
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9255778098020951,
        "std": 0.006092268344766663,
        "ci_lower": 0.9129639848891234,
        "ci_upper": 0.9367085901989841
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5471812561474767,
        "std": 0.005702186388087124,
        "ci_lower": 0.5357721531096247,
        "ci_upper": 0.558157082473651
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5170134402007295,
        "std": 0.00889027166114832,
        "ci_lower": 0.49941120985581294,
        "ci_upper": 0.5340866030009337
      }
    }
  },
  "('rj', 'RoBERTa', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.24972864234683648,
        "std": 0.14764893441382537,
        "ci_lower": 0.016454105822673887,
        "ci_upper": 0.5440364364782752
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.2475251261593561,
        "std": 0.14438824600665473,
        "ci_lower": 0.02376558400085567,
        "ci_upper": 0.5475237167708911
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.745243902957133,
        "std": 0.07228913583552662,
        "ci_lower": 1.603518549420548,
        "ci_upper": 1.8831084157846238
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.45749032789583327,
        "std": 0.008604958842211526,
        "ci_lower": 0.44120658098870474,
        "ci_upper": 0.4749814984630173
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.637717036047718,
        "std": 0.009338527628927376,
        "ci_lower": 0.6191017009704173,
        "ci_upper": 0.6558306212027741
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.8208828249721511,
        "std": 0.005068101910664631,
        "ci_lower": 0.811163474331821,
        "ci_upper": 0.830905099247726
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8997044940246628,
        "std": 0.004955146163749169,
        "ci_lower": 0.890001948768002,
        "ci_upper": 0.9092137107261157
      }
    }
  },
  "('rj', 'RoBERTa', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3856916325461293,
        "std": 0.15846714534046194,
        "ci_lower": 0.09041917374041253,
        "ci_upper": 0.7120475068146469
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.191520106733985,
        "std": 0.14782340902993224,
        "ci_lower": 0.00709902004645056,
        "ci_upper": 0.4985895131094469
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.5607431420639941,
        "std": 0.07399472423384391,
        "ci_lower": 1.4174540239833653,
        "ci_upper": 1.704022623826441
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.14656538397258442,
        "std": 0.004569746173397372,
        "ci_lower": 0.1375963696149602,
        "ci_upper": 0.15553645866070767
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9247343421712219,
        "std": 0.006571870077670784,
        "ci_lower": 0.9121199616218347,
        "ci_upper": 0.9376083299154715
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5372546967883924,
        "std": 0.005647120792369547,
        "ci_lower": 0.5259176689007118,
        "ci_upper": 0.5481373716793313
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5149693279574694,
        "std": 0.010281885338366032,
        "ci_lower": 0.49526626716645905,
        "ci_upper": 0.5358066251146165
      }
    }
  },
  "('rj', 'RoBERTa', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.14743542139038246,
        "std": 0.15264691433487723,
        "ci_lower": 0.0013420004717370902,
        "ci_upper": 0.48091077128865306
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.7275708566701984,
        "std": 0.07615890951325378,
        "ci_lower": 1.577913445682304,
        "ci_upper": 1.8776577685570663
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.15974428038517055,
        "std": 0.004899055597292067,
        "ci_lower": 0.14985791082141273,
        "ci_upper": 0.1689662218671711
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9171590205321047,
        "std": 0.006806270773078376,
        "ci_lower": 0.9038792118121346,
        "ci_upper": 0.9303915017165552
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.544666686911594,
        "std": 0.005658082860974196,
        "ci_lower": 0.5335451288795454,
        "ci_upper": 0.5555328373265292
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5309105371085832,
        "std": 0.009461982754386542,
        "ci_lower": 0.5123656871310789,
        "ci_upper": 0.5496774543752266
      }
    }
  },
  "('rj', 'RoBERTa', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.24972864234683648,
        "std": 0.14764893441382537,
        "ci_lower": 0.016454105822673887,
        "ci_upper": 0.5440364364782752
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.2418592332613904,
        "std": 0.14636899429735437,
        "ci_lower": 0.014664560064444835,
        "ci_upper": 0.5358628085451517
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.7724710273511513,
        "std": 0.07603766625320539,
        "ci_lower": 1.6253236693597015,
        "ci_upper": 1.9230869905773056
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.20835935626709562,
        "std": 0.00495242196373937,
        "ci_lower": 0.19860598695900494,
        "ci_upper": 0.21800156137570162
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8447027445727756,
        "std": 0.008175407911578442,
        "ci_lower": 0.8282685240220543,
        "ci_upper": 0.8602267524289574
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6724272157704623,
        "std": 0.005561044172632626,
        "ci_lower": 0.6614832546840895,
        "ci_upper": 0.6832361190230862
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.7931690209182275,
        "std": 0.00783883072581484,
        "ci_lower": 0.778471319449834,
        "ci_upper": 0.8089081452851365
      }
    }
  }
}