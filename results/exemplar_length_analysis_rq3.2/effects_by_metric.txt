# Effects of Exemplar Length by Metric

## Top-1 Accuracy

- Average effect (500→2500): 0.1316 (increase)
- Higher is better: False
- Overall impact: Negative (longer exemplars worsen this metric)

### Results by LLM:

- Llama-3.1: -0.1775
- Claude-3.5: -0.1434
- Gpt-4O: 0.4150

## Top-5 Accuracy

- Average effect (500→2500): 0.0577 (increase)
- Higher is better: False
- Overall impact: Negative (longer exemplars worsen this metric)

### Results by LLM:

- Gemma-2: -0.0134
- Llama-3.1: 0.0422
- Ministral: -0.0878
- Claude-3.5: 0.1350
- Gpt-4O: 0.2125

## True Class Confidence

- Average effect (500→2500): -0.0082 (decrease)
- Higher is better: False
- Overall impact: Positive (longer exemplars improve this metric)

### Results by LLM:

- Gemma-2: 0.0066
- Llama-3.1: -0.0770
- Ministral: -0.0130
- Claude-3.5: -0.0849
- Gpt-4O: 0.1273

## Prediction Entropy

- Average effect (500→2500): 0.0908 (increase)
- Higher is better: True
- Overall impact: Positive (longer exemplars improve this metric)

### Results by LLM:

- Gemma-2: -0.2432
- Llama-3.1: 0.4259
- Ministral: -0.1099
- Claude-3.5: 0.2598
- Gpt-4O: 0.1215

## bertscore

- Average effect (500→2500): 0.0107 (increase)
- Higher is better: True
- Overall impact: Positive (longer exemplars improve this metric)

### Results by LLM:

- Gemma-2: 0.0042
- Llama-3.1: 0.0026
- Ministral: 0.0018
- Claude-3.5: 0.0221
- Gpt-4O: 0.0227

## pinc

- Average effect (500→2500): -0.0090 (decrease)
- Higher is better: True
- Overall impact: Negative (longer exemplars worsen this metric)

### Results by LLM:

- Gemma-2: -0.0032
- Llama-3.1: 0.0026
- Ministral: -0.0028
- Claude-3.5: -0.0032
- Gpt-4O: -0.0383

