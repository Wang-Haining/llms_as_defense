{
  "('ebg', 'LogReg', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.1624620801804,
        "std": 0.14658379624989498,
        "ci_lower": 0.0013635951412811836,
        "ci_upper": 0.47563220964679537
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.16612937666809233,
        "std": 0.15238793156098354,
        "ci_lower": 0.003250171412753903,
        "ci_upper": 0.4882882484753737
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.729653277950064,
        "std": 0.0590228457629027,
        "ci_lower": 2.611425357618408,
        "ci_upper": 2.843861060540772
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.17828424258461045,
        "std": 0.003026633677511471,
        "ci_lower": 0.17226323677632413,
        "ci_upper": 0.18407836314555903
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.946846572152591,
        "std": 0.0035787881379086782,
        "ci_lower": 0.939507808910616,
        "ci_upper": 0.9534792924925071
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.499391633253296,
        "std": 0.0040514793866587165,
        "ci_lower": 0.4914440204869863,
        "ci_upper": 0.5073995846810201
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.12347899319124243,
        "std": 0.012121373699742203,
        "ci_lower": 0.10071580973630717,
        "ci_upper": 0.14759572823776676
      }
    }
  },
  "('ebg', 'LogReg', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.18400042185528304,
        "std": 0.15142585952723722,
        "ci_lower": 0.005111049814274888,
        "ci_upper": 0.5046177424211228
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.24604318958555685,
        "std": 0.1464063251877246,
        "ci_lower": 0.0166889840275688,
        "ci_upper": 0.5375058893552458
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.17151457340180326,
        "std": 0.14676952010909783,
        "ci_lower": 0.0023814104071294535,
        "ci_upper": 0.4828441344035099
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.7282135798959066,
        "std": 0.05685745221697121,
        "ci_lower": 2.6186278471392836,
        "ci_upper": 2.8415907851041187
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.1818365293084677,
        "std": 0.0034449865406666566,
        "ci_lower": 0.17522821837810512,
        "ci_upper": 0.18860074247688013
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9477577609313941,
        "std": 0.003571438107470755,
        "ci_lower": 0.940907745567143,
        "ci_upper": 0.954793603322584
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5030401276932828,
        "std": 0.004384018508687948,
        "ci_lower": 0.4938120005750013,
        "ci_upper": 0.5111692726218744
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.1253890874701562,
        "std": 0.01264550851310785,
        "ci_lower": 0.1007180681673517,
        "ci_upper": 0.1498659278295265
      }
    }
  },
  "('ebg', 'LogReg', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.18400042185528304,
        "std": 0.15142585952723722,
        "ci_lower": 0.005111049814274888,
        "ci_upper": 0.5046177424211228
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.27495671170619695,
        "std": 0.14960014028463836,
        "ci_lower": 0.02937024618980144,
        "ci_upper": 0.580229830035496
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.16427525071528862,
        "std": 0.14510161610795397,
        "ci_lower": 0.001781950724931791,
        "ci_upper": 0.4750198477980376
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.4844455426805516,
        "std": 0.05550020892016838,
        "ci_lower": 2.37116179445533,
        "ci_upper": 2.5899084410836197
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.15440045328033689,
        "std": 0.00312780299373328,
        "ci_lower": 0.14821606526992778,
        "ci_upper": 0.1605994424210135
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9438081729538357,
        "std": 0.0037990940642569316,
        "ci_lower": 0.9360121825115923,
        "ci_upper": 0.9509018610204611
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.49078140127506376,
        "std": 0.004126961494294127,
        "ci_lower": 0.4829455576228664,
        "ci_upper": 0.499009562706239
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.12161396153329182,
        "std": 0.012365977658782069,
        "ci_lower": 0.09748151920105064,
        "ci_upper": 0.14569034125302768
      }
    }
  },
  "('ebg', 'LogReg', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.21696600533925262,
        "std": 0.15097396474024674,
        "ci_lower": 0.006336258248271914,
        "ci_upper": 0.5254646587636552
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.4165233409995908,
        "std": 0.15774133991043482,
        "ci_lower": 0.11591048305923744,
        "ci_upper": 0.744050861006474
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.2091982792822968,
        "std": 0.14641305312955447,
        "ci_lower": 0.004955313944508726,
        "ci_upper": 0.5082117543983063
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.1973854067813807,
        "std": 0.050383230838963636,
        "ci_lower": 2.101777942837002,
        "ci_upper": 2.298863196843294
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.20212666083591113,
        "std": 0.0037357714307317694,
        "ci_lower": 0.19494053305539905,
        "ci_upper": 0.2095832405416138
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8691992264194576,
        "std": 0.0054051194348737185,
        "ci_lower": 0.8585401994323383,
        "ci_upper": 0.8798121661486353
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6497804172718863,
        "std": 0.003851069404379604,
        "ci_lower": 0.6422506506022762,
        "ci_upper": 0.6575594126272244
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8563275788598674,
        "std": 0.004111329499882695,
        "ci_lower": 0.8482580040631531,
        "ci_upper": 0.8643073623059309
      }
    }
  },
  "('ebg', 'LogReg', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "LogReg",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.6666666666666666,
        "post_mean": 0.40317144280819334,
        "std": 0.1588295903178742,
        "ci_lower": 0.1011285468082695,
        "ci_upper": 0.7251905719973368
      },
      "accuracy@5": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.5557589926618536,
        "std": 0.15701870148218697,
        "ci_lower": 0.23779710445250826,
        "ci_upper": 0.8586864340374704
      },
      "true_class_confidence": {
        "pre_value": 0.4264818602385359,
        "post_mean": 0.34271256740783085,
        "std": 0.15602794188079933,
        "ci_lower": 0.05809469405437358,
        "ci_upper": 0.664283837749639
      },
      "entropy": {
        "pre_value": 2.627006112023701,
        "post_mean": 2.7111921729279644,
        "std": 0.05628633484530169,
        "ci_lower": 2.6014077007136303,
        "ci_upper": 2.823433176625865
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.4689852305741349,
        "std": 0.006895941258879153,
        "ci_lower": 0.45522795171072467,
        "ci_upper": 0.482305657811636
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.6800547006053266,
        "std": 0.005881399503814468,
        "ci_lower": 0.6681363520530212,
        "ci_upper": 0.6911258851906854
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.8020931430421734,
        "std": 0.004077635871150593,
        "ci_lower": 0.7939682470789055,
        "ci_upper": 0.8099862184286057
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.941306041084403,
        "std": 0.0025015738875352393,
        "ci_lower": 0.9365574068744619,
        "ci_upper": 0.9464913869766521
      }
    }
  },
  "('ebg', 'SVM', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.21696600533925262,
        "std": 0.15097396474024674,
        "ci_lower": 0.006336258248271914,
        "ci_upper": 0.5254646587636552
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.15794412878196457,
        "std": 0.14682329892090745,
        "ci_lower": 0.0027969655050987274,
        "ci_upper": 0.4827633489534238
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.729653277950064,
        "std": 0.0590228457629027,
        "ci_lower": 2.611425357618408,
        "ci_upper": 2.843861060540772
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.17828424258461045,
        "std": 0.003026633677511471,
        "ci_lower": 0.17226323677632413,
        "ci_upper": 0.18407836314555903
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.946846572152591,
        "std": 0.0035787881379086782,
        "ci_lower": 0.939507808910616,
        "ci_upper": 0.9534792924925071
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.499391633253296,
        "std": 0.0040514793866587165,
        "ci_lower": 0.4914440204869863,
        "ci_upper": 0.5073995846810201
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.12347899319124243,
        "std": 0.012121373699742203,
        "ci_lower": 0.10071580973630717,
        "ci_upper": 0.14759572823776676
      }
    }
  },
  "('ebg', 'SVM', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.21696600533925262,
        "std": 0.15097396474024674,
        "ci_lower": 0.006336258248271914,
        "ci_upper": 0.5254646587636552
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.16077607799911292,
        "std": 0.14958348232182936,
        "ci_lower": 0.0026348950977349854,
        "ci_upper": 0.48261127223170547
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.7282135798959066,
        "std": 0.05685745221697121,
        "ci_lower": 2.6186278471392836,
        "ci_upper": 2.8415907851041187
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.1818365293084677,
        "std": 0.0034449865406666566,
        "ci_lower": 0.17522821837810512,
        "ci_upper": 0.18860074247688013
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9477577609313941,
        "std": 0.003571438107470755,
        "ci_lower": 0.940907745567143,
        "ci_upper": 0.954793603322584
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5030401276932828,
        "std": 0.004384018508687948,
        "ci_lower": 0.4938120005750013,
        "ci_upper": 0.5111692726218744
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.1253890874701562,
        "std": 0.01264550851310785,
        "ci_lower": 0.1007180681673517,
        "ci_upper": 0.1498659278295265
      }
    }
  },
  "('ebg', 'SVM', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.18400042185528304,
        "std": 0.15142585952723722,
        "ci_lower": 0.005111049814274888,
        "ci_upper": 0.5046177424211228
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.19652666369483304,
        "std": 0.1479391672935303,
        "ci_lower": 0.003529935670758092,
        "ci_upper": 0.5073225092382845
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.16859504090920444,
        "std": 0.15176927271209537,
        "ci_lower": 0.002179202710074928,
        "ci_upper": 0.4915766427992953
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.4844455426805516,
        "std": 0.05550020892016838,
        "ci_lower": 2.37116179445533,
        "ci_upper": 2.5899084410836197
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.15440045328033689,
        "std": 0.00312780299373328,
        "ci_lower": 0.14821606526992778,
        "ci_upper": 0.1605994424210135
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9438081729538357,
        "std": 0.0037990940642569316,
        "ci_lower": 0.9360121825115923,
        "ci_upper": 0.9509018610204611
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.49078140127506376,
        "std": 0.004126961494294127,
        "ci_lower": 0.4829455576228664,
        "ci_upper": 0.499009562706239
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.12161396153329182,
        "std": 0.012365977658782069,
        "ci_lower": 0.09748151920105064,
        "ci_upper": 0.14569034125302768
      }
    }
  },
  "('ebg', 'SVM', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.18400042185528304,
        "std": 0.15142585952723722,
        "ci_lower": 0.005111049814274888,
        "ci_upper": 0.5046177424211228
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.32300244969814457,
        "std": 0.15548491580870608,
        "ci_lower": 0.06159220968399123,
        "ci_upper": 0.6445781584583743
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.1874506331642146,
        "std": 0.14891862740050324,
        "ci_lower": 0.0081404617368757,
        "ci_upper": 0.5073220396797731
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.1973854067813807,
        "std": 0.050383230838963636,
        "ci_lower": 2.101777942837002,
        "ci_upper": 2.298863196843294
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.20212666083591113,
        "std": 0.0037357714307317694,
        "ci_lower": 0.19494053305539905,
        "ci_upper": 0.2095832405416138
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8691992264194576,
        "std": 0.0054051194348737185,
        "ci_lower": 0.8585401994323383,
        "ci_upper": 0.8798121661486353
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6497804172718863,
        "std": 0.003851069404379604,
        "ci_lower": 0.6422506506022762,
        "ci_upper": 0.6575594126272244
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8563275788598674,
        "std": 0.004111329499882695,
        "ci_lower": 0.8482580040631531,
        "ci_upper": 0.8643073623059309
      }
    }
  },
  "('ebg', 'SVM', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "SVM",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.7333333333333333,
        "post_mean": 0.36867571265835164,
        "std": 0.15586411849818388,
        "ci_lower": 0.06415296453954625,
        "ci_upper": 0.6762136767532781
      },
      "accuracy@5": {
        "pre_value": 0.8888888888888888,
        "post_mean": 0.5993963539762319,
        "std": 0.15575521893150973,
        "ci_lower": 0.28598228455395597,
        "ci_upper": 0.9064456205181831
      },
      "true_class_confidence": {
        "pre_value": 0.2527915090698283,
        "post_mean": 0.2463830135308812,
        "std": 0.14967787941966595,
        "ci_lower": 0.01448932018089157,
        "ci_upper": 0.555311453439499
      },
      "entropy": {
        "pre_value": 4.267334661829222,
        "post_mean": 2.7111921729279644,
        "std": 0.05628633484530169,
        "ci_lower": 2.6014077007136303,
        "ci_upper": 2.823433176625865
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.4689852305741349,
        "std": 0.006895941258879153,
        "ci_lower": 0.45522795171072467,
        "ci_upper": 0.482305657811636
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.6800547006053266,
        "std": 0.005881399503814468,
        "ci_lower": 0.6681363520530212,
        "ci_upper": 0.6911258851906854
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.8020931430421734,
        "std": 0.004077635871150593,
        "ci_lower": 0.7939682470789055,
        "ci_upper": 0.8099862184286057
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.941306041084403,
        "std": 0.0025015738875352393,
        "ci_lower": 0.9365574068744619,
        "ci_upper": 0.9464913869766521
      }
    }
  },
  "('ebg', 'RoBERTa', 'Llama-3.1')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.19652666369483304,
        "std": 0.1479391672935303,
        "ci_lower": 0.003529935670758092,
        "ci_upper": 0.5073225092382845
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.1478410057901835,
        "std": 0.15295738379483723,
        "ci_lower": 0.0004282023211877537,
        "ci_upper": 0.4836209537595389
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.729653277950064,
        "std": 0.0590228457629027,
        "ci_lower": 2.611425357618408,
        "ci_upper": 2.843861060540772
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.17828424258461045,
        "std": 0.003026633677511471,
        "ci_lower": 0.17226323677632413,
        "ci_upper": 0.18407836314555903
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.946846572152591,
        "std": 0.0035787881379086782,
        "ci_lower": 0.939507808910616,
        "ci_upper": 0.9534792924925071
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.499391633253296,
        "std": 0.0040514793866587165,
        "ci_lower": 0.4914440204869863,
        "ci_upper": 0.5073995846810201
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.12347899319124243,
        "std": 0.012121373699742203,
        "ci_lower": 0.10071580973630717,
        "ci_upper": 0.14759572823776676
      }
    }
  },
  "('ebg', 'RoBERTa', 'Gemma-2')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.1624620801804,
        "std": 0.14658379624989498,
        "ci_lower": 0.0013635951412811836,
        "ci_upper": 0.47563220964679537
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.14092755563915105,
        "std": 0.15333813778139732,
        "ci_lower": 0.0005535219623853405,
        "ci_upper": 0.4688273222947046
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.7282135798959066,
        "std": 0.05685745221697121,
        "ci_lower": 2.6186278471392836,
        "ci_upper": 2.8415907851041187
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.1818365293084677,
        "std": 0.0034449865406666566,
        "ci_lower": 0.17522821837810512,
        "ci_upper": 0.18860074247688013
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9477577609313941,
        "std": 0.003571438107470755,
        "ci_lower": 0.940907745567143,
        "ci_upper": 0.954793603322584
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5030401276932828,
        "std": 0.004384018508687948,
        "ci_lower": 0.4938120005750013,
        "ci_upper": 0.5111692726218744
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.1253890874701562,
        "std": 0.01264550851310785,
        "ci_lower": 0.1007180681673517,
        "ci_upper": 0.1498659278295265
      }
    }
  },
  "('ebg', 'RoBERTa', 'Ministral')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.2297867002688684,
        "std": 0.14848866012546683,
        "ci_lower": 0.013160651280901258,
        "ci_upper": 0.5416935411062885
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.14263339806763245,
        "std": 0.14889644379152556,
        "ci_lower": 0.0008446964090171418,
        "ci_upper": 0.4669508448373535
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.4844455426805516,
        "std": 0.05550020892016838,
        "ci_lower": 2.37116179445533,
        "ci_upper": 2.5899084410836197
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.15440045328033689,
        "std": 0.00312780299373328,
        "ci_lower": 0.14821606526992778,
        "ci_upper": 0.1605994424210135
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9438081729538357,
        "std": 0.0037990940642569316,
        "ci_lower": 0.9360121825115923,
        "ci_upper": 0.9509018610204611
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.49078140127506376,
        "std": 0.004126961494294127,
        "ci_lower": 0.4829455576228664,
        "ci_upper": 0.499009562706239
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.12161396153329182,
        "std": 0.012365977658782069,
        "ci_lower": 0.09748151920105064,
        "ci_upper": 0.14569034125302768
      }
    }
  },
  "('ebg', 'RoBERTa', 'Claude-3.5')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.459938021129627,
        "std": 0.15712915342385333,
        "ci_lower": 0.16316247909253312,
        "ci_upper": 0.7874310610614852
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.6344167095090265,
        "std": 0.15340999474227324,
        "ci_lower": 0.30933306480478934,
        "ci_upper": 0.912070466627938
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.43850101930838914,
        "std": 0.15793012704288892,
        "ci_lower": 0.11788863451172514,
        "ci_upper": 0.7571667118930439
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.1973854067813807,
        "std": 0.050383230838963636,
        "ci_lower": 2.101777942837002,
        "ci_upper": 2.298863196843294
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.20212666083591113,
        "std": 0.0037357714307317694,
        "ci_lower": 0.19494053305539905,
        "ci_upper": 0.2095832405416138
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8691992264194576,
        "std": 0.0054051194348737185,
        "ci_lower": 0.8585401994323383,
        "ci_upper": 0.8798121661486353
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6497804172718863,
        "std": 0.003851069404379604,
        "ci_lower": 0.6422506506022762,
        "ci_upper": 0.6575594126272244
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8563275788598674,
        "std": 0.004111329499882695,
        "ci_lower": 0.8482580040631531,
        "ci_upper": 0.8643073623059309
      }
    }
  },
  "('ebg', 'RoBERTa', 'GPT-4o')": {
    "corpus": "ebg",
    "threat_model": "RoBERTa",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.8666666666666667,
        "post_mean": 0.5411251329825744,
        "std": 0.16004393772281902,
        "ci_lower": 0.23885691908773168,
        "ci_upper": 0.8726573184758236
      },
      "accuracy@5": {
        "pre_value": 0.9333333333333333,
        "post_mean": 0.6780866458797011,
        "std": 0.15279232688093042,
        "ci_lower": 0.3611318773698095,
        "ci_upper": 0.9502484596498406
      },
      "true_class_confidence": {
        "pre_value": 0.8305529356002808,
        "post_mean": 0.5087976701326664,
        "std": 0.15442085871735267,
        "ci_lower": 0.20511144800500028,
        "ci_upper": 0.8128465487002228
      },
      "entropy": {
        "pre_value": 0.7004076838493347,
        "post_mean": 2.7111921729279644,
        "std": 0.05628633484530169,
        "ci_lower": 2.6014077007136303,
        "ci_upper": 2.823433176625865
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.4689852305741349,
        "std": 0.006895941258879153,
        "ci_lower": 0.45522795171072467,
        "ci_upper": 0.482305657811636
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.6800547006053266,
        "std": 0.005881399503814468,
        "ci_lower": 0.6681363520530212,
        "ci_upper": 0.6911258851906854
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.8020931430421734,
        "std": 0.004077635871150593,
        "ci_lower": 0.7939682470789055,
        "ci_upper": 0.8099862184286057
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.941306041084403,
        "std": 0.0025015738875352393,
        "ci_lower": 0.9365574068744619,
        "ci_upper": 0.9464913869766521
      }
    }
  }
}