{
  "('rj', 'LogReg', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.18821024308152567,
        "std": 0.15112486239168474,
        "ci_lower": 0.004459258011798474,
        "ci_upper": 0.5010000366221296
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.9589849561226922,
        "std": 0.06334748023898619,
        "ci_lower": 1.8346430029681675,
        "ci_upper": 2.082629222064294
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.18928985168159498,
        "std": 0.004665412319210188,
        "ci_lower": 0.18014914841688526,
        "ci_upper": 0.19860132934267394
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8906516443874702,
        "std": 0.008087487367154462,
        "ci_lower": 0.8741272162684361,
        "ci_upper": 0.9055841636099796
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5430646843052828,
        "std": 0.005843650006411323,
        "ci_lower": 0.5314720080785111,
        "ci_upper": 0.5540475845449782
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.503475023090635,
        "std": 0.0097706110909598,
        "ci_lower": 0.4841793180427096,
        "ci_upper": 0.5222995737180876
      }
    }
  },
  "('rj', 'LogReg', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.24972864234683648,
        "std": 0.14764893441382537,
        "ci_lower": 0.016454105822673887,
        "ci_upper": 0.5440364364782752
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.15334956521802712,
        "std": 0.14850824345223856,
        "ci_lower": 0.0017393084964637967,
        "ci_upper": 0.4816085545451242
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.7687772236771302,
        "std": 0.05970761691168293,
        "ci_lower": 1.649774156686004,
        "ci_upper": 1.88418561425352
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.15069462907094539,
        "std": 0.004370132054236348,
        "ci_lower": 0.14228229868403328,
        "ci_upper": 0.15930515206872814
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8755428577592096,
        "std": 0.008911940987920036,
        "ci_lower": 0.8583825368604814,
        "ci_upper": 0.893419312274023
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5275790342860505,
        "std": 0.005761762766497326,
        "ci_lower": 0.5159170498742708,
        "ci_upper": 0.538313494558428
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.44386246809495095,
        "std": 0.011796447918351672,
        "ci_lower": 0.41976453697323474,
        "ci_upper": 0.46618337662288906
      }
    }
  },
  "('rj', 'LogReg', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.4175914408028651,
        "std": 0.15604192512739995,
        "ci_lower": 0.10180037803387029,
        "ci_upper": 0.7192677877613619
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.2669400492118025,
        "std": 0.14802652455227241,
        "ci_lower": 0.02225306567429675,
        "ci_upper": 0.563788425863381
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.9504478619864394,
        "std": 0.06710246580435944,
        "ci_lower": 1.8236289382380975,
        "ci_upper": 2.086447689337771
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.40787938179600586,
        "std": 0.009533188730509146,
        "ci_lower": 0.3900910814080931,
        "ci_upper": 0.4278379501619941
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.5850958881352223,
        "std": 0.011277131251203186,
        "ci_lower": 0.5626441444872621,
        "ci_upper": 0.6070029824733466
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.7825350121972179,
        "std": 0.005927321088914144,
        "ci_lower": 0.770820820368604,
        "ci_upper": 0.7940552744547875
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8469240874596642,
        "std": 0.006539668314403183,
        "ci_lower": 0.8343677355133274,
        "ci_upper": 0.8600194013670108
      }
    }
  },
  "('rj', 'LogReg', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.19854516944572856,
        "std": 0.14375738649669972,
        "ci_lower": 0.0060494626426742725,
        "ci_upper": 0.49707041345210395
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.4622248459323186,
        "std": 0.0703321368586579,
        "ci_lower": 1.3230496139586552,
        "ci_upper": 1.5981854193522527
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.1324510481357052,
        "std": 0.0043508409229858006,
        "ci_lower": 0.1240628595235566,
        "ci_upper": 0.14101779385334337
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.884555981587066,
        "std": 0.008566588832619536,
        "ci_lower": 0.867900629402076,
        "ci_upper": 0.9011745538287244
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5351321865067514,
        "std": 0.005825376081329237,
        "ci_lower": 0.5237220797522786,
        "ci_upper": 0.5465574409620997
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.49085451905234617,
        "std": 0.011216556519427661,
        "ci_lower": 0.4692823494762472,
        "ci_upper": 0.5135246854253219
      }
    }
  },
  "('rj', 'LogReg', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.4175914408028651,
        "std": 0.15604192512739995,
        "ci_lower": 0.10180037803387029,
        "ci_upper": 0.7192677877613619
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.28066961061181883,
        "std": 0.15111994885184768,
        "ci_lower": 0.03340641651569537,
        "ci_upper": 0.5944072755116211
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.7432662122606535,
        "std": 0.06871749153177084,
        "ci_lower": 1.6060342949726576,
        "ci_upper": 1.8754498861910704
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.2780096708110537,
        "std": 0.005830605655364004,
        "ci_lower": 0.2663046935314648,
        "ci_upper": 0.2889205360982499
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.7665661359298355,
        "std": 0.0093415179068725,
        "ci_lower": 0.7483149167302776,
        "ci_upper": 0.7849251023081287
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6933074781550381,
        "std": 0.005717056731685359,
        "ci_lower": 0.6822846774561513,
        "ci_upper": 0.7047184955191714
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.7493913099448503,
        "std": 0.007806836519292905,
        "ci_lower": 0.7347485005917025,
        "ci_upper": 0.7650817409394345
      }
    }
  },
  "('rj', 'SVM', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.17856641328860542,
        "std": 0.1453798368004008,
        "ci_lower": 0.005726688582667255,
        "ci_upper": 0.4852136971823175
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.9589849561226922,
        "std": 0.06334748023898619,
        "ci_lower": 1.8346430029681675,
        "ci_upper": 2.082629222064294
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.18928985168159498,
        "std": 0.004665412319210188,
        "ci_lower": 0.18014914841688526,
        "ci_upper": 0.19860132934267394
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8906516443874702,
        "std": 0.008087487367154462,
        "ci_lower": 0.8741272162684361,
        "ci_upper": 0.9055841636099796
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5430646843052828,
        "std": 0.005843650006411323,
        "ci_lower": 0.5314720080785111,
        "ci_upper": 0.5540475845449782
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.503475023090635,
        "std": 0.0097706110909598,
        "ci_lower": 0.4841793180427096,
        "ci_upper": 0.5222995737180876
      }
    }
  },
  "('rj', 'SVM', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.1835202831183085,
        "std": 0.14585687407005815,
        "ci_lower": 0.0011179882431778358,
        "ci_upper": 0.478411002763052
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.7687772236771302,
        "std": 0.05970761691168293,
        "ci_lower": 1.649774156686004,
        "ci_upper": 1.88418561425352
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.15069462907094539,
        "std": 0.004370132054236348,
        "ci_lower": 0.14228229868403328,
        "ci_upper": 0.15930515206872814
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8755428577592096,
        "std": 0.008911940987920036,
        "ci_lower": 0.8583825368604814,
        "ci_upper": 0.893419312274023
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5275790342860505,
        "std": 0.005761762766497326,
        "ci_lower": 0.5159170498742708,
        "ci_upper": 0.538313494558428
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.44386246809495095,
        "std": 0.011796447918351672,
        "ci_lower": 0.41976453697323474,
        "ci_upper": 0.46618337662288906
      }
    }
  },
  "('rj', 'SVM', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.4175914408028651,
        "std": 0.15604192512739995,
        "ci_lower": 0.10180037803387029,
        "ci_upper": 0.7192677877613619
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.1912066765090517,
        "std": 0.14511264361792756,
        "ci_lower": 0.007113361742458551,
        "ci_upper": 0.5071312379766735
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.9504478619864394,
        "std": 0.06710246580435944,
        "ci_lower": 1.8236289382380975,
        "ci_upper": 2.086447689337771
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.40787938179600586,
        "std": 0.009533188730509146,
        "ci_lower": 0.3900910814080931,
        "ci_upper": 0.4278379501619941
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.5850958881352223,
        "std": 0.011277131251203186,
        "ci_lower": 0.5626441444872621,
        "ci_upper": 0.6070029824733466
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.7825350121972179,
        "std": 0.005927321088914144,
        "ci_lower": 0.770820820368604,
        "ci_upper": 0.7940552744547875
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8469240874596642,
        "std": 0.006539668314403183,
        "ci_lower": 0.8343677355133274,
        "ci_upper": 0.8600194013670108
      }
    }
  },
  "('rj', 'SVM', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.17256384669450023,
        "std": 0.1495380585315821,
        "ci_lower": 0.001835042956044793,
        "ci_upper": 0.48739458927262386
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.4622248459323186,
        "std": 0.0703321368586579,
        "ci_lower": 1.3230496139586552,
        "ci_upper": 1.5981854193522527
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.1324510481357052,
        "std": 0.0043508409229858006,
        "ci_lower": 0.1240628595235566,
        "ci_upper": 0.14101779385334337
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.884555981587066,
        "std": 0.008566588832619536,
        "ci_lower": 0.867900629402076,
        "ci_upper": 0.9011745538287244
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5351321865067514,
        "std": 0.005825376081329237,
        "ci_lower": 0.5237220797522786,
        "ci_upper": 0.5465574409620997
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.49085451905234617,
        "std": 0.011216556519427661,
        "ci_lower": 0.4692823494762472,
        "ci_upper": 0.5135246854253219
      }
    }
  },
  "('rj', 'SVM', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.4175914408028651,
        "std": 0.15604192512739995,
        "ci_lower": 0.10180037803387029,
        "ci_upper": 0.7192677877613619
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.19198265371790604,
        "std": 0.14722214699909836,
        "ci_lower": 0.007513690081046647,
        "ci_upper": 0.5071279879997002
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.7432662122606535,
        "std": 0.06871749153177084,
        "ci_lower": 1.6060342949726576,
        "ci_upper": 1.8754498861910704
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.2780096708110537,
        "std": 0.005830605655364004,
        "ci_lower": 0.2663046935314648,
        "ci_upper": 0.2889205360982499
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.7665661359298355,
        "std": 0.0093415179068725,
        "ci_lower": 0.7483149167302776,
        "ci_upper": 0.7849251023081287
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6933074781550381,
        "std": 0.005717056731685359,
        "ci_lower": 0.6822846774561513,
        "ci_upper": 0.7047184955191714
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.7493913099448503,
        "std": 0.007806836519292905,
        "ci_lower": 0.7347485005917025,
        "ci_upper": 0.7650817409394345
      }
    }
  },
  "('rj', 'RoBERTa', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.18452623940831853,
        "std": 0.14378265497042844,
        "ci_lower": 0.005189134715172743,
        "ci_upper": 0.4804886510540539
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.9589849561226922,
        "std": 0.06334748023898619,
        "ci_lower": 1.8346430029681675,
        "ci_upper": 2.082629222064294
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.18928985168159498,
        "std": 0.004665412319210188,
        "ci_lower": 0.18014914841688526,
        "ci_upper": 0.19860132934267394
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8906516443874702,
        "std": 0.008087487367154462,
        "ci_lower": 0.8741272162684361,
        "ci_upper": 0.9055841636099796
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5430646843052828,
        "std": 0.005843650006411323,
        "ci_lower": 0.5314720080785111,
        "ci_upper": 0.5540475845449782
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.503475023090635,
        "std": 0.0097706110909598,
        "ci_lower": 0.4841793180427096,
        "ci_upper": 0.5222995737180876
      }
    }
  },
  "('rj', 'RoBERTa', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.19225640952779216,
        "std": 0.1477087173135534,
        "ci_lower": 0.008370270525679504,
        "ci_upper": 0.5001915437007788
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.7687772236771302,
        "std": 0.05970761691168293,
        "ci_lower": 1.649774156686004,
        "ci_upper": 1.88418561425352
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.15069462907094539,
        "std": 0.004370132054236348,
        "ci_lower": 0.14228229868403328,
        "ci_upper": 0.15930515206872814
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.8755428577592096,
        "std": 0.008911940987920036,
        "ci_lower": 0.8583825368604814,
        "ci_upper": 0.893419312274023
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5275790342860505,
        "std": 0.005761762766497326,
        "ci_lower": 0.5159170498742708,
        "ci_upper": 0.538313494558428
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.44386246809495095,
        "std": 0.011796447918351672,
        "ci_lower": 0.41976453697323474,
        "ci_upper": 0.46618337662288906
      }
    }
  },
  "('rj', 'RoBERTa', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.4175914408028651,
        "std": 0.15604192512739995,
        "ci_lower": 0.10180037803387029,
        "ci_upper": 0.7192677877613619
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.19417847599691881,
        "std": 0.15099194389590373,
        "ci_lower": 0.00715748771448864,
        "ci_upper": 0.5218407474948257
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.9504478619864394,
        "std": 0.06710246580435944,
        "ci_lower": 1.8236289382380975,
        "ci_upper": 2.086447689337771
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.40787938179600586,
        "std": 0.009533188730509146,
        "ci_lower": 0.3900910814080931,
        "ci_upper": 0.4278379501619941
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.5850958881352223,
        "std": 0.011277131251203186,
        "ci_lower": 0.5626441444872621,
        "ci_upper": 0.6070029824733466
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.7825350121972179,
        "std": 0.005927321088914144,
        "ci_lower": 0.770820820368604,
        "ci_upper": 0.7940552744547875
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8469240874596642,
        "std": 0.006539668314403183,
        "ci_lower": 0.8343677355133274,
        "ci_upper": 0.8600194013670108
      }
    }
  },
  "('rj', 'RoBERTa', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.1949588891266958,
        "std": 0.14682189524323336,
        "ci_lower": 0.009445522234275769,
        "ci_upper": 0.5039595349274311
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.4622248459323186,
        "std": 0.0703321368586579,
        "ci_lower": 1.3230496139586552,
        "ci_upper": 1.5981854193522527
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.1324510481357052,
        "std": 0.0043508409229858006,
        "ci_lower": 0.1240628595235566,
        "ci_upper": 0.14101779385334337
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.884555981587066,
        "std": 0.008566588832619536,
        "ci_lower": 0.867900629402076,
        "ci_upper": 0.9011745538287244
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5351321865067514,
        "std": 0.005825376081329237,
        "ci_lower": 0.5237220797522786,
        "ci_upper": 0.5465574409620997
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.49085451905234617,
        "std": 0.011216556519427661,
        "ci_lower": 0.4692823494762472,
        "ci_upper": 0.5135246854253219
      }
    }
  },
  "('rj', 'RoBERTa', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.5159537162272743,
        "std": 0.15986205888447794,
        "ci_lower": 0.1955641140928672,
        "ci_upper": 0.8410852710675281
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.18386967791791192,
        "std": 0.14474020625180192,
        "ci_lower": 0.006905983452242431,
        "ci_upper": 0.4853869038781338
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.7432662122606535,
        "std": 0.06871749153177084,
        "ci_lower": 1.6060342949726576,
        "ci_upper": 1.8754498861910704
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.2780096708110537,
        "std": 0.005830605655364004,
        "ci_lower": 0.2663046935314648,
        "ci_upper": 0.2889205360982499
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.7665661359298355,
        "std": 0.0093415179068725,
        "ci_lower": 0.7483149167302776,
        "ci_upper": 0.7849251023081287
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6933074781550381,
        "std": 0.005717056731685359,
        "ci_lower": 0.6822846774561513,
        "ci_upper": 0.7047184955191714
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.7493913099448503,
        "std": 0.007806836519292905,
        "ci_lower": 0.7347485005917025,
        "ci_upper": 0.7650817409394345
      }
    }
  }
}