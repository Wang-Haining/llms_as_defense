{
  "('rj', 'LogReg', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.15342927562010975,
        "std": 0.15881192291697024,
        "ci_lower": 0.0016721721502119301,
        "ci_upper": 0.5131091533576763
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.8378290836322948,
        "std": 0.07667896432876635,
        "ci_lower": 1.6892609690405438,
        "ci_upper": 1.9900954559524227
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.20971978296537333,
        "std": 0.005120141662771514,
        "ci_lower": 0.19959449569625975,
        "ci_upper": 0.21953983043839234
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9368368924797191,
        "std": 0.005499362199420507,
        "ci_lower": 0.9259312724387926,
        "ci_upper": 0.9471964442508252
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5390448398667438,
        "std": 0.005712796038311129,
        "ci_lower": 0.5282008207567597,
        "ci_upper": 0.5509271632149246
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5167250190282873,
        "std": 0.009844859779760868,
        "ci_lower": 0.4969819001171667,
        "ci_upper": 0.5355873852031947
      }
    }
  },
  "('rj', 'LogReg', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3856916325461293,
        "std": 0.15846714534046194,
        "ci_lower": 0.09041917374041253,
        "ci_upper": 0.7120475068146469
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.20435391568542363,
        "std": 0.14517059033901966,
        "ci_lower": 0.008316274215724858,
        "ci_upper": 0.5125872591728775
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.9576714073418937,
        "std": 0.07533953196285542,
        "ci_lower": 1.8064849476491927,
        "ci_upper": 2.100700745248214
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.20610909997376908,
        "std": 0.004922319144654384,
        "ci_lower": 0.19709466570607506,
        "ci_upper": 0.2163378609311421
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9349159879998897,
        "std": 0.005503353338318423,
        "ci_lower": 0.9233887583043913,
        "ci_upper": 0.9447488905621135
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5423609425411141,
        "std": 0.0056811159949020935,
        "ci_lower": 0.531502283820765,
        "ci_upper": 0.5538445330190852
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5130143312231501,
        "std": 0.009670632336769744,
        "ci_lower": 0.4942092224027524,
        "ci_upper": 0.5327765024445991
      }
    }
  },
  "('rj', 'LogReg', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.1467278262414943,
        "std": 0.15464197430447188,
        "ci_lower": 0.0012895372744554825,
        "ci_upper": 0.4878285553095309
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.607306807140658,
        "std": 0.06799598927304598,
        "ci_lower": 1.471987635479944,
        "ci_upper": 1.73557010944257
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.15781269293768754,
        "std": 0.004422087524862265,
        "ci_lower": 0.14889420229571607,
        "ci_upper": 0.16623031617767234
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.92169252251657,
        "std": 0.0062883821913145834,
        "ci_lower": 0.909355881012935,
        "ci_upper": 0.9337947366980969
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6264116772443871,
        "std": 0.005600963246483749,
        "ci_lower": 0.6157305660903871,
        "ci_upper": 0.637372363007418
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.7343733787028779,
        "std": 0.007430724030716669,
        "ci_lower": 0.7196724438289717,
        "ci_upper": 0.7486204689932363
      }
    }
  },
  "('rj', 'LogReg', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3856916325461293,
        "std": 0.15846714534046194,
        "ci_lower": 0.09041917374041253,
        "ci_upper": 0.7120475068146469
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.18687903284964993,
        "std": 0.14989527269039915,
        "ci_lower": 0.0051401103586549025,
        "ci_upper": 0.5055415347387014
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.8922986040432406,
        "std": 0.0768893584001086,
        "ci_lower": 1.7429309554366248,
        "ci_upper": 2.042367425765762
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.16873488014016885,
        "std": 0.004801223635165264,
        "ci_lower": 0.15890284231228546,
        "ci_upper": 0.17780641021081953
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9322085392881981,
        "std": 0.005936202617504903,
        "ci_lower": 0.9204505868712454,
        "ci_upper": 0.9437071638639642
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5397951735385279,
        "std": 0.005832201174982741,
        "ci_lower": 0.5288906201912963,
        "ci_upper": 0.5514518727208922
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5302565351082497,
        "std": 0.01056077697104771,
        "ci_lower": 0.5088962146931706,
        "ci_upper": 0.5509477560987756
      }
    }
  },
  "('rj', 'LogReg', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3856916325461293,
        "std": 0.15846714534046194,
        "ci_lower": 0.09041917374041253,
        "ci_upper": 0.7120475068146469
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.20162718106275662,
        "std": 0.14382944025357722,
        "ci_lower": 0.009850544061910988,
        "ci_upper": 0.4985990910444516
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.9549001716219616,
        "std": 0.07274857703523936,
        "ci_lower": 1.8065066417947113,
        "ci_upper": 2.093555906453551
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.38315084434241314,
        "std": 0.007369639937474366,
        "ci_lower": 0.3685019378128357,
        "ci_upper": 0.3974558764637938
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.7405191984613262,
        "std": 0.008442890675378425,
        "ci_lower": 0.7236017760218654,
        "ci_upper": 0.7564314028267316
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.7827951167597829,
        "std": 0.0055505715691405375,
        "ci_lower": 0.7722842397917516,
        "ci_upper": 0.7941309915798899
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8724588789217849,
        "std": 0.0067169874922753375,
        "ci_lower": 0.8593059768098663,
        "ci_upper": 0.8857559438996786
      }
    }
  },
  "('rj', 'SVM', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.18285086733269615,
        "std": 0.14269207107468776,
        "ci_lower": 0.0047972963797699095,
        "ci_upper": 0.4795415478955977
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.8378290836322948,
        "std": 0.07667896432876635,
        "ci_lower": 1.6892609690405438,
        "ci_upper": 1.9900954559524227
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.20971978296537333,
        "std": 0.005120141662771514,
        "ci_lower": 0.19959449569625975,
        "ci_upper": 0.21953983043839234
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9368368924797191,
        "std": 0.005499362199420507,
        "ci_lower": 0.9259312724387926,
        "ci_upper": 0.9471964442508252
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5390448398667438,
        "std": 0.005712796038311129,
        "ci_lower": 0.5282008207567597,
        "ci_upper": 0.5509271632149246
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5167250190282873,
        "std": 0.009844859779760868,
        "ci_lower": 0.4969819001171667,
        "ci_upper": 0.5355873852031947
      }
    }
  },
  "('rj', 'SVM', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.17873591653915605,
        "std": 0.14727695756725953,
        "ci_lower": 0.005720881310471736,
        "ci_upper": 0.4992536858537438
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.9576714073418937,
        "std": 0.07533953196285542,
        "ci_lower": 1.8064849476491927,
        "ci_upper": 2.100700745248214
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.20610909997376908,
        "std": 0.004922319144654384,
        "ci_lower": 0.19709466570607506,
        "ci_upper": 0.2163378609311421
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9349159879998897,
        "std": 0.005503353338318423,
        "ci_lower": 0.9233887583043913,
        "ci_upper": 0.9447488905621135
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5423609425411141,
        "std": 0.0056811159949020935,
        "ci_lower": 0.531502283820765,
        "ci_upper": 0.5538445330190852
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5130143312231501,
        "std": 0.009670632336769744,
        "ci_lower": 0.4942092224027524,
        "ci_upper": 0.5327765024445991
      }
    }
  },
  "('rj', 'SVM', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.3856916325461293,
        "std": 0.15846714534046194,
        "ci_lower": 0.09041917374041253,
        "ci_upper": 0.7120475068146469
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.18286138092053478,
        "std": 0.1465478902340656,
        "ci_lower": 0.005546885583050994,
        "ci_upper": 0.49277610459900223
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.607306807140658,
        "std": 0.06799598927304598,
        "ci_lower": 1.471987635479944,
        "ci_upper": 1.73557010944257
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.15781269293768754,
        "std": 0.004422087524862265,
        "ci_lower": 0.14889420229571607,
        "ci_upper": 0.16623031617767234
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.92169252251657,
        "std": 0.0062883821913145834,
        "ci_lower": 0.909355881012935,
        "ci_upper": 0.9337947366980969
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6264116772443871,
        "std": 0.005600963246483749,
        "ci_lower": 0.6157305660903871,
        "ci_upper": 0.637372363007418
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.7343733787028779,
        "std": 0.007430724030716669,
        "ci_lower": 0.7196724438289717,
        "ci_upper": 0.7486204689932363
      }
    }
  },
  "('rj', 'SVM', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.24972864234683648,
        "std": 0.14764893441382537,
        "ci_lower": 0.016454105822673887,
        "ci_upper": 0.5440364364782752
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.18488119421929813,
        "std": 0.15290025190075515,
        "ci_lower": 0.006895898108027447,
        "ci_upper": 0.5171437204026393
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.8922986040432406,
        "std": 0.0768893584001086,
        "ci_lower": 1.7429309554366248,
        "ci_upper": 2.042367425765762
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.16873488014016885,
        "std": 0.004801223635165264,
        "ci_lower": 0.15890284231228546,
        "ci_upper": 0.17780641021081953
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9322085392881981,
        "std": 0.005936202617504903,
        "ci_lower": 0.9204505868712454,
        "ci_upper": 0.9437071638639642
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5397951735385279,
        "std": 0.005832201174982741,
        "ci_lower": 0.5288906201912963,
        "ci_upper": 0.5514518727208922
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5302565351082497,
        "std": 0.01056077697104771,
        "ci_lower": 0.5088962146931706,
        "ci_upper": 0.5509477560987756
      }
    }
  },
  "('rj', 'SVM', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.4851382528093997,
        "std": 0.1586010648462134,
        "ci_lower": 0.1845844189955757,
        "ci_upper": 0.8137150326037198
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.19552854277386728,
        "std": 0.14820333064955943,
        "ci_lower": 0.0034227958219029244,
        "ci_upper": 0.5037015437230222
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.9549001716219616,
        "std": 0.07274857703523936,
        "ci_lower": 1.8065066417947113,
        "ci_upper": 2.093555906453551
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.38315084434241314,
        "std": 0.007369639937474366,
        "ci_lower": 0.3685019378128357,
        "ci_upper": 0.3974558764637938
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.7405191984613262,
        "std": 0.008442890675378425,
        "ci_lower": 0.7236017760218654,
        "ci_upper": 0.7564314028267316
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.7827951167597829,
        "std": 0.0055505715691405375,
        "ci_lower": 0.7722842397917516,
        "ci_upper": 0.7941309915798899
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8724588789217849,
        "std": 0.0067169874922753375,
        "ci_lower": 0.8593059768098663,
        "ci_upper": 0.8857559438996786
      }
    }
  },
  "('rj', 'RoBERTa', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.18181850358451515,
        "std": 0.1457516923522967,
        "ci_lower": 0.0025432798433277213,
        "ci_upper": 0.49137840085345497
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.8378290836322948,
        "std": 0.07667896432876635,
        "ci_lower": 1.6892609690405438,
        "ci_upper": 1.9900954559524227
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.20971978296537333,
        "std": 0.005120141662771514,
        "ci_lower": 0.19959449569625975,
        "ci_upper": 0.21953983043839234
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9368368924797191,
        "std": 0.005499362199420507,
        "ci_lower": 0.9259312724387926,
        "ci_upper": 0.9471964442508252
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5390448398667438,
        "std": 0.005712796038311129,
        "ci_lower": 0.5282008207567597,
        "ci_upper": 0.5509271632149246
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5167250190282873,
        "std": 0.009844859779760868,
        "ci_lower": 0.4969819001171667,
        "ci_upper": 0.5355873852031947
      }
    }
  },
  "('rj', 'RoBERTa', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.16325994243436434,
        "std": 0.1491165192442223,
        "ci_lower": 0.004368383750445523,
        "ci_upper": 0.49109411846074513
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.9576714073418937,
        "std": 0.07533953196285542,
        "ci_lower": 1.8064849476491927,
        "ci_upper": 2.100700745248214
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.20610909997376908,
        "std": 0.004922319144654384,
        "ci_lower": 0.19709466570607506,
        "ci_upper": 0.2163378609311421
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9349159879998897,
        "std": 0.005503353338318423,
        "ci_lower": 0.9233887583043913,
        "ci_upper": 0.9447488905621135
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5423609425411141,
        "std": 0.0056811159949020935,
        "ci_lower": 0.531502283820765,
        "ci_upper": 0.5538445330190852
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5130143312231501,
        "std": 0.009670632336769744,
        "ci_lower": 0.4942092224027524,
        "ci_upper": 0.5327765024445991
      }
    }
  },
  "('rj', 'RoBERTa', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.24972864234683648,
        "std": 0.14764893441382537,
        "ci_lower": 0.016454105822673887,
        "ci_upper": 0.5440364364782752
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.2327355150300303,
        "std": 0.14726344166623934,
        "ci_lower": 0.014453557399080056,
        "ci_upper": 0.5348096591347826
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.607306807140658,
        "std": 0.06799598927304598,
        "ci_lower": 1.471987635479944,
        "ci_upper": 1.73557010944257
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.15781269293768754,
        "std": 0.004422087524862265,
        "ci_lower": 0.14889420229571607,
        "ci_upper": 0.16623031617767234
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.92169252251657,
        "std": 0.0062883821913145834,
        "ci_lower": 0.909355881012935,
        "ci_upper": 0.9337947366980969
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.6264116772443871,
        "std": 0.005600963246483749,
        "ci_lower": 0.6157305660903871,
        "ci_upper": 0.637372363007418
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.7343733787028779,
        "std": 0.007430724030716669,
        "ci_lower": 0.7196724438289717,
        "ci_upper": 0.7486204689932363
      }
    }
  },
  "('rj', 'RoBERTa', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.1614538202074393,
        "std": 0.14878524331091286,
        "ci_lower": 0.002384233965943248,
        "ci_upper": 0.48756084928441995
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.8922986040432406,
        "std": 0.0768893584001086,
        "ci_lower": 1.7429309554366248,
        "ci_upper": 2.042367425765762
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.16873488014016885,
        "std": 0.004801223635165264,
        "ci_lower": 0.15890284231228546,
        "ci_upper": 0.17780641021081953
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9322085392881981,
        "std": 0.005936202617504903,
        "ci_lower": 0.9204505868712454,
        "ci_upper": 0.9437071638639642
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5397951735385279,
        "std": 0.005832201174982741,
        "ci_lower": 0.5288906201912963,
        "ci_upper": 0.5514518727208922
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5302565351082497,
        "std": 0.01056077697104771,
        "ci_lower": 0.5088962146931706,
        "ci_upper": 0.5509477560987756
      }
    }
  },
  "('rj', 'RoBERTa', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.24972864234683648,
        "std": 0.14764893441382537,
        "ci_lower": 0.016454105822673887,
        "ci_upper": 0.5440364364782752
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3856916325461293,
        "std": 0.15846714534046194,
        "ci_lower": 0.09041917374041253,
        "ci_upper": 0.7120475068146469
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.24663098615305515,
        "std": 0.14746718249431054,
        "ci_lower": 0.020155081286003088,
        "ci_upper": 0.5551530206298106
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.9549001716219616,
        "std": 0.07274857703523936,
        "ci_lower": 1.8065066417947113,
        "ci_upper": 2.093555906453551
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.38315084434241314,
        "std": 0.007369639937474366,
        "ci_lower": 0.3685019378128357,
        "ci_upper": 0.3974558764637938
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.7405191984613262,
        "std": 0.008442890675378425,
        "ci_lower": 0.7236017760218654,
        "ci_upper": 0.7564314028267316
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.7827951167597829,
        "std": 0.0055505715691405375,
        "ci_lower": 0.7722842397917516,
        "ci_upper": 0.7941309915798899
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8724588789217849,
        "std": 0.0067169874922753375,
        "ci_lower": 0.8593059768098663,
        "ci_upper": 0.8857559438996786
      }
    }
  }
}