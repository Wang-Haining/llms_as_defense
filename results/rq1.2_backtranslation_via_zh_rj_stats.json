{
  "('rj', 'LogReg', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.5827813536756096,
        "std": 0.1590552115799062,
        "ci_lower": 0.2650422839392498,
        "ci_upper": 0.8878760657861513
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.2298391491170759,
        "std": 0.14333595027599608,
        "ci_lower": 0.013327370149997912,
        "ci_upper": 0.5174353558378328
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.7214372314690471,
        "std": 0.06698409404014596,
        "ci_lower": 1.5939592906818585,
        "ci_upper": 1.8550606303590358
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.6351361529055691,
        "std": 0.010087953082983858,
        "ci_lower": 0.6146317898427686,
        "ci_upper": 0.6548046367204657
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.46081633510029435,
        "std": 0.009002787785197171,
        "ci_lower": 0.44389043629746977,
        "ci_upper": 0.4795842360593898
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.88413699368225,
        "std": 0.004263811036967768,
        "ci_lower": 0.875755210082704,
        "ci_upper": 0.8925307541625617
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.9364404671848947,
        "std": 0.0036233780347145854,
        "ci_lower": 0.9288292376931777,
        "ci_upper": 0.9431226750512027
      }
    }
  },
  "('rj', 'LogReg', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.22096832533639899,
        "std": 0.14888598405797357,
        "ci_lower": 0.012769997160263688,
        "ci_upper": 0.5348363812751216
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.710518567524428,
        "std": 0.06830482125510685,
        "ci_lower": 1.575762544500737,
        "ci_upper": 1.8436427659384211
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.2512347653877257,
        "std": 0.005176754623066242,
        "ci_lower": 0.24121170616119356,
        "ci_upper": 0.26137004925457047
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9123420357805322,
        "std": 0.006564722508863408,
        "ci_lower": 0.8999801681020231,
        "ci_upper": 0.9251756792095821
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5483633601872562,
        "std": 0.005739234822236021,
        "ci_lower": 0.5368207527573101,
        "ci_upper": 0.5592035171037965
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5132875235866257,
        "std": 0.008747506260803239,
        "ci_lower": 0.49740511949191574,
        "ci_upper": 0.5318230690267989
      }
    }
  },
  "('rj', 'LogReg', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.20843110213257557,
        "std": 0.14846376482931659,
        "ci_lower": 0.010378883935282935,
        "ci_upper": 0.5200827791784545
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.7602230647394586,
        "std": 0.06745534834320069,
        "ci_lower": 1.6303660514072038,
        "ci_upper": 1.8922933102952106
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.23516799670636754,
        "std": 0.005813584986064236,
        "ci_lower": 0.22341593733256146,
        "ci_upper": 0.24647773643998924
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.910838398630994,
        "std": 0.0065860105641687,
        "ci_lower": 0.8970447080647583,
        "ci_upper": 0.9230919553516631
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.548880485160111,
        "std": 0.005800362718862657,
        "ci_lower": 0.5376592765935594,
        "ci_upper": 0.5600279674667189
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5159363346804969,
        "std": 0.009856328839089979,
        "ci_lower": 0.4973257890523502,
        "ci_upper": 0.5361938723922169
      }
    }
  },
  "('rj', 'LogReg', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.19699375582971546,
        "std": 0.14696988777261133,
        "ci_lower": 0.004066117108950132,
        "ci_upper": 0.50620465065055
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.7416861008729725,
        "std": 0.06863204900581589,
        "ci_lower": 1.6014257110421586,
        "ci_upper": 1.873052322927367
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.2528786818817547,
        "std": 0.005045739299782324,
        "ci_lower": 0.2430817014535995,
        "ci_upper": 0.2628750221911361
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9121980486187133,
        "std": 0.006498266779955636,
        "ci_lower": 0.8995700749534947,
        "ci_upper": 0.9245856747176141
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5495170310521575,
        "std": 0.005781291957535502,
        "ci_lower": 0.5376449492930309,
        "ci_upper": 0.5605797728881071
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5058406001175717,
        "std": 0.00884986547201959,
        "ci_lower": 0.4883100641042251,
        "ci_upper": 0.5229139912133731
      }
    }
  },
  "('rj', 'LogReg', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "LogReg",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.3528812828877993,
        "std": 0.15500211192598803,
        "ci_lower": 0.07467043206211117,
        "ci_upper": 0.6705710392957202
      },
      "true_class_confidence": {
        "pre_value": 0.2347824771315894,
        "post_mean": 0.1654067152298242,
        "std": 0.14309103431006143,
        "ci_lower": 0.0019603425711679297,
        "ci_upper": 0.4750374700389128
      },
      "entropy": {
        "pre_value": 1.1018900402848348,
        "post_mean": 1.9874607001322133,
        "std": 0.07192957375552313,
        "ci_lower": 1.8535586125492398,
        "ci_upper": 2.137891187261404
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.30861094305789843,
        "std": 0.007562407723061714,
        "ci_lower": 0.29302632134224343,
        "ci_upper": 0.32266118089736257
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.7773878860721866,
        "std": 0.00866393986749842,
        "ci_lower": 0.7602419192775647,
        "ci_upper": 0.7944075386665408
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.7370229606464236,
        "std": 0.005968749884778984,
        "ci_lower": 0.725434578910595,
        "ci_upper": 0.7486756607225935
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8611902637022172,
        "std": 0.00614227870873199,
        "ci_lower": 0.8492305146635699,
        "ci_upper": 0.8732015426866034
      }
    }
  },
  "('rj', 'SVM', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.24972864234683648,
        "std": 0.14764893441382537,
        "ci_lower": 0.016454105822673887,
        "ci_upper": 0.5440364364782752
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.5159537162272743,
        "std": 0.15986205888447794,
        "ci_lower": 0.1955641140928672,
        "ci_upper": 0.8410852710675281
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.213723572648506,
        "std": 0.14969891113199146,
        "ci_lower": 0.008817205099957324,
        "ci_upper": 0.5260475962461167
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.7214372314690471,
        "std": 0.06698409404014596,
        "ci_lower": 1.5939592906818585,
        "ci_upper": 1.8550606303590358
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.6351361529055691,
        "std": 0.010087953082983858,
        "ci_lower": 0.6146317898427686,
        "ci_upper": 0.6548046367204657
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.46081633510029435,
        "std": 0.009002787785197171,
        "ci_lower": 0.44389043629746977,
        "ci_upper": 0.4795842360593898
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.88413699368225,
        "std": 0.004263811036967768,
        "ci_lower": 0.875755210082704,
        "ci_upper": 0.8925307541625617
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.9364404671848947,
        "std": 0.0036233780347145854,
        "ci_lower": 0.9288292376931777,
        "ci_upper": 0.9431226750512027
      }
    }
  },
  "('rj', 'SVM', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.18229482649012568,
        "std": 0.14574233728852534,
        "ci_lower": 0.0038309032042746884,
        "ci_upper": 0.4853921804891306
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.710518567524428,
        "std": 0.06830482125510685,
        "ci_lower": 1.575762544500737,
        "ci_upper": 1.8436427659384211
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.2512347653877257,
        "std": 0.005176754623066242,
        "ci_lower": 0.24121170616119356,
        "ci_upper": 0.26137004925457047
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9123420357805322,
        "std": 0.006564722508863408,
        "ci_lower": 0.8999801681020231,
        "ci_upper": 0.9251756792095821
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5483633601872562,
        "std": 0.005739234822236021,
        "ci_lower": 0.5368207527573101,
        "ci_upper": 0.5592035171037965
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5132875235866257,
        "std": 0.008747506260803239,
        "ci_lower": 0.49740511949191574,
        "ci_upper": 0.5318230690267989
      }
    }
  },
  "('rj', 'SVM', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.28644280324051224,
        "std": 0.15150747343465973,
        "ci_lower": 0.038579560739608845,
        "ci_upper": 0.5990025646912381
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.1818450394108905,
        "std": 0.15096267266619962,
        "ci_lower": 0.006818273531632569,
        "ci_upper": 0.5099760077738561
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.7602230647394586,
        "std": 0.06745534834320069,
        "ci_lower": 1.6303660514072038,
        "ci_upper": 1.8922933102952106
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.23516799670636754,
        "std": 0.005813584986064236,
        "ci_lower": 0.22341593733256146,
        "ci_upper": 0.24647773643998924
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.910838398630994,
        "std": 0.0065860105641687,
        "ci_lower": 0.8970447080647583,
        "ci_upper": 0.9230919553516631
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.548880485160111,
        "std": 0.005800362718862657,
        "ci_lower": 0.5376592765935594,
        "ci_upper": 0.5600279674667189
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5159363346804969,
        "std": 0.009856328839089979,
        "ci_lower": 0.4973257890523502,
        "ci_upper": 0.5361938723922169
      }
    }
  },
  "('rj', 'SVM', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.1837139244272668,
        "std": 0.14391157172841848,
        "ci_lower": 0.006601630856124044,
        "ci_upper": 0.47965593441657817
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.7416861008729725,
        "std": 0.06863204900581589,
        "ci_lower": 1.6014257110421586,
        "ci_upper": 1.873052322927367
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.2528786818817547,
        "std": 0.005045739299782324,
        "ci_lower": 0.2430817014535995,
        "ci_upper": 0.2628750221911361
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9121980486187133,
        "std": 0.006498266779955636,
        "ci_lower": 0.8995700749534947,
        "ci_upper": 0.9245856747176141
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5495170310521575,
        "std": 0.005781291957535502,
        "ci_lower": 0.5376449492930309,
        "ci_upper": 0.5605797728881071
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5058406001175717,
        "std": 0.00884986547201959,
        "ci_lower": 0.4883100641042251,
        "ci_upper": 0.5229139912133731
      }
    }
  },
  "('rj', 'SVM', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "SVM",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.2857142857142857,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "accuracy@5": {
        "pre_value": 0.5238095238095238,
        "post_mean": 0.3856916325461293,
        "std": 0.15846714534046194,
        "ci_lower": 0.09041917374041253,
        "ci_upper": 0.7120475068146469
      },
      "true_class_confidence": {
        "pre_value": 0.13195051529659466,
        "post_mean": 0.1938796483488017,
        "std": 0.1509874073605217,
        "ci_lower": 0.0044830314103460975,
        "ci_upper": 0.5213489229506394
      },
      "entropy": {
        "pre_value": 3.3525514996139236,
        "post_mean": 1.9874607001322133,
        "std": 0.07192957375552313,
        "ci_lower": 1.8535586125492398,
        "ci_upper": 2.137891187261404
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.30861094305789843,
        "std": 0.007562407723061714,
        "ci_lower": 0.29302632134224343,
        "ci_upper": 0.32266118089736257
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.7773878860721866,
        "std": 0.00866393986749842,
        "ci_lower": 0.7602419192775647,
        "ci_upper": 0.7944075386665408
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.7370229606464236,
        "std": 0.005968749884778984,
        "ci_lower": 0.725434578910595,
        "ci_upper": 0.7486756607225935
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8611902637022172,
        "std": 0.00614227870873199,
        "ci_lower": 0.8492305146635699,
        "ci_upper": 0.8732015426866034
      }
    }
  },
  "('rj', 'RoBERTa', 'GPT-4o')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "GPT-4o",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.24972864234683648,
        "std": 0.14764893441382537,
        "ci_lower": 0.016454105822673887,
        "ci_upper": 0.5440364364782752
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.44949292108390054,
        "std": 0.15983506177789672,
        "ci_lower": 0.13335833594386848,
        "ci_upper": 0.7727533040958386
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.26137195092907967,
        "std": 0.1533096338347239,
        "ci_lower": 0.015292981276506457,
        "ci_upper": 0.5741904334243503
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.7214372314690471,
        "std": 0.06698409404014596,
        "ci_lower": 1.5939592906818585,
        "ci_upper": 1.8550606303590358
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.6351361529055691,
        "std": 0.010087953082983858,
        "ci_lower": 0.6146317898427686,
        "ci_upper": 0.6548046367204657
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.46081633510029435,
        "std": 0.009002787785197171,
        "ci_lower": 0.44389043629746977,
        "ci_upper": 0.4795842360593898
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.88413699368225,
        "std": 0.004263811036967768,
        "ci_lower": 0.875755210082704,
        "ci_upper": 0.8925307541625617
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.9364404671848947,
        "std": 0.0036233780347145854,
        "ci_lower": 0.9288292376931777,
        "ci_upper": 0.9431226750512027
      }
    }
  },
  "('rj', 'RoBERTa', 'Gemma-2')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Gemma-2",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1259342532258233,
        "std": 0.15921677212933338,
        "ci_lower": 0.0005515869267397737,
        "ci_upper": 0.4920190817080465
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.24972864234683648,
        "std": 0.14764893441382537,
        "ci_lower": 0.016454105822673887,
        "ci_upper": 0.5440364364782752
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.1644801245125106,
        "std": 0.143686815939629,
        "ci_lower": 0.0030241201094596544,
        "ci_upper": 0.47629886784015035
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.710518567524428,
        "std": 0.06830482125510685,
        "ci_lower": 1.575762544500737,
        "ci_upper": 1.8436427659384211
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.2512347653877257,
        "std": 0.005176754623066242,
        "ci_lower": 0.24121170616119356,
        "ci_upper": 0.26137004925457047
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9123420357805322,
        "std": 0.006564722508863408,
        "ci_lower": 0.8999801681020231,
        "ci_upper": 0.9251756792095821
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5483633601872562,
        "std": 0.005739234822236021,
        "ci_lower": 0.5368207527573101,
        "ci_upper": 0.5592035171037965
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5132875235866257,
        "std": 0.008747506260803239,
        "ci_lower": 0.49740511949191574,
        "ci_upper": 0.5318230690267989
      }
    }
  },
  "('rj', 'RoBERTa', 'Ministral')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Ministral",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.24972864234683648,
        "std": 0.14764893441382537,
        "ci_lower": 0.016454105822673887,
        "ci_upper": 0.5440364364782752
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.17817964867430822,
        "std": 0.1472397603057684,
        "ci_lower": 0.005138584261229678,
        "ci_upper": 0.49231754587788795
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.7602230647394586,
        "std": 0.06745534834320069,
        "ci_lower": 1.6303660514072038,
        "ci_upper": 1.8922933102952106
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.23516799670636754,
        "std": 0.005813584986064236,
        "ci_lower": 0.22341593733256146,
        "ci_upper": 0.24647773643998924
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.910838398630994,
        "std": 0.0065860105641687,
        "ci_lower": 0.8970447080647583,
        "ci_upper": 0.9230919553516631
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.548880485160111,
        "std": 0.005800362718862657,
        "ci_lower": 0.5376592765935594,
        "ci_upper": 0.5600279674667189
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5159363346804969,
        "std": 0.009856328839089979,
        "ci_lower": 0.4973257890523502,
        "ci_upper": 0.5361938723922169
      }
    }
  },
  "('rj', 'RoBERTa', 'Llama-3.1')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Llama-3.1",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.1849646065380778,
        "std": 0.1474677474130982,
        "ci_lower": 0.0020649781325356165,
        "ci_upper": 0.498881369797724
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.21951052824969977,
        "std": 0.1486903781551234,
        "ci_lower": 0.006515396685296505,
        "ci_upper": 0.5356550091020563
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.1787593759448031,
        "std": 0.15087714346860032,
        "ci_lower": 0.0015483003764951311,
        "ci_upper": 0.5000716094213385
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.7416861008729725,
        "std": 0.06863204900581589,
        "ci_lower": 1.6014257110421586,
        "ci_upper": 1.873052322927367
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.2528786818817547,
        "std": 0.005045739299782324,
        "ci_lower": 0.2430817014535995,
        "ci_upper": 0.2628750221911361
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.9121980486187133,
        "std": 0.006498266779955636,
        "ci_lower": 0.8995700749534947,
        "ci_upper": 0.9245856747176141
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.5495170310521575,
        "std": 0.005781291957535502,
        "ci_lower": 0.5376449492930309,
        "ci_upper": 0.5605797728881071
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.5058406001175717,
        "std": 0.00884986547201959,
        "ci_lower": 0.4883100641042251,
        "ci_upper": 0.5229139912133731
      }
    }
  },
  "('rj', 'RoBERTa', 'Claude-3.5')": {
    "corpus": "rj",
    "threat_model": "RoBERTa",
    "defense_model": "Claude-3.5",
    "effectiveness_estimates": {
      "accuracy@1": {
        "pre_value": 0.19047619047619047,
        "post_mean": 0.24972864234683648,
        "std": 0.14764893441382537,
        "ci_lower": 0.016454105822673887,
        "ci_upper": 0.5440364364782752
      },
      "accuracy@5": {
        "pre_value": 0.5714285714285714,
        "post_mean": 0.31917303420131443,
        "std": 0.15214789742552304,
        "ci_lower": 0.049426692663440935,
        "ci_upper": 0.6237624650115318
      },
      "true_class_confidence": {
        "pre_value": 0.19828349351882935,
        "post_mean": 0.25411440559353243,
        "std": 0.14471356419568068,
        "ci_lower": 0.00914488007729802,
        "ci_upper": 0.53770522942702
      },
      "entropy": {
        "pre_value": 1.1630594730377197,
        "post_mean": 1.9874607001322133,
        "std": 0.07192957375552313,
        "ci_lower": 1.8535586125492398,
        "ci_upper": 2.137891187261404
      }
    },
    "quality_estimates": {
      "meteor": {
        "pre_value": 1.0,
        "post_mean": 0.30861094305789843,
        "std": 0.007562407723061714,
        "ci_lower": 0.29302632134224343,
        "ci_upper": 0.32266118089736257
      },
      "pinc": {
        "pre_value": 0.0,
        "post_mean": 0.7773878860721866,
        "std": 0.00866393986749842,
        "ci_lower": 0.7602419192775647,
        "ci_upper": 0.7944075386665408
      },
      "bertscore": {
        "pre_value": 1.0,
        "post_mean": 0.7370229606464236,
        "std": 0.005968749884778984,
        "ci_lower": 0.725434578910595,
        "ci_upper": 0.7486756607225935
      },
      "sbert": {
        "pre_value": 1.0,
        "post_mean": 0.8611902637022172,
        "std": 0.00614227870873199,
        "ci_lower": 0.8492305146635699,
        "ci_upper": 0.8732015426866034
      }
    }
  }
}