
I do think gender discrimination still exists but its not as one sided as
it used to be. Back in the day gender discrimination was mainly emphasized
toward women. Women could only have jobs as secretaries or jobs that
required helping; no real authority. Now I believe both men and women are
discriminated against because of their gender. I see this in the fact that
men are looked down upon if they want to be a stay at home dad or work as
a secretary or a nurse. Why do men always have to have the power? For
example I would like to be a nurse practioner in the emergency room, and
my main reason for wanting to be a nurse practioner instead of a doctor is
because I don't want all the responsibility and I want to have a family
and a life outside of my career. I feel like men can't want what I want,
they can't not want responsiblity and authority afterall isn't that what
is ingrained in our heads as a child, men are the head of the household,
they bring in the money.
On the opposite spectrum women are also discriminated against in the
workplace but its not as obvious. There are laws now that prevent women
from  being discriminated against but that doesn't mean that people don't
find ways to get around those laws. I believe that women in most
conservative businesses have to work twice as hard as men do who have the
same criteria as they do in order to reach there level. But this is only
looking at movies and the media which is what I base most of my knowledge
on being that I have never been discriminated against nor anyone I know.

