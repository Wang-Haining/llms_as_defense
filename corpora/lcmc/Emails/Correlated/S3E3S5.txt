I think gender discrimination is very much still an issue in the US workplace, and will be for probably the rest of my lifetime.  There has been a lot of progress in this area though, aspecially recently as more and more women become well paid executives.  Women are discriminated against daily, and not just in pay.  The way they are treated by men in the workplace, especially their bosses, is discrimination by its very definition.  Many women are not treated equally, and not treated the same as men.  Whenever a man makes a comment about a woman's body, or a boss pinches a butt, that's discrimination.  It's demeaning for the woman, and in many cases can cause her to feel uncomfortable in the workplace and even have to change jobs.  The workplace should be an equal playing field for everyone, women shouldn't be made to feel inferior like that.  Noone should have to feel that their boss is coming on to them, and think "Oh, maybe if I just sleep with him just once, he'll open a trapdoor in the glass ceiling."  That ceiling just shouldn't be there in the first place.

Pay is becoming much more equal in the salaried desk jobs where it was first raised as an issue.  Overall, women are hired much more often, which was the first big step.  Company regulations about pay and treatment are much stricter for fear of lawsuit.  Yet you still hear about the woman getting paid 75% of what a man doing her same job is getting, or the woman who enters an interview and leaves with "I'm sorry, you're just not what we're looking for."  Eventually, the way things are going, this will hopefully disappear from society.

Men are discriminated against in the workplace as well.  In traditionally female occupations like nurse or daycare provider, they may not only be teased by co-workers but by anyone they meet.  However, the discrimination against men does not usually include such aspects as pay discrimination or unwanted sexual attention, so it is not as serious.  I think it's rare for a man to leave his job because the workplace environment was made uncomfortable by excessive teasing.  This is also becoming less and less prevalent as men appear on TV doing traditionally female things – as hairstylists on Project Runway and What Not to Wear, as nurses on ER and Grey's Anatomy, and in many other roles.

The same thing is happening to women, as more and more women appear in positions of power in popular TV shows.  Like it or not, TV is a powerful influence on society today.  Trends are set through TV, and it teaches us social norms.  By seeing women running businesses on TV, men believe it is more acceptable when they see it done in real life.  This also means that younger generations will grow up accepting these things as normal.  As long as powerful women are portrayed in a positive light and respected on TV, the younger generation will fell that it is acceptable for a woman to have a higher paying job than her husband, and to even be a top CEO.  Women will be known for performing differently then men, but no better and no worse.  It will be seen as good to have the differing perspectives of men and women working together on one issue.

Parents need to help along this change in sentiment by talking about women in a positive light in front of their children.  Negative attitudes like "That's not her place" and "She should be home with the kids" are what will drag potentially successful women down.  Also, parents should encourage their female children to choose challenging professions, and to educate them about the variety of possibilities open to them.  When all of these things come about, discrimination will truly no longer be an issue in the workplace.

